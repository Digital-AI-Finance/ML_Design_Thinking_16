% Act 4: Synthesis (4 slides)

\begin{frame}{23. Deep Learning Evolution Timeline}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{Key Milestones:}

\textbf{2012 - AlexNet:}
\begin{itemize}
\item CNNs + ImageNet breakthrough
\item 8-layer network, ReLU activation
\item GPU acceleration
\end{itemize}

\textbf{2014 - Sequence-to-Sequence:}
\begin{itemize}
\item RNNs for machine translation
\item Encoder-decoder architecture
\end{itemize}

\textbf{2017 - Transformers:}
\begin{itemize}
\item ``Attention Is All You Need''
\item Self-attention mechanism
\item Foundation for GPT, BERT
\end{itemize}

\textbf{2022 - GPT-4:}
\begin{itemize}
\item 1.7 trillion parameters
\item Multimodal capabilities
\item Human-level performance
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/deep_learning_timeline.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{From 8 layers to 1000+ layers in just one decade}
\end{frame}

\begin{frame}{24. Architecture Design Principles}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{Universal Design Principles:}

\textbf{1. Locality:}
\begin{itemize}
\item Nearby elements are related
\item CNNs: Spatial locality
\item RNNs: Temporal locality
\end{itemize}

\textbf{2. Hierarchy:}
\begin{itemize}
\item Build complexity gradually
\item Low-level → High-level features
\item Mirrors human cognition
\end{itemize}

\textbf{3. Invariance:}
\begin{itemize}
\item Robust to irrelevant changes
\item Translation, rotation, scale
\item Attention: Permutation invariance
\end{itemize}

\textbf{4. Efficiency:}
\begin{itemize}
\item Parameter sharing
\item Computational optimization
\item Memory constraints
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/design_principles.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Good architectures encode the right inductive biases for the domain}
\end{frame}

\begin{frame}{25. Modern Applications: Computer Vision, NLP, Multimodal}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{Computer Vision:}
\begin{itemize}
\item Object detection (YOLO, R-CNN)
\item Image segmentation
\item Medical imaging diagnosis
\item Autonomous driving
\end{itemize}

\textbf{Natural Language Processing:}
\begin{itemize}
\item Machine translation (95\% human quality)
\item Text generation (GPT family)
\item Question answering
\item Code generation
\end{itemize}

\textbf{Multimodal AI:}
\begin{itemize}
\item Image captioning
\item Visual question answering
\item Video understanding
\item Robotics integration
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/modern_applications.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Neural networks now match or exceed human performance in many domains}
\end{frame}

\begin{frame}{26. Summary \& Preview: Generative AI}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{What We Learned:}
\begin{itemize}
\item Perceptrons: Linear limitations
\item MLPs: Non-linear but shallow
\item Deep networks: Vanishing gradients
\item Modern architectures: Structured solutions
\end{itemize}

\textbf{Key Insight:}
\textcolor{mlblue}{Architecture matters more than size}

\textbf{Next: Generative AI}
\begin{itemize}
\item From recognition → generation
\item VAEs, GANs, Diffusion models
\item Large language models
\item Applications in innovation
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/summary_preview.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Neural networks: From solving XOR to generating Shakespeare}
\end{frame}