import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mutual_info_score
from sklearn.feature_selection import mutual_info_regression
from scipy.stats import pearsonr
import seaborn as sns
from itertools import combinations

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic user behavior dataset
n_users = 10000
n_features = 20

# Create base features with controlled correlations
data = np.random.randn(n_users, n_features)

# Create hidden correlations between features
# Strong linear correlations
data[:, 1] = 0.8 * data[:, 0] + 0.2 * np.random.randn(n_users)
data[:, 5] = 0.7 * data[:, 4] + 0.3 * np.random.randn(n_users)
data[:, 9] = -0.75 * data[:, 8] + 0.25 * np.random.randn(n_users)

# Non-linear relationships
data[:, 3] = np.sin(data[:, 2]) + 0.2 * np.random.randn(n_users)
data[:, 7] = data[:, 6]**2 + 0.3 * np.random.randn(n_users)
data[:, 11] = np.exp(-data[:, 10]**2) + 0.2 * np.random.randn(n_users)

# Complex multi-feature interactions
data[:, 13] = data[:, 12] * data[:, 14] + 0.3 * np.random.randn(n_users)
data[:, 16] = np.sin(data[:, 15]) * np.cos(data[:, 17]) + 0.2 * np.random.randn(n_users)
data[:, 19] = (data[:, 18] > 0).astype(float) * data[:, 0] + 0.3 * np.random.randn(n_users)

# Discretize data for mutual information calculation
n_bins = 10
data_discrete = np.zeros_like(data)
for i in range(n_features):
    data_discrete[:, i] = pd.cut(data[:, i], bins=n_bins, labels=False)

# Calculate mutual information between all feature pairs
mi_matrix = np.zeros((n_features, n_features))
for i in range(n_features):
    for j in range(i+1, n_features):
        mi_score = mutual_info_score(data_discrete[:, i].astype(int), 
                                      data_discrete[:, j].astype(int))
        mi_matrix[i, j] = mi_score
        mi_matrix[j, i] = mi_score

# Get all feature pairs sorted by MI
feature_pairs = []
for i in range(n_features):
    for j in range(i+1, n_features):
        feature_pairs.append((i, j, mi_matrix[i, j]))
feature_pairs.sort(key=lambda x: x[2], reverse=True)

# Simulate different discovery methods
methods = {
    'Manual': 5,      # Only top 5 most obvious correlations
    'Statistical': 20, # Top 20 linear correlations found by correlation analysis
    'ML Basic': 50,    # Top 50 pairs found by basic ML
    'ML Advanced': 100, # Top 100 pairs with MI analysis
    'Deep Learning': 190  # All 190 pairs including complex interactions
}

# Calculate cumulative information captured by each method
cumulative_info = {}
for method, n_pairs in methods.items():
    # Sum the MI values for the top n_pairs
    total_mi = sum([pair[2] for pair in feature_pairs[:min(n_pairs, len(feature_pairs))]])
    cumulative_info[method] = total_mi

# Create visualization
plt.figure(figsize=(10, 6))
colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4', '#9467bd']
methods_list = list(cumulative_info.keys())
values = list(cumulative_info.values())

bars = plt.bar(methods_list, values, color=colors, edgecolor='black', linewidth=1.5)

# Add value labels on bars
for bar, value in zip(bars, values):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{value:.2f} bits',
             ha='center', va='bottom', fontsize=10, fontweight='bold')

plt.xlabel('Discovery Method', fontsize=12, fontweight='bold')
plt.ylabel('Cumulative Mutual Information (bits)', fontsize=12, fontweight='bold')
plt.title('Information Discovery: Manual vs ML Approaches\n' + 
          'Real MI calculations on 10,000 users Ã— 20 features', 
          fontsize=14, fontweight='bold')

# Add grid for better readability
plt.grid(axis='y', alpha=0.3, linestyle='--')

# Add annotation explaining the trend
plt.annotate('ML discovers hidden\nnon-linear relationships', 
             xy=(3.5, values[3]), xytext=(3, values[3] + 1),
             arrowprops=dict(arrowstyle='->', color='black', lw=1.5),
             fontsize=10, ha='center')

plt.annotate('Manual only finds\nobvious correlations', 
             xy=(0, values[0]), xytext=(-0.5, values[0] + 2),
             arrowprops=dict(arrowstyle='->', color='black', lw=1.5),
             fontsize=10, ha='center')

plt.tight_layout()

# Save the chart
plt.savefig('charts/ml_impact.pdf', dpi=300, bbox_inches='tight')
plt.savefig('charts/ml_impact.png', dpi=150, bbox_inches='tight')  # Also save PNG for preview
print(f"Chart saved to charts/ml_impact.pdf")

# Print summary statistics
print("\n=== Mutual Information Discovery Summary ===")
print(f"Total features: {n_features}")
print(f"Total feature pairs: {len(feature_pairs)}")
print(f"\nTop 10 discovered relationships (by MI):")
for i, (f1, f2, mi) in enumerate(feature_pairs[:10], 1):
    print(f"{i:2d}. Features {f1:2d} - {f2:2d}: {mi:.4f} bits")

print(f"\nCumulative information by method:")
for method, info in cumulative_info.items():
    print(f"  {method:15s}: {info:6.2f} bits")

# Calculate improvement percentages
baseline = cumulative_info['Manual']
for method, info in cumulative_info.items():
    if method != 'Manual':
        improvement = ((info - baseline) / baseline) * 100
        print(f"  {method:15s}: +{improvement:.1f}% vs Manual")