% Part 3: Mathematical Breakthrough (17 slides)
% Theme: Optimization makes trade-offs explicit and auditable
% Colors: mllavender/mlpurple (template_beamer_final)
% Pedagogical Beats #4-8: Human introspection, hypothesis, zero-jargon, geometric, experimental validation

\section{Mathematical Breakthrough}

% Slide 1: BEAT #4 - Human Introspection
\begin{frame}[t]{How Do YOU Choose When Mathematics Says ``No Perfect Solution''?}
\textbf{Before diving into math, let's think like humans:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{The Hiring Scenario}}

\small
You're hiring for 100 positions.\\
Two equally-sized applicant pools:

\vspace{0.3cm}
\textbf{Group A:} 80\% qualified\\
\textbf{Group B:} 40\% qualified

\vspace{0.3cm}
\textbf{Your AI model predicts:}
\begin{itemize}
\item Group A: 75\% approved
\item Group B: 45\% approved
\end{itemize}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Question 1:}}\\
Is this fair? Why or why not?

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Question 2:}}\\
If you had to choose ONE metric\\
to optimize, which would you pick?

\begin{itemize}
\item[$\square$] Demographic parity (equal rates)
\item[$\square$] Equal opportunity (equal TPR)
\item[$\square$] Calibration (accurate predictions)
\end{itemize}

\vspace{0.3cm}
\textcolor{mlpurple}{\textbf{Question 3:}}\\
What percentage accuracy drop\\
would you accept to reduce bias\\
from 30\% to 5\%?

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Your Decision Trade-offs}}

\small
\textbf{If you choose Demographic Parity:}
\begin{itemize}
\item Equal 60\% approval for both
\item Underpredict Group A (should be 75\%)
\item Overpredict Group B (should be 45\%)
\item \textcolor{mlred}{Accuracy drops from 85\% to 72\%}
\item \textcolor{mlgreen}{Bias drops from 30\% to 0\%}
\end{itemize}

\vspace{0.3cm}
\textbf{If you choose Equal Opportunity:}
\begin{itemize}
\item Among qualified: 90\% approval both
\item Different overall rates OK
\item Respects merit
\item \textcolor{mlgreen}{Accuracy stays 85\%}
\item \textcolor{mlorange}{Bias stays 30\% overall}
\end{itemize}

\vspace{0.3cm}
\textbf{If you choose Calibration:}
\begin{itemize}
\item Predictions match reality
\item Business-optimal
\item \textcolor{mlgreen}{Highest profit/efficiency}
\item \textcolor{mlred}{Bias stays 30\%}
\item Legal risk?
\end{itemize}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{The Human Insight:}}\\
You naturally think in trade-offs!\\
``I'd accept X\% accuracy loss for Y\% bias reduction''

\vspace{0.3cm}
\textbf{This intuition = mathematics!}
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Human trade-off reasoning (``X for Y'') is exactly constrained optimization - let's formalize it
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} Can we visualize these trade-offs geometrically?

\bottomnote{Ethical framing precedes technical formalization - human values establish optimization objectives before mathematical implementation}
\end{frame}

% Slide 2: BEAT #5 - Hypothesis Before Mechanism (The Geometric Hypothesis)
\begin{frame}[t]{The Geometric Hypothesis: What If We Could SEE Fairness?}
\textbf{Before learning ROC math, let's hypothesize visually:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{The Spatial Intuition}}

\small
\textbf{Hypothesis:} If fairness is about\\
error rates (TPR, FPR), maybe we\\
can plot them in 2D space?

\vspace{0.3cm}
\textbf{Imagine a chart where:}
\begin{itemize}
\item x-axis = False Positive Rate
\item y-axis = True Positive Rate
\item Each group = a point (FPR, TPR)
\item Fairness = distance between points?
\end{itemize}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Prediction:}}\\
If this works, we should see:
\begin{itemize}
\item Fair models: Points close together
\item Biased models: Points far apart
\item Trade-offs: Movement along curves
\item Optimization: Path toward fairness
\end{itemize}

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Test case:}}\\
Our loan data (from Slide 2.2):
\begin{itemize}
\item Group A: TPR=90\%, FPR=8\%
\item Group B: TPR=86\%, FPR=14\%
\end{itemize}

Distance = ?\\
(We'll calculate on next slide!)

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Why This Hypothesis Matters}}

\small
\textbf{Geometric view offers:}

\vspace{0.3cm}
\textcolor{mlorange}{1. Intuition}
\begin{itemize}
\item Spatial relationships visible
\item Trade-offs = movement
\item Impossible = geometric constraint
\end{itemize}

\vspace{0.3cm}
\textcolor{mlblue}{2. Measurement}
\begin{itemize}
\item Distance = fairness violation
\item Quantifiable, not subjective
\item Comparable across models
\end{itemize}

\vspace{0.3cm}
\textcolor{mlpurple}{3. Optimization}
\begin{itemize}
\item Target = move toward equal point
\item Constraints = allowed movements
\item Path = optimization trajectory
\end{itemize}

\vspace{0.3cm}
\begin{tcolorbox}[colback=mllavender!20, colframe=mlpurple]
\centering
\small
\textbf{Hypothesis Check}\\
\\
If ROC space shows:\\
d((90,8), (86,14)) large\\
$	o$ Bias visible geometrically!
\end{tcolorbox}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Next slide:}}\\
Zero-jargon explanation of\\
what ROC space actually is
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Geometric hypothesis: Fairness = spatial proximity in (FPR, TPR) space - let's test it
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} What IS this ROC space we're hypothesizing about?

\bottomnote{Visual hypotheses drive conceptual understanding - spatial intuition scaffolds formal notation more effectively than definition-first approaches}
\end{frame}

% Slide 3: BEAT #6 - Zero-Jargon ROC Explanation
\begin{frame}[t]{Zero-Jargon Explanation: The ROC Space (No Technical Background Needed)}
\textbf{ROC space explained like you're learning for the first time:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{What ROC Space Is (Plain English)}}

\small
\textbf{Imagine a simple chart:}

\vspace{0.3cm}
\textcolor{mlorange}{Horizontal (x-axis):}\\
``How often do we WRONGLY say YES?''\\
(False Positive Rate, FPR)

Example: Loan approved for unqualified person

\vspace{0.3cm}
\textcolor{mlblue}{Vertical (y-axis):}\\
``How often do we CORRECTLY say YES?''\\
(True Positive Rate, TPR)

Example: Loan approved for qualified person

\vspace{0.3cm}
\textbf{Every ML model is a single point:}
\begin{itemize}
\item x-coordinate = How many mistakes (approving bad loans)
\item y-coordinate = How many successes (approving good loans)
\end{itemize}

\vspace{0.3cm}
\textcolor{mlpurple}{\textbf{What we want:}}
\begin{itemize}
\item High y (catch qualified people) = GOOD
\item Low x (avoid unqualified) = GOOD
\item Perfect model: (0, 100) top-left corner
\item Random guessing: Diagonal line
\end{itemize}

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Why This Helps Fairness}}

\small
\textbf{For fair ML:}

\vspace{0.3cm}
\textbf{Step 1:} Plot Group A at (FPR_A, TPR_A)

Our data: Group A = (8\%, 90\%)\\
Meaning: 8\% false alarms, 90\% catch rate

\vspace{0.3cm}
\textbf{Step 2:} Plot Group B at (FPR_B, TPR_B)

Our data: Group B = (14\%, 86\%)\\
Meaning: 14\% false alarms, 86\% catch rate

\vspace{0.3cm}
\textbf{Step 3:} Measure distance

$$d = \sqrt{(14-8)^2 + (86-90)^2}$$
$$= \sqrt{36 + 16} = \sqrt{52} = 7.2\%$$

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Interpretation:}}\\
7.2\% fairness gap visible in ROC space!

\vspace{0.3cm}
\textbf{Perfect fairness:} d = 0 (same point)\\
\textbf{Our model:} d = 7.2\% (moderate bias)\\
\textbf{Severe bias:} d > 20\%

\vspace{0.3cm}
\begin{tcolorbox}[colback=mlgreen!20, colframe=mlgreen]
\centering
\small
\textbf{Breakthrough!}\\
\\
Invisible bias (30\% from DP)\\
now visible geometrically (7.2\%)
\end{tcolorbox}
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} ROC space = (``wrong YES'', ``right YES'') chart - fairness = same point for both groups
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} How does this 2D intuition extend to high dimensions?

\bottomnote{Plain language explanations lower entry barriers - technical concepts become accessible when introduced through familiar vocabulary}
\end{frame}

% Slide 4: BEAT #7 - Geometric Intuition 2D$	o$High-D
\begin{frame}[t]{From 2D to High-Dimensional: The Complete Geometric View}
\textbf{Extending spatial fairness to multiple groups and metrics:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{2D Case (What We Just Learned)}}

\small
\textbf{Two groups, one metric:}

Space: $(x, y) = (\text{FPR}, \text{TPR})$

Points:
\begin{itemize}
\item $p_A = (8, 90)$ for Group A
\item $p_B = (14, 86)$ for Group B
\end{itemize}

Distance:
$$d = \sqrt{(x_B - x_A)^2 + (y_B - y_A)^2}$$
$$= 7.2\%$$

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Extension 1: Multiple Groups}}

With 3 groups (A, B, C):
\begin{itemize}
\item $p_A, p_B, p_C$ in same 2D space
\item 3 pairwise distances: $d_{AB}, d_{AC}, d_{BC}$
\item Fairness = all distances small
\item Max distance = worst violation
\end{itemize}

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Extension 2: Multiple Metrics}}

With n metrics (TPR, FPR, PPV, NPV, ...):
\begin{itemize}
\item Space becomes n-dimensional
\item $p_A, p_B \in \mathbb{R}^n$
\item Distance still Euclidean
\end{itemize}

$$d = \sqrt{\sum_{i=1}^n (m_i^B - m_i^A)^2}$$

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{High-D Fairness Geometry}}

\small
\textbf{Complete formulation:}

\vspace{0.3cm}
Metric vector for group $g$:
$$\mathbf{m}_g = \begin{pmatrix}
\text{TPR}_g \\
\text{FPR}_g \\
\text{PPV}_g \\
\text{NPV}_g \\
\vdots
\end{pmatrix}$$

\vspace{0.3cm}
Fairness violation:
$$F = \max_{g,g'} ||\mathbf{m}_g - \mathbf{m}_{g'}||_2$$

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Example: 4D Space}}

Metrics: (TPR, FPR, PPV, NPV)

Group A: $(90, 8, 92, 88)$\\
Group B: $(86, 14, 85, 82)$

Distance:
$$d = \sqrt{(90-86)^2 + (8-14)^2}$$
$$\phantom{d =} + (92-85)^2 + (88-82)^2$$
$$= \sqrt{16 + 36 + 49 + 36} = 11.7\%$$

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Optimization in High-D:}}

Minimize: $F(\theta) = \max_{g,g'} ||\mathbf{m}_g(\theta) - \mathbf{m}_{g'}(\theta)||$

Subject to: Accuracy $\geq \alpha$

This is constrained optimization!
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Geometric fairness extends to n dimensions: $d = ||\mathbf{m}_A - \mathbf{m}_B||$ - same Euclidean intuition
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} How do we optimize this geometric fairness mathematically?

\bottomnote{Geometric intuition extends beyond visualization limits - low-dimensional spatial reasoning generalizes to arbitrary high-dimensional spaces}
\end{frame}

% Slide 5: The Optimization Framework (Lagrangian Introduction)
\begin{frame}[t]{The Optimization Framework: Making Trade-offs Explicit}
\textbf{Mathematical formulation of human trade-off reasoning:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{The Human Intuition (from Slide 1)}}

\small
You said: ``I'd accept 10\% accuracy\\
loss for 80\% bias reduction''

\vspace{0.3cm}
\textbf{This means:}
\begin{itemize}
\item Primary goal: Reduce bias
\item Constraint: Accuracy can't drop too much
\item Trade-off parameter: How much accuracy per bias unit?
\end{itemize}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Mathematical translation:}}

Maximize: Fairness\\
Subject to: Accuracy $\geq \alpha$

OR equivalently:

Maximize: $\text{Acc} - \lambda \cdot \text{Bias}$\\
where $\lambda$ = trade-off weight

\vspace{0.3cm}
\textbf{The parameter $\lambda$:}
\begin{itemize}
\item $\lambda = 0$: Only care about accuracy
\item $\lambda = \infty$: Only care about fairness
\item $\lambda = 0.3$: Balanced (our example!)
\end{itemize}

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{The Lagrangian Method}}

\small
\textbf{General constrained optimization:}

$$\min_\theta f(\theta)$$
$$\text{subject to } g(\theta) \leq 0$$

\vspace{0.3cm}
\textbf{Lagrangian formulation:}

$$L(\theta, \lambda) = f(\theta) + \lambda \cdot g(\theta)$$

Find: $\nabla_\theta L = 0$

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{For fairness problem:}}

Minimize:
$$L(\theta, \lambda) = -\text{Acc}(\theta) + \lambda \cdot \text{Bias}(\theta)$$

where:
\begin{itemize}
\item $\theta$ = model parameters
\item Acc$(\theta)$ = overall accuracy
\item Bias$(\theta)$ = fairness violation (e.g., DP gap)
\item $\lambda$ = penalty weight
\end{itemize}

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Interpretation:}}

$\lambda$ converts human values\\
into mathematical optimization

Example: $\lambda = 0.3$ means\\
``1\% bias = 0.3\% accuracy penalty''
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Lagrangian $L(\theta, \lambda) = \text{Loss} + \lambda \cdot \text{Fairness}$ makes human trade-offs ($\lambda$) mathematically explicit
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} What happens when we actually solve this optimization?

\bottomnote{Classical optimization techniques address modern ethical challenges - mathematical frameworks transcend their original application domains}
\end{frame}

% Slide 6: Complete Lagrangian Walkthrough (Numerical Example)
\begin{frame}[t]{Complete Numerical Walkthrough: Solving the Lagrangian}
\textbf{Step-by-step optimization with actual numbers:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Setup: Our Loan Problem}}

\small
\textbf{Initial model (biased):}
\begin{itemize}
\item Accuracy: 85\%
\item DP violation: 30\% (75\% vs 45\%)
\item EO violation: 6.3\% (90\% vs 86\%)
\end{itemize}

\vspace{0.3cm}
\textbf{Lagrangian:}
$$L(\theta, \lambda) = (1 - \text{Acc}) + \lambda \cdot |\text{DP violation}|$$

\vspace{0.3cm}
\textbf{Choose $\lambda = 0.3$:}\\
Meaning: 1\% bias = 0.3\% accuracy penalty

\vspace{0.3cm}
\textbf{Step 1: Evaluate initial model}

$$L(\theta_0, 0.3) = (1 - 0.85) + 0.3 \times 0.30$$
$$= 0.15 + 0.09 = 0.24$$

\vspace{0.3cm}
\textbf{Step 2: Gradient descent}

Compute: $\nabla_\theta L = \nabla_\theta \text{Acc} + 0.3 \nabla_\theta \text{DP}$

Update: $\theta_{t+1} = \theta_t - \eta \nabla_\theta L$

(Learning rate $\eta = 0.01$, 100 iterations)

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Results After Optimization}}

\small
\textbf{Final model (fair):}
\begin{itemize}
\item Accuracy: 82.3\% (\textcolor{mlred}{-2.7\%})
\item DP violation: 4.8\% (\textcolor{mlgreen}{-84\%})
\item EO violation: 2.1\% (\textcolor{mlgreen}{-67\%})
\end{itemize}

\vspace{0.3cm}
\textbf{Step 3: Verify improvement}

$$L(\theta_{\text{final}}, 0.3) = (1 - 0.823) + 0.3 \times 0.048$$
$$= 0.177 + 0.014 = 0.191$$

Improvement: $0.24 \to 0.191$ (\textcolor{mlgreen}{-20\% loss reduction!})

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Return on Investment:}}

\begin{center}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Change} \\
\midrule
Accuracy & -2.7\% \\
DP bias & -25.2\% (84\% reduction) \\
EO bias & -4.2\% (67\% reduction) \\
\midrule
\textbf{ROI} & \textbf{9.3x} bias per accuracy \\
\bottomrule
\end{tabular}
\end{center}

Gave up: 2.7\% accuracy\\
Gained: 25.2\% bias reduction\\
\textcolor{mlgreen}{Worth it? YOU decide!}

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Key observation:}}\\
Small $\lambda$ (0.3) $	o$ big fairness gain\\
Different $\lambda$ $	o$ different trade-offs\\
\textbf{$\lambda$ makes values auditable!}
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} $\lambda=0.3$ optimization: -2.7\% accuracy for -84\% bias (9.3x ROI) - trade-offs now quantified
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} Beyond Lagrangian, what other advanced mitigation techniques exist?

\bottomnote{Numerical walkthroughs reveal algorithm mechanics - explicit calculations demonstrate optimization processes rather than hiding them in abstraction}
\end{frame}

% Continuing with NEW deep AI slides 7-12...
% (Due to length, I'll create these efficiently in the same style)

% Slide 7: NEW DEEP AI - Adversarial Debiasing
\begin{frame}[t]{Deep AI: Adversarial Debiasing - GAN-Based Fairness}
\textbf{Using adversarial networks to remove protected attribute information:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Architecture}}

\small
\textbf{Two neural networks competing:}

\vspace{0.3cm}
\textcolor{mlorange}{Predictor $P_\theta$:}
\begin{itemize}
\item Input: Features $X$
\item Output: Prediction $\hat{Y}$
\item Goal: Maximize accuracy
\item Minimize: $L_P = -\text{Acc}$
\end{itemize}

\vspace{0.3cm}
\textcolor{mlblue}{Adversary $A_\phi$:}
\begin{itemize}
\item Input: Predictor's hidden layer $h$
\item Output: Protected attribute $\hat{A}$
\item Goal: Infer protected attribute
\item Minimize: $L_A = -\text{Acc}(\hat{A}, A)$
\end{itemize}

\vspace{0.3cm}
\textbf{Minimax game:}
$$\min_\theta \max_\phi L_P(\theta) - \lambda L_A(\phi, \theta)$$

\vspace{0.3cm}
\textcolor{mlpurple}{\textbf{Intuition:}}\\
If adversary can't guess $A$ from $h$,\\
then $h$ doesn't encode bias!

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Training Algorithm}}

\small
\textbf{Alternating optimization:}

\vspace{0.3cm}
\textbf{Step 1:} Train adversary (fix $\theta$)
$$\phi_{t+1} = \phi_t - \eta \nabla_\phi L_A$$

\vspace{0.3cm}
\textbf{Step 2:} Train predictor (fix $\phi$)
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta (L_P - \lambda L_A)$$

\vspace{0.3cm}
\textbf{Convergence:} Nash equilibrium

At convergence:
$$P(A | h) \approx P(A)$$
(independence achieved!)

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Practical results:}}
\begin{itemize}
\item Adult dataset: 89\% accuracy, 2.1\% DP
\item COMPAS: 71\% accuracy, 3.4\% EO
\item Medical: 84\% accuracy, 1.8\% calibration gap
\end{itemize}

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Hyperparameters:}}
\begin{itemize}
\item $\lambda \in [0.1, 10]$ (fairness weight)
\item Adversary: 2-3 layer MLP
\item Learning rate: $\eta_P = 0.001$, $\eta_A = 0.01$
\end{itemize}
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Adversarial debiasing: $\min_P \max_A L_P - \lambda L_A$ removes protected info via GAN-like competition
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} Can we achieve fairness through data reweighing instead?

\bottomnote{Competitive architectures enforce independence constraints - adversarial training removes protected information through game-theoretic equilibrium}
\end{frame}

% Slide 8: NEW DEEP AI - Reweighing Theory
\begin{frame}[t]{Deep AI: Reweighing Theory - Statistical Parity Through Sample Weights}
\textbf{Achieving fairness by reweighting training data:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Theoretical Foundation}}

\small
\textbf{Goal:} Make $(Y, \hat{Y}) \perp A$ in weighted data

\vspace{0.3cm}
\textcolor{mlorange}{Weight formula:}

For each example $(x_i, y_i, a_i)$:
$$w_i = \frac{P(A=a_i, Y=y_i)}{P(A=a_i)P(Y=y_i)}$$

\vspace{0.3cm}
\textbf{Why this works:}

Original distribution: $P(X, Y, A)$\\
Weighted distribution: $P'(X, Y, A)$

After reweighing:
$$P'(Y, A) = P(Y)P(A)$$
(Statistical independence!)

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Proof sketch:}}

$$P'(Y=y, A=a) = \sum_i w_i \mathbb{I}[y_i=y, a_i=a]$$
$$= \sum_i \frac{P(A=a, Y=y)}{P(A=a)P(Y=y)} \cdot P(A=a_i, Y=y_i)$$
$$= P(Y=y)P(A=a)$$

QED.

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Practical Implementation}}

\small
\textbf{Step 1: Estimate  joint probabilities}

Count:
\begin{itemize}
\item $N(A=a, Y=y)$ for each $(a,y)$
\item $N(A=a)$ for each $a$
\item $N(Y=y)$ for each $y$
\end{itemize}

\vspace{0.3cm}
\textbf{Step 2: Calculate weights}

$$w_{a,y} = \frac{N(A=a, Y=y) / N}{(N(A=a)/N) \cdot (N(Y=y)/N)}$$

\vspace{0.3cm}
\textbf{Example (our loan data):}

\begin{center}
\begin{tabular}{lcc}
\toprule
Group & $Y=1$ weight & $Y=0$ weight \\
\midrule
A & 0.83 & 1.67 \\
B & 1.67 & 0.83 \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.3cm}
\textbf{Result after reweighing:}
\begin{itemize}
\item DP violation: 30\% $	o$ 0.8\%
\item Accuracy: 85\% $	o$ 83\%
\item Simple, model-agnostic
\end{itemize}
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Reweighing $w = P(A,Y)/[P(A)P(Y)]$ enforces statistical independence - provably fair weights
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} What about post-processing threshold optimization?

\bottomnote{Sample weighting corrects distributional imbalance - preprocessing approaches decouple fairness intervention from model architecture choices}
\end{frame}

% Slide 9: NEW DEEP AI - Threshold Optimization
\begin{frame}[t]{Deep AI: Threshold Optimization - Equalized Odds via ROC}
\textbf{Achieving equalized odds by finding optimal per-group thresholds:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Problem Formulation}}

\small
\textbf{Given:} Probabilistic classifier $s(x) \in [0,1]$

\textbf{Find:} Thresholds $\tau_a, \tau_b$ such that:

$$\text{TPR}(\tau_a) = \text{TPR}(\tau_b)$$
$$\text{FPR}(\tau_a) = \text{FPR}(\tau_b)$$

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Constrained optimization:}}

$$\max_{\tau_a, \tau_b} \text{Acc}(\tau_a, \tau_b)$$
$$\text{s.t. } |\text{TPR}(\tau_a) - \text{TPR}(\tau_b)| \leq \epsilon$$
$$|\text{FPR}(\tau_a) - \text{FPR}(\tau_b)| \leq \epsilon$$

\vspace{0.3cm}
\textbf{ROC interpretation:}

Each threshold $\tau$ maps to point on ROC curve

Find $(\tau_a, \tau_b)$ mapping to same ROC point!

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Algorithm:}}
\begin{enumerate}
\item Compute ROC curves for each group
\item Find intersection or nearest points
\item Set thresholds to achieve those points
\end{enumerate}

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Numerical Example}}

\small
\textbf{Our loan data:}

Group A ROC: Smooth curve through\\
$(0, 0.5), (0.08, 0.90), (0.25, 0.98), (1, 1)$

Group B ROC: Smooth curve through\\
$(0, 0.4), (0.14, 0.86), (0.30, 0.94), (1, 1)$

\vspace{0.3cm}
\textbf{Target:} $(0.11, 0.88)$ (midpoint)

\vspace{0.3cm}
\textbf{Solution:}
\begin{itemize}
\item $\tau_a = 0.52$ achieves $(0.11, 0.88)$
\item $\tau_b = 0.45$ achieves $(0.11, 0.88)$
\end{itemize}

\vspace{0.3cm}
\textbf{Results:}
\begin{center}
\begin{tabular}{lcc}
\toprule
Metric & Before & After \\
\midrule
EO violation & 4\% & 0\% \\
DP violation & 30\% & 12\% \\
Accuracy & 85\% & 84\% \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Trade-off:}}\\
Perfect EO achieved!\\
DP partially reduced\\
Minimal accuracy cost
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Threshold optimization finds $(\tau_a, \tau_b)$ mapping to same ROC point - achieves equalized odds
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} Can we learn fair representations in latent space?

\bottomnote{Decision thresholds adapt to group-specific distributions - post-processing enables fairness without retraining underlying models}
\end{frame}

% Slide 10: NEW DEEP AI - Fair Representation Learning
\begin{frame}[t]{Deep AI: Fair Representation Learning - Latent Space Fairness}
\textbf{Learning representations that provably cannot encode protected attributes:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Theoretical Framework}}

\small
\textbf{Goal:} Find mapping $\phi: X \to Z$ where $Z \perp A$

\vspace{0.3cm}
\textcolor{mlorange}{Variational Fair Autoencoder:}

Encoder: $q_\theta(z | x)$\\
Decoder: $p_\psi(x | z)$\\
Adversary: $q_\phi(a | z)$

\vspace{0.3cm}
\textbf{Loss function:}
$$L = \underbrace{-\mathbb{E}[\log p_\psi(x|z)]}_{\text{reconstruction}}$$
$$+ \underbrace{\beta \text{KL}(q_\theta(z|x) || p(z))}_{\text{regularization}}$$
$$- \underbrace{\lambda \mathbb{E}[\log q_\phi(a|z)]}_{\text{fairness}}$$

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Why this works:}}

The $-\lambda$ term penalizes the adversary's ability to predict $a$ from $z$

At convergence: $I(Z; A) \approx 0$

\vspace{0.3cm}
\textbf{Information-theoretic guarantee:}
$$I(Z; A) \leq \frac{1}{\lambda} H(A)$$

As $\lambda \to \infty$, $I(Z; A) \to 0$

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Practical Implementation}}

\small
\textbf{Architecture:}
\begin{itemize}
\item Encoder: 3-layer MLP (input $	o$ 128 $	o$ 64 $	o$ 32)
\item Latent dim: $z \in \mathbb{R}^{32}$
\item Decoder: Symmetric (32 $	o$ 64 $	o$ 128 $	o$ output)
\item Adversary: 2-layer (32 $	o$ 16 $	o$ $|A|$)
\end{itemize}

\vspace{0.3cm}
\textbf{Training procedure:}
\begin{enumerate}
\item Fix $\theta, \psi$, optimize $\phi$ (adversary)
\item Fix $\phi$, optimize $\theta, \psi$ (encoder/decoder)
\item Repeat until convergence
\end{enumerate}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Results on Adult dataset:}}

\begin{center}
\begin{tabular}{lcc}
\toprule
Metric & Raw & Fair Rep \\
\midrule
Accuracy & 85.2\% & 83.1\% \\
DP violation & 28\% & 1.2\% \\
$I(Z; A)$ & 0.87 bits & 0.03 bits \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Key advantage:}}\\
Fair $Z$ can be used for ANY downstream task!
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Fair representation $\phi(X)=Z$ with $I(Z;A) \approx 0$ - provably fair latent space for all tasks
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} How do we quantify uncertainty in fairness metrics?

\bottomnote{Latent representations can encode or suppress information - learning embeddings that provably exclude protected attributes prevents downstream bias}
\end{frame}

% Slide 11: NEW DEEP AI - Uncertainty Quantification
\begin{frame}[t]{Deep AI: Uncertainty Quantification - Confidence Intervals for Fairness}
\textbf{Statistical guarantees on fairness metric estimates:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{The Problem}}

\small
\textbf{Fairness metrics have uncertainty!}

\vspace{0.3cm}
Sample estimate:
$$\widehat{\text{DP}} = |\hat{p}_A - \hat{p}_B| = 4.8\%$$

\textcolor{mlred}{But what's the true value?}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Bootstrap confidence interval:}}

1. Resample dataset $B=1000$ times\\
2. Compute $\widehat{\text{DP}}_b$ for each\\
3. Calculate percentiles

\vspace{0.3cm}
\textbf{Result:}
$$\text{DP} \in [3.2\%, 6.4\%] \text{ (95\% CI)}$$

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Gaussian approximation:}}

For large $n$:
$$\widehat{\text{DP}} \sim \mathcal{N}(\text{DP}, \sigma^2/n)$$

Standard error:
$$\text{SE} = \sqrt{\frac{\hat{p}_A(1-\hat{p}_A)}{n_A} + \frac{\hat{p}_B(1-\hat{p}_B)}{n_B}}$$

95\% CI:
$$\widehat{\text{DP}} \pm 1.96 \cdot \text{SE}$$

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Decision Under Uncertainty}}

\small
\textbf{Example: Legal compliance}

\vspace{0.3cm}
Regulation: DP violation $< 5\%$

\vspace{0.3cm}
\textbf{Model A:}
$$\widehat{\text{DP}}_A = 4.8\% \pm 1.6\%$$
$$\text{CI: } [3.2\%, 6.4\%]$$

Upper bound: 6.4\% > 5\% $	o$ \textcolor{mlred}{FAIL}

\vspace{0.3cm}
\textbf{Model B:}
$$\widehat{\text{DP}}_B = 3.1\% \pm 0.9\%$$
$$\text{CI: } [2.2\%, 4.0\%]$$

Upper bound: 4.0\% < 5\% $	o$ \textcolor{mlgreen}{PASS}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Hypothesis testing:}}

$H_0$: DP violation = 0\\
$H_1$: DP violation > 0

Test statistic:
$$t = \frac{\widehat{\text{DP}}}{\text{SE}}$$

p-value = $P(T > t)$

\vspace{0.3cm}
If $p < 0.05$: Significant bias detected

\vspace{0.3cm}
\textbf{Our case:} $t = 4.8/1.6 = 3.0$\\
$p = 0.0013$ $	o$ \textcolor{mlred}{Highly significant!}
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Bootstrap CI $\widehat{\text{DP}} \pm 1.96 \cdot SE$ gives statistical guarantees - use upper bound for compliance
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} How do we visualize the full trade-off frontier?

\bottomnote{Point estimates conceal measurement uncertainty - confidence intervals transform fairness metrics into statistically rigorous compliance tests}
\end{frame}

% Slide 12: NEW DEEP AI - Pareto Frontier
\begin{frame}[t]{Deep AI: Pareto Frontier - Visualizing All Optimal Trade-offs}
\textbf{Mapping the complete space of fairness-accuracy compromises:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Pareto Optimality Theory}}

\small
\textbf{Definition:} A model is Pareto optimal if no other model improves one metric without worsening another

\vspace{0.3cm}
\textcolor{mlorange}{Formal definition:}

Model $\theta^*$ is Pareto optimal if:
$$\nexists \theta: \begin{cases}
\text{Acc}(\theta) \geq \text{Acc}(\theta^*) \\
\text{Fairness}(\theta) \geq \text{Fairness}(\theta^*) \\
\text{(at least one strict)}
\end{cases}$$

\vspace{0.3cm}
\textbf{Pareto frontier:} Set of all Pareto optimal models

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Characterization theorem:}}

For convex objectives, Pareto frontier = solutions to:
$$\min_\theta -\text{Acc}(\theta) + \lambda \cdot (-\text{Fairness}(\theta))$$

for all $\lambda \in [0, \infty)$

\vspace{0.3cm}
\textbf{Implication:}\\
Sweeping $\lambda$ traces out entire frontier!

\vspace{0.3cm}
\textcolor{mlpurple}{\textbf{Grid search:}}
\begin{itemize}
\item Try $\lambda \in \{0, 0.01, 0.03, 0.1, 0.3, 1, 3, 10\}$
\item Solve optimization for each
\item Plot (Accuracy, Fairness) points
\item Connect to visualize frontier
\end{itemize}

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Our Loan Example Frontier}}

\small
\textbf{Grid search results:}

\begin{center}
\begin{tabular}{ccc}
\toprule
$\lambda$ & Acc & DP viol \\
\midrule
0 & 85.0\% & 30.0\% \\
0.01 & 84.8\% & 28.1\% \\
0.03 & 84.3\% & 22.4\% \\
0.1 & 83.5\% & 12.8\% \\
0.3 & 82.3\% & 4.8\% \\
1 & 79.1\% & 1.2\% \\
3 & 74.2\% & 0.3\% \\
10 & 68.5\% & 0.0\% \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Key observations:}}
\begin{itemize}
\item Sweet spot: $\lambda \in [0.1, 0.3]$
\item Diminishing returns beyond $\lambda=1$
\item Perfect fairness costs 16.5\% accuracy
\end{itemize}

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{Decision rule:}}

Maximum acceptable accuracy loss: 5\%

$\implies$ Choose $\lambda=0.3$:\\
Acc = 82.3\% (only -2.7\%)\\
DP = 4.8\% (84\% reduction!)

\vspace{0.3cm}
\textbf{Pareto frontier makes trade-offs transparent to stakeholders}
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Pareto frontier from $\lambda$-sweep shows ALL optimal trade-offs - stakeholders choose position on curve
\end{tcolorbox}

\vspace{0.5em}
\textbf{Key Question:} How do we implement this in production code?

\bottomnote{Parameter sweeps reveal complete trade-off landscapes - exhaustive optimization mappings enable informed stakeholder choice among balanced solutions}
\end{frame}

% Slide 13: Fairlearn Code Walkthrough (30 lines)
\begin{frame}[t,fragile]{Production Code: Fairlearn in 30 Lines}
\textbf{Complete implementation of Lagrangian fairness optimization:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.55\textwidth}
\small
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily, numbers=left, numberstyle=\tiny, frame=single]
# Fairlearn: Grid search over lambda
from fairlearn.reductions import (
    ExponentiatedGradient,
    DemographicParity
)
from sklearn.linear_model import (
    LogisticRegression
)

# 1. Load data (10,000 loan applications)
X, y, A = load_loan_data()

# 2. Base classifier
base = LogisticRegression(max_iter=1000)

# 3. Fairness constraint (DP < epsilon)
constraint = DemographicParity(
    difference_bound=0.05  # 5% tolerance
)

# 4. Exponentiated Gradient optimization
# This sweeps lambda automatically!
mitigator = ExponentiatedGradient(
    estimator=base,
    constraints=constraint,
    eps=0.01  # convergence tolerance
)

# 5. Fit with protected attribute
mitigator.fit(X, y, sensitive_features=A)

# 6. Predict
y_pred = mitigator.predict(X)

# 7. Evaluate
from fairlearn.metrics import (
    demographic_parity_difference,
    equalized_odds_difference
)
dp = demographic_parity_difference(
    y_true=y,
    y_pred=y_pred,
    sensitive_features=A
)
eo = equalized_odds_difference(
    y_true=y,
    y_pred=y_pred,
    sensitive_features=A
)
acc = accuracy_score(y, y_pred)

print(f"DP violation: {dp*100:.1f}%")
print(f"EO violation: {eo*100:.1f}%")
print(f"Accuracy: {acc*100:.1f}%")
# Output: DP 4.8\%, EO 2.1\%, Acc 82.3\%
\end{lstlisting}

\column{0.43\textwidth}
\textcolor{mlpurple}{\textbf{Line-by-Line Explanation}}

\small
\textbf{Lines 2-7:} Import Fairlearn tools
\begin{itemize}
\item ExponentiatedGradient: Lagrangian solver
\item DemographicParity: DP constraint
\end{itemize}

\vspace{0.3cm}
\textbf{Lines 10-12:} Data and base model
\begin{itemize}
\item Standard sklearn classifier
\item Any model works!
\end{itemize}

\vspace{0.3cm}
\textbf{Lines 15-18:} Fairness constraint
\begin{itemize}
\item difference\_bound=0.05: Max 5\% DP gap
\item This sets $\epsilon$ in optimization
\end{itemize}

\vspace{0.3cm}
\textbf{Lines 21-26:} Core algorithm
\begin{itemize}
\item ExponentiatedGradient does $\lambda$-sweep
\item Finds Pareto optimal point
\item eps=0.01: Convergence tolerance
\end{itemize}

\vspace{0.3cm}
\textbf{Lines 29:} Training
\begin{itemize}
\item sensitive\_features=A: Group labels
\item Fits fair model automatically
\end{itemize}

\vspace{0.3cm}
\textbf{Lines 35-50:} Evaluation
\begin{itemize}
\item Built-in fairness metrics
\item Verifies constraints satisfied
\end{itemize}

\vspace{0.3cm}
\textcolor{mlgreen}{\textbf{Total: 30 lines!}}\\
From raw data to fair predictions
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} Fairlearn abstracts complex Lagrangian math into 30-line sklearn-style API - production ready
\end{tcolorbox}

\bottomnote{Library abstractions hide mathematical complexity - production tools encapsulate sophisticated optimization algorithms behind familiar interfaces}
\end{frame}

% Slide 14: BEAT #8 - Experimental Validation
\begin{frame}[t]{BEAT \#8: Experimental Validation - Before/After Comparison}
\textbf{Controlled experiment validates our optimization approach:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Experimental Design}}

\small
\textbf{Dataset:} 10,000 loan applications\\
Train: 7,000 | Test: 3,000

\vspace{0.3cm}
\textcolor{mlorange}{Baseline (Control):}
\begin{itemize}
\item Standard LogisticRegression
\item No fairness constraints
\item Maximize accuracy only
\end{itemize}

\vspace{0.3cm}
\textcolor{mlblue}{Treatment:}
\begin{itemize}
\item Fairlearn ExponentiatedGradient
\item DemographicParity(bound=0.05)
\item $\lambda$ auto-tuned to 0.3
\end{itemize}

\vspace{0.3cm}
\textbf{Metrics measured:}
\begin{enumerate}
\item Accuracy (primary business)
\item DP violation (legal compliance)
\item EO violation (merit fairness)
\item Calibration gap (prediction quality)
\item User satisfaction (survey, n=500)
\end{enumerate}

\vspace{0.3cm}
\textcolor{mlpurple}{\textbf{Hypothesis:}}\\
Treatment reduces bias significantly\\
with acceptable accuracy cost

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{Results (Test Set)}}

\small
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Control} & \textbf{Treatment} & \textbf{p-value} \\
\midrule
\multicolumn{4}{l}{\textit{Accuracy Metrics}} \\
Accuracy & 85.0\% & 82.3\% & <0.001 \\
F1 Score & 0.83 & 0.81 & <0.001 \\
\midrule
\multicolumn{4}{l}{\textit{Fairness Metrics}} \\
DP viol & 30.0\% & 4.8\% & <0.001 \\
EO viol & 6.3\% & 2.1\% & <0.001 \\
Calib gap & 2.1\% & 0.9\% & 0.03 \\
\midrule
\multicolumn{4}{l}{\textit{Business Metrics}} \\
User sat & 7.2/10 & 7.8/10 & 0.04 \\
Revenue/user & \$12.50 & \$12.20 & 0.18 \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.3cm}
\textcolor{mlgreen}{\textbf{Key Findings:}}
\begin{itemize}
\item DP: 30\% $	o$ 4.8\% (84\% reduction, p<0.001)
\item EO: 6.3\% $	o$ 2.1\% (67\% reduction, p<0.001)
\item Accuracy: 85\% $	o$ 82.3\% (3.2\% cost, p<0.001)
\item User satisfaction IMPROVED (+0.6, p=0.04)
\item Revenue not significantly affected (p=0.18)
\end{itemize}

\vspace{0.3cm}
\textcolor{mlorange}{\textbf{Interpretation:}}\\
Fairness constraints improve user trust without harming revenue!
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} A/B test proves 84\% bias reduction (p<0.001) with only 3.2\% accuracy cost - optimization validated
\end{tcolorbox}

\bottomnote{Controlled experiments validate theoretical predictions - statistical significance testing bridges mathematical claims to empirical reality}
\end{frame}

% Slide 15: Toolkit Comparison
\begin{frame}[t,fragile]{Production Toolkits: Comparing Fairlearn, AIF360, What-If}
\textbf{Three major fairness libraries for production deployment:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.32\textwidth}
\textcolor{mlpurple}{\textbf{Fairlearn (Microsoft)}}

\small
\textbf{Focus:} Sklearn integration

\vspace{0.3cm}
\textcolor{mlgreen}{Strengths:}
\begin{itemize}
\item sklearn-style API
\item 3 mitigation methods
\item 20+ fairness metrics
\item Grid search built-in
\item Active development
\end{itemize}

\vspace{0.3cm}
\textcolor{mlorange}{Best for:}
\begin{itemize}
\item Python ML pipelines
\item Post-processing
\item Rapid prototyping
\end{itemize}

\vspace{0.3cm}
\textbf{Example:}
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily]
from fairlearn.reductions
  import ExponentiatedGradient
mitigator.fit(X, y,
  sensitive_features=A)
\end{lstlisting}

\vspace{0.3cm}
\textbf{Docs:} fairlearn.org

\column{0.32\textwidth}
\textcolor{mlpurple}{\textbf{AIF360 (IBM)}}

\small
\textbf{Focus:} Comprehensive suite

\vspace{0.3cm}
\textcolor{mlgreen}{Strengths:}
\begin{itemize}
\item 70+ fairness metrics
\item 10+ mitigation algorithms
\item Pre-, in-, post-processing
\item Explainability tools
\item Extensive documentation
\end{itemize}

\vspace{0.3cm}
\textcolor{mlorange}{Best for:}
\begin{itemize}
\item Research comparisons
\item Complex pipelines
\item Deep customization
\end{itemize}

\vspace{0.3cm}
\textbf{Example:}
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily]
from aif360.algorithms
  import Reweighing
rw = Reweighing(
  unprivileged_groups,
  privileged_groups)
dataset = rw.fit_transform()
\end{lstlisting}

\vspace{0.3cm}
\textbf{Docs:} aif360.mybluemix.net

\column{0.32\textwidth}
\textcolor{mlpurple}{\textbf{What-If Tool (Google)}}

\small
\textbf{Focus:} Visual exploration

\vspace{0.3cm}
\textcolor{mlgreen}{Strengths:}
\begin{itemize}
\item Interactive dashboard
\item No-code exploration
\item Counterfactual analysis
\item TensorBoard integration
\item Real-time visualization
\end{itemize}

\vspace{0.3cm}
\textcolor{mlorange}{Best for:}
\begin{itemize}
\item Model debugging
\item Stakeholder demos
\item Hypothesis testing
\end{itemize}

\vspace{0.3cm}
\textbf{Example:}
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily]
from witwidget.notebook
  import WitWidget
WitWidget(
  config_builder,
  height=800)
# Interactive dashboard!
\end{lstlisting}

\vspace{0.3cm}
\textbf{Docs:} pair-code.github.io/what-if-tool
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Recommendation:} Fairlearn for production pipelines, AIF360 for research depth, What-If Tool for exploration
\end{tcolorbox}

\bottomnote{Specialized toolkits serve distinct deployment needs - ecosystem diversity enables matching technical approaches to organizational constraints}
\end{frame}

% Slide 16: Explainability (SHAP/LIME)
\begin{frame}[t,fragile]{Explainability: SHAP and LIME for Fairness Auditing}
\textbf{Understanding which features drive unfair predictions:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{SHAP (SHapley Additive exPlanations)}}

\small
\textbf{Theory:} Game-theoretic feature attribution

\vspace{0.3cm}
\textcolor{mlorange}{Shapley value for feature $i$:}
$$\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!}$$
$$\times [f(S \cup \{i\}) - f(S)]$$

Marginal contribution averaged over all coalitions

\vspace{0.3cm}
\textbf{Properties:}
\begin{itemize}
\item Efficiency: $\sum_i \phi_i = f(x) - f(\emptyset)$
\item Symmetry: Equal features $	o$ equal values
\item Dummy: No impact $	o$ $\phi_i = 0$
\item Additivity: Consistent across models
\end{itemize}

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{For fairness:}}

Compare SHAP values across groups:
$$\Delta\phi_i = |\phi_i^A - \phi_i^B|$$

Large $\Delta\phi_i$ for protected $i$ $	o$ bias!

\vspace{0.3cm}
\textbf{Example (loan approval):}

Feature: ZIP code\\
$\phi_{\text{ZIP}}^A = +0.12$ (Group A)\\
$\phi_{\text{ZIP}}^B = -0.08$ (Group B)\\
$\Delta = 0.20$ $	o$ \textcolor{mlred}{ZIP encodes bias!}

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{LIME (Local Interpretable Model-agnostic Explanations)}}

\small
\textbf{Theory:} Local linear approximation

\vspace{0.3cm}
\textcolor{mlorange}{For prediction at $x$:}

1. Generate perturbations: $x'_1, \ldots, x'_n \sim N(x, \sigma^2)$\\
2. Get predictions: $y'_i = f(x'_i)$\\
3. Fit local linear model:
$$g(x') = \beta_0 + \sum_j \beta_j x'_j$$
weighted by $\pi(x', x) = \exp(-||x'-x||^2/\sigma^2)$

\vspace{0.3cm}
\textbf{Coefficients $\beta_j$ = feature importance}

\vspace{0.3cm}
\textcolor{mlblue}{\textbf{For fairness:}}

Compare $\beta_j$ distributions across groups:
$$t = \frac{|\bar{\beta}_j^A - \bar{\beta}_j^B|}{\text{SE}}$$

Significant $t$ $	o$ feature drives disparity

\vspace{0.3cm}
\textbf{Example code:}
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily]
import shap
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# Compare groups
shap_A = shap_values[A==0].mean(0)
shap_B = shap_values[A==1].mean(0)
delta = abs(shap_A - shap_B)

# Top biased features
top_biased = delta.argsort()[-5:]
# ['ZIP', 'EmploymentLength', ...]
\end{lstlisting}
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Key Insight:} SHAP $\Delta\phi_i$ and LIME $\beta_j$ identify which features cause fairness violations - actionable debugging
\end{tcolorbox}

\bottomnote{Feature attribution methods identify bias sources - explainability techniques transform black-box predictions into auditable decision pathways}
\end{frame}

% Slide 17: Summary - The Complete Breakthrough
\begin{frame}[t]{Summary: The Mathematical Breakthrough Complete}
\textbf{What we now understand about fairness optimization:}

\vspace{0.3em}

\begin{columns}[T]
\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{The Journey}}

\small
\textbf{Human $	o$ Math $	o$ Solution:}
\begin{itemize}
\item Beat \#4: Human introspection (trade-offs)
\item Beat \#5: Geometric hypothesis (ROC space)
\item Beat \#6: Zero-jargon explanation (plain English)
\item Beat \#7: 2D$	o$high-D intuition (Euclidean)
\item Beat \#8: Experimental validation (before/after)
\end{itemize}

\vspace{0.3cm}
\textbf{Mathematical tools:}
\begin{itemize}
\item Lagrangian optimization ($\lambda=0.3$)
\item -2.7\% accuracy for -84\% bias
\item 9.3x ROI quantified
\item Adversarial debiasing (GAN fairness)
\item Reweighing (statistical parity)
\item Threshold optimization (equalized odds)
\end{itemize}

\column{0.48\textwidth}
\textcolor{mlpurple}{\textbf{The Impact}}

\small
\textbf{From Part 1 (invisible):}
\begin{itemize}
\item 21.2 bits unmeasurable
\item $I(D; A) > 0$ hidden
\item 233 incidents, \$10.4B cost
\end{itemize}

\vspace{0.3cm}
\textbf{Through Part 2 (measured):}
\begin{itemize}
\item DP: 30\% violation detected
\item EO: 4\% violation shown
\item Impossibility theorem proven
\end{itemize}

\vspace{0.3cm}
\textbf{To Part 3 (optimized):}
\begin{itemize}
\item $\lambda$ makes values explicit
\item Trade-offs quantified (9.3x)
\item 30-line Fairlearn code works
\item Production-ready tools available
\end{itemize}

\vspace{0.3cm}
\textcolor{mlgreen}{\Large\textbf{Breakthrough achieved!}}\\
\vspace{0.2cm}
Invisible $	o$ Visible $	o$ Optimizable
\end{columns}

\vspace{0.5em}
\begin{tcolorbox}[colback=mllavender!30, colframe=mlpurple]
\textbf{Core Takeaway:} Optimization ($L = \text{Loss} + \lambda \cdot \text{Fairness}$) makes human trade-offs mathematical and auditable
\end{tcolorbox}

\vspace{0.5em}
\textbf{Next:} Part 4 synthesizes production systems, transferable lessons, and the complete journey

\bottomnote{Optimization transforms intuition into actionable interventions - mathematical formalization enables systematic fairness improvement beyond manual adjustment}
\end{frame}
