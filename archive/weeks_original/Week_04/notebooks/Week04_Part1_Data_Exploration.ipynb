{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Part 1: Data Exploration for Classification\n",
    "\n",
    "## Interactive Learning: Exploring Innovation Success Patterns\n",
    "\n",
    "This notebook provides hands-on exploration of classification datasets, helping you understand:\n",
    "- How to explore data before building classifiers\n",
    "- Feature distributions and their relationship to outcomes\n",
    "- Data preparation techniques for classification\n",
    "- Visual patterns that indicate classification potential\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Load and understand classification datasets\n",
    "2. Visualize feature distributions by class\n",
    "3. Identify patterns and relationships\n",
    "4. Prepare data for classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_iris, load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Scikit-learn loaded and ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Generation and Loading Functions\n",
    "\n",
    "def generate_innovation_dataset(n_samples=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate a synthetic innovation success dataset.\n",
    "    \n",
    "    Features represent:\n",
    "    - Novelty Score: How innovative the idea is (0-100)\n",
    "    - Market Size: Potential market in millions\n",
    "    - Team Experience: Years of combined team experience\n",
    "    - Development Time: Months to develop\n",
    "    - Budget Efficiency: Percentage of budget used efficiently\n",
    "    - Competitive Advantage: Uniqueness score (0-100)\n",
    "    \n",
    "    Target: Success (1) or Failure (0)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate base features\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=6,\n",
    "        n_informative=5,\n",
    "        n_redundant=1,\n",
    "        n_clusters_per_class=2,\n",
    "        weights=[0.4, 0.6],  # 60% success rate\n",
    "        flip_y=0.05,  # 5% label noise\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Transform to realistic ranges\n",
    "    novelty_score = 50 + 20 * X[:, 0]  # 0-100 scale\n",
    "    novelty_score = np.clip(novelty_score, 0, 100)\n",
    "    \n",
    "    market_size = np.exp(2 + 0.5 * X[:, 1]) * 10  # Millions, log-normal\n",
    "    market_size = np.clip(market_size, 1, 1000)\n",
    "    \n",
    "    team_experience = 5 + 3 * X[:, 2]  # Years\n",
    "    team_experience = np.clip(team_experience, 0, 20)\n",
    "    \n",
    "    development_time = 12 + 6 * X[:, 3]  # Months\n",
    "    development_time = np.clip(development_time, 3, 36)\n",
    "    \n",
    "    budget_efficiency = 50 + 15 * X[:, 4]  # Percentage\n",
    "    budget_efficiency = np.clip(budget_efficiency, 10, 100)\n",
    "    \n",
    "    competitive_advantage = 50 + 20 * X[:, 5]  # 0-100 scale\n",
    "    competitive_advantage = np.clip(competitive_advantage, 0, 100)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'novelty_score': novelty_score,\n",
    "        'market_size': market_size,\n",
    "        'team_experience': team_experience,\n",
    "        'development_time': development_time,\n",
    "        'budget_efficiency': budget_efficiency,\n",
    "        'competitive_advantage': competitive_advantage,\n",
    "        'success': y\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_standard_datasets():\n",
    "    \"\"\"\n",
    "    Load standard classification datasets for comparison.\n",
    "    Returns dictionary of datasets.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Iris dataset\n",
    "    iris = load_iris()\n",
    "    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    iris_df['species'] = iris.target\n",
    "    datasets['iris'] = iris_df\n",
    "    \n",
    "    # Wine dataset\n",
    "    wine = load_wine()\n",
    "    wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "    wine_df['quality'] = wine.target\n",
    "    datasets['wine'] = wine_df\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def get_data_summary(df, target_column):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data summary statistics.\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'shape': df.shape,\n",
    "        'features': list(df.drop(columns=[target_column]).columns),\n",
    "        'target_distribution': df[target_column].value_counts(normalize=True).to_dict(),\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'dtypes': df.dtypes.to_dict()\n",
    "    }\n",
    "    \n",
    "    # Basic statistics\n",
    "    summary['statistics'] = df.describe().to_dict()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Visualization Functions\n",
    "\n",
    "def plot_feature_distributions(df, target_column, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Plot distribution of features by target class.\n",
    "    \"\"\"\n",
    "    features = [col for col in df.columns if col != target_column]\n",
    "    n_features = len(features)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot distributions for each class\n",
    "        for class_val in df[target_column].unique():\n",
    "            data = df[df[target_column] == class_val][feature]\n",
    "            label = f'Class {class_val}' if isinstance(class_val, (int, float)) else class_val\n",
    "            ax.hist(data, alpha=0.5, label=label, bins=20, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Feature Distributions by Class', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_matrix(df, target_column, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot correlation matrix including target variable.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr = df.corr()\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', \n",
    "                cmap='coolwarm', center=0, vmin=-1, vmax=1,\n",
    "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    \n",
    "    plt.title('Feature Correlation Matrix', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_pca_visualization(df, target_column, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Visualize data in 2D using PCA.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot 1: PCA scatter plot\n",
    "    scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, \n",
    "                             cmap='viridis', alpha=0.6, edgecolor='black', linewidth=0.5)\n",
    "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    axes[0].set_title('PCA Visualization')\n",
    "    plt.colorbar(scatter, ax=axes[0])\n",
    "    \n",
    "    # Plot 2: Explained variance\n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(X_scaled)\n",
    "    cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    \n",
    "    axes[1].bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
    "               pca_full.explained_variance_ratio_, alpha=0.7, label='Individual')\n",
    "    axes[1].plot(range(1, len(cumsum_var) + 1), cumsum_var, \n",
    "                'r-o', linewidth=2, markersize=6, label='Cumulative')\n",
    "    axes[1].set_xlabel('Principal Component')\n",
    "    axes[1].set_ylabel('Explained Variance Ratio')\n",
    "    axes[1].set_title('PCA Explained Variance')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Feature loadings\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        axes[2].arrow(0, 0, loadings[i, 0], loadings[i, 1], \n",
    "                     head_width=0.05, head_length=0.05, fc='blue', ec='blue', alpha=0.6)\n",
    "        axes[2].text(loadings[i, 0]*1.1, loadings[i, 1]*1.1, feature, \n",
    "                    fontsize=8, ha='center')\n",
    "    \n",
    "    axes[2].set_xlim(-1, 1)\n",
    "    axes[2].set_ylim(-1, 1)\n",
    "    axes[2].set_xlabel('PC1 Loadings')\n",
    "    axes[2].set_ylabel('PC2 Loadings')\n",
    "    axes[2].set_title('PCA Feature Loadings')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[2].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_feature_importance(df, target_column, figsize=(12, 5)):\n",
    "    \"\"\"\n",
    "    Calculate and plot feature importance using statistical tests.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # F-statistic importance\n",
    "    selector_f = SelectKBest(score_func=f_classif, k='all')\n",
    "    selector_f.fit(X, y)\n",
    "    f_scores = selector_f.scores_\n",
    "    \n",
    "    # Mutual information importance\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "    \n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Plot F-statistic scores\n",
    "    indices = np.argsort(f_scores)[::-1]\n",
    "    axes[0].barh(range(len(indices)), f_scores[indices], color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "    axes[0].set_yticks(range(len(indices)))\n",
    "    axes[0].set_yticklabels([feature_names[i] for i in indices])\n",
    "    axes[0].set_xlabel('F-statistic Score')\n",
    "    axes[0].set_title('Feature Importance (ANOVA F-test)')\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot Mutual Information scores\n",
    "    indices = np.argsort(mi_scores)[::-1]\n",
    "    axes[1].barh(range(len(indices)), mi_scores[indices], color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "    axes[1].set_yticks(range(len(indices)))\n",
    "    axes[1].set_yticklabels([feature_names[i] for i in indices])\n",
    "    axes[1].set_xlabel('Mutual Information Score')\n",
    "    axes[1].set_title('Feature Importance (Mutual Information)')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Preparation Functions\n",
    "\n",
    "def prepare_classification_data(df, target_column, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare data for classification: split, scale, and encode.\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create DataFrames with scaled data\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'X_train_scaled': X_train_scaled_df,\n",
    "        'X_test_scaled': X_test_scaled_df,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': list(X.columns)\n",
    "    }\n",
    "\n",
    "def analyze_class_balance(y, title=\"Class Distribution\"):\n",
    "    \"\"\"\n",
    "    Analyze and visualize class balance.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Count plot\n",
    "    class_counts = y.value_counts()\n",
    "    axes[0].bar(class_counts.index, class_counts.values, \n",
    "               color='steelblue', edgecolor='navy', alpha=0.7)\n",
    "    axes[0].set_xlabel('Class')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title(f'{title} - Counts')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (idx, val) in enumerate(class_counts.items()):\n",
    "        axes[0].text(i, val, str(val), ha='center', va='bottom')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(class_counts.values, labels=class_counts.index, \n",
    "               autopct='%1.1f%%', startangle=90, colors=sns.color_palette('husl', len(class_counts)))\n",
    "    axes[1].set_title(f'{title} - Proportions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{title} Statistics:\")\n",
    "    print(f\"Total samples: {len(y)}\")\n",
    "    for class_label, count in class_counts.items():\n",
    "        print(f\"Class {class_label}: {count} ({count/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for imbalance\n",
    "    imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "    print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}\")\n",
    "    if imbalance_ratio > 3:\n",
    "        print(\"WARNING: Dataset is highly imbalanced! Consider using:\")\n",
    "        print(\"- Class weight balancing\")\n",
    "        print(\"- SMOTE or other oversampling techniques\")\n",
    "        print(\"- Stratified sampling\")\n",
    "    elif imbalance_ratio > 1.5:\n",
    "        print(\"Note: Dataset shows some imbalance. Consider using stratified sampling.\")\n",
    "    else:\n",
    "        print(\"Dataset is well-balanced.\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def detect_outliers(df, target_column, method='zscore', threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers in the dataset using various methods.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=[target_column])\n",
    "    outlier_indices = set()\n",
    "    \n",
    "    if method == 'zscore':\n",
    "        # Z-score method\n",
    "        z_scores = np.abs(stats.zscore(X))\n",
    "        outlier_mask = (z_scores > threshold).any(axis=1)\n",
    "        outlier_indices = set(X.index[outlier_mask])\n",
    "    \n",
    "    elif method == 'iqr':\n",
    "        # IQR method\n",
    "        Q1 = X.quantile(0.25)\n",
    "        Q3 = X.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = ((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "        outlier_indices = set(X.index[outlier_mask])\n",
    "    \n",
    "    print(f\"\\nOutlier Detection Results ({method} method):\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Outliers detected: {len(outlier_indices)} ({len(outlier_indices)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return list(outlier_indices)\n",
    "\n",
    "def create_interaction_features(df, target_column):\n",
    "    \"\"\"\n",
    "    Create interaction features for better classification.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=[target_column])\n",
    "    df_interactions = df.copy()\n",
    "    \n",
    "    # Create ratio features\n",
    "    if 'novelty_score' in X.columns and 'competitive_advantage' in X.columns:\n",
    "        df_interactions['innovation_index'] = (\n",
    "            df['novelty_score'] * df['competitive_advantage'] / 100\n",
    "        )\n",
    "    \n",
    "    if 'budget_efficiency' in X.columns and 'development_time' in X.columns:\n",
    "        df_interactions['efficiency_ratio'] = (\n",
    "            df['budget_efficiency'] / (df['development_time'] + 1)\n",
    "        )\n",
    "    \n",
    "    if 'market_size' in X.columns and 'team_experience' in X.columns:\n",
    "        df_interactions['market_experience_score'] = (\n",
    "            np.log1p(df['market_size']) * df['team_experience']\n",
    "        )\n",
    "    \n",
    "    print(\"\\nInteraction Features Created:\")\n",
    "    new_features = [col for col in df_interactions.columns if col not in df.columns]\n",
    "    for feature in new_features:\n",
    "        print(f\"- {feature}\")\n",
    "    \n",
    "    return df_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and Explore the Innovation Dataset\n",
    "\n",
    "Let's start by generating our innovation success dataset and exploring its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate and load the innovation dataset\n",
    "\n",
    "# Generate innovation dataset\n",
    "print(\"Generating innovation success dataset...\")\n",
    "innovation_df = generate_innovation_dataset(n_samples=1000)\n",
    "\n",
    "print(\"\\nDataset created successfully!\")\n",
    "print(f\"Shape: {innovation_df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "innovation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Get comprehensive data summary\n",
    "\n",
    "summary = get_data_summary(innovation_df, 'success')\n",
    "\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nDataset shape: {summary['shape'][0]} samples, {summary['shape'][1]} columns\")\n",
    "print(f\"\\nFeatures ({len(summary['features'])}):\n",
    "for feature in summary['features']:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "for class_val, proportion in summary['target_distribution'].items():\n",
    "    print(f\"  Class {class_val}: {proportion:.1%}\")\n",
    "\n",
    "print(f\"\\nMissing values: {sum(summary['missing_values'].values())}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "innovation_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualize Feature Distributions\n",
    "\n",
    "Understanding how features are distributed across different classes helps identify which features might be good predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Plot feature distributions by class\n",
    "\n",
    "print(\"Visualizing feature distributions by success/failure...\\n\")\n",
    "fig = plot_feature_distributions(innovation_df, 'success')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Features with clear separation between classes are better predictors\")\n",
    "print(\"- Overlapping distributions indicate features that might not discriminate well\")\n",
    "print(\"- Look for features where successful and failed innovations have different patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Analyze feature correlations\n",
    "\n",
    "print(\"Analyzing feature correlations...\\n\")\n",
    "fig = plot_correlation_matrix(innovation_df, 'success')\n",
    "plt.show()\n",
    "\n",
    "# Find features most correlated with success\n",
    "correlations_with_target = innovation_df.corr()['success'].sort_values(ascending=False)\n",
    "print(\"\\nFeatures most correlated with success:\")\n",
    "for feature, corr in correlations_with_target.items():\n",
    "    if feature != 'success':\n",
    "        print(f\"  {feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dimensionality Reduction and Visualization\n",
    "\n",
    "PCA helps us visualize high-dimensional data in 2D and understand which features contribute most to variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: PCA visualization\n",
    "\n",
    "print(\"Applying PCA for visualization...\\n\")\n",
    "fig = plot_pca_visualization(innovation_df, 'success')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPCA Insights:\")\n",
    "print(\"- Left plot: How well the data separates in 2D\")\n",
    "print(\"- Middle plot: How many components needed to capture variance\")\n",
    "print(\"- Right plot: Which features contribute most to principal components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Importance Analysis\n",
    "\n",
    "Before building classifiers, let's identify which features are most important for predicting success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Calculate feature importance\n",
    "\n",
    "print(\"Calculating feature importance...\\n\")\n",
    "fig = plot_feature_importance(innovation_df, 'success')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Methods:\")\n",
    "print(\"- F-statistic: Measures linear relationships\")\n",
    "print(\"- Mutual Information: Captures non-linear relationships\")\n",
    "print(\"\\nFeatures with high scores in both methods are strong predictors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Preparation for Classification\n",
    "\n",
    "Now let's prepare the data for classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Prepare data for classification\n",
    "\n",
    "print(\"Preparing data for classification...\\n\")\n",
    "data_dict = prepare_classification_data(innovation_df, 'success', test_size=0.2)\n",
    "\n",
    "print(f\"Training set: {data_dict['X_train'].shape[0]} samples\")\n",
    "print(f\"Test set: {data_dict['X_test'].shape[0]} samples\")\n",
    "print(f\"Features: {len(data_dict['feature_names'])}\")\n",
    "print(f\"\\nFeature names: {', '.join(data_dict['feature_names'])}\")\n",
    "\n",
    "print(\"\\nData has been:\")\n",
    "print(\"✓ Split into training and test sets\")\n",
    "print(\"✓ Scaled to zero mean and unit variance\")\n",
    "print(\"✓ Stratified to maintain class balance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Analyze class balance\n",
    "\n",
    "print(\"Analyzing class balance in training and test sets...\\n\")\n",
    "\n",
    "# Training set balance\n",
    "fig_train = analyze_class_balance(data_dict['y_train'], \"Training Set Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Test set balance\n",
    "fig_test = analyze_class_balance(data_dict['y_test'], \"Test Set Class Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Outlier Detection and Feature Engineering\n",
    "\n",
    "Let's identify outliers and create interaction features to improve classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Detect outliers\n",
    "\n",
    "print(\"Detecting outliers using different methods...\\n\")\n",
    "\n",
    "# Z-score method\n",
    "outliers_zscore = detect_outliers(innovation_df, 'success', method='zscore', threshold=3)\n",
    "\n",
    "# IQR method\n",
    "outliers_iqr = detect_outliers(innovation_df, 'success', method='iqr')\n",
    "\n",
    "# Common outliers\n",
    "common_outliers = set(outliers_zscore) & set(outliers_iqr)\n",
    "print(f\"\\nOutliers detected by both methods: {len(common_outliers)}\")\n",
    "\n",
    "if len(common_outliers) > 0:\n",
    "    print(\"\\nSample of outlier records:\")\n",
    "    print(innovation_df.iloc[list(common_outliers)[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Create interaction features\n",
    "\n",
    "print(\"Creating interaction features...\\n\")\n",
    "innovation_enhanced = create_interaction_features(innovation_df, 'success')\n",
    "\n",
    "print(\"\\nEnhanced dataset shape:\", innovation_enhanced.shape)\n",
    "print(\"\\nNew features added:\", innovation_enhanced.shape[1] - innovation_df.shape[1])\n",
    "\n",
    "# Show correlation of new features with target\n",
    "new_features = [col for col in innovation_enhanced.columns if col not in innovation_df.columns]\n",
    "if new_features:\n",
    "    print(\"\\nNew feature correlations with success:\")\n",
    "    for feature in new_features:\n",
    "        corr = innovation_enhanced[[feature, 'success']].corr().iloc[0, 1]\n",
    "        print(f\"  {feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Compare with Standard Datasets\n",
    "\n",
    "Let's also explore standard classification datasets to understand different data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Load and explore standard datasets\n",
    "\n",
    "print(\"Loading standard classification datasets...\\n\")\n",
    "standard_datasets = load_standard_datasets()\n",
    "\n",
    "for name, df in standard_datasets.items():\n",
    "    print(f\"\\n{name.upper()} Dataset:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Features: {list(df.columns[:-1])}\")\n",
    "    print(f\"  Classes: {df.iloc[:, -1].nunique()}\")\n",
    "    print(f\"  Class distribution:\")\n",
    "    for class_val, count in df.iloc[:, -1].value_counts().items():\n",
    "        print(f\"    Class {class_val}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Exploration\n",
    "\n",
    "Now you can explore the data interactively! Try modifying the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Interactive parameter exploration\n",
    "\n",
    "# Try different parameters!\n",
    "SAMPLE_SIZE = 500  # Change this: 100, 500, 1000, 5000\n",
    "TEST_SIZE = 0.3    # Change this: 0.1, 0.2, 0.3, 0.4\n",
    "RANDOM_STATE = 42  # Change this for different random splits\n",
    "\n",
    "# Generate new dataset with your parameters\n",
    "custom_df = generate_innovation_dataset(n_samples=SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# Prepare the data\n",
    "custom_data = prepare_classification_data(custom_df, 'success', \n",
    "                                         test_size=TEST_SIZE, \n",
    "                                         random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Custom Dataset Configuration:\")\n",
    "print(f\"  Total samples: {SAMPLE_SIZE}\")\n",
    "print(f\"  Training samples: {len(custom_data['y_train'])}\")\n",
    "print(f\"  Test samples: {len(custom_data['y_test'])}\")\n",
    "print(f\"  Test size ratio: {TEST_SIZE:.0%}\")\n",
    "\n",
    "# Quick visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot training distribution\n",
    "train_counts = custom_data['y_train'].value_counts()\n",
    "axes[0].bar(train_counts.index, train_counts.values, color='steelblue', alpha=0.7)\n",
    "axes[0].set_title(f'Training Set (n={len(custom_data[\"y_train\"])})')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Plot test distribution\n",
    "test_counts = custom_data['y_test'].value_counts()\n",
    "axes[1].bar(test_counts.index, test_counts.values, color='coral', alpha=0.7)\n",
    "axes[1].set_title(f'Test Set (n={len(custom_data[\"y_test\"])})')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Learned:\n",
    "1. **Data Exploration**: How to thoroughly explore a classification dataset\n",
    "2. **Feature Analysis**: Identifying important features using statistical methods\n",
    "3. **Visualization**: Multiple ways to visualize classification data\n",
    "4. **Data Preparation**: Properly splitting and scaling data for ML\n",
    "5. **Class Balance**: Checking for and handling imbalanced datasets\n",
    "6. **Feature Engineering**: Creating new features to improve classification\n",
    "\n",
    "### Key Takeaways:\n",
    "- Always explore your data before building models\n",
    "- Check for class imbalance and outliers\n",
    "- Use multiple visualization techniques to understand patterns\n",
    "- Feature importance helps focus on relevant variables\n",
    "- Proper data preparation is crucial for model performance\n",
    "\n",
    "### Next Steps:\n",
    "1. **Part 2**: Build and compare different classification algorithms\n",
    "2. **Part 3**: Evaluate models and tune hyperparameters\n",
    "3. **Exercises**: Try with your own dataset\n",
    "\n",
    "### Exercises to Try:\n",
    "1. Generate a highly imbalanced dataset (90% one class) and explore it\n",
    "2. Create additional interaction features and check their importance\n",
    "3. Compare the innovation dataset with the standard datasets\n",
    "4. Try different outlier detection thresholds\n",
    "5. Visualize the data using different dimensionality reduction techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Export prepared data for next notebooks\n",
    "\n",
    "# Save the prepared data for use in other notebooks\n",
    "print(\"Saving prepared data for next notebooks...\")\n",
    "\n",
    "# Save as CSV\n",
    "innovation_df.to_csv('innovation_dataset.csv', index=False)\n",
    "innovation_enhanced.to_csv('innovation_dataset_enhanced.csv', index=False)\n",
    "\n",
    "print(\"✓ Saved: innovation_dataset.csv\")\n",
    "print(\"✓ Saved: innovation_dataset_enhanced.csv\")\n",
    "print(\"\\nData ready for Part 2: Building Classification Models!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}