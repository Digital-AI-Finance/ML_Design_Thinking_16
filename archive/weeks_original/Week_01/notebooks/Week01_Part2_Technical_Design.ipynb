{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1: Clustering for Innovation - Part 2\n",
        "## Technical Implementation & Design Integration\n",
        "\n",
        "This notebook continues from Part 1, covering the technical deep dive and design applications.\n",
        "\n",
        "**Part 2 Contents:**\n",
        "- Section 0: Complete Setup & ALL Functions (50+ total)\n",
        "- Section 3: Technical Deep Dive (function calls only)\n",
        "- Section 4: Design Integration (function calls only)\n",
        "\n",
        "**Note:** All code is organized as functions at the beginning for modularity and reusability.\n",
        "**Prerequisites:** Run Part 1 first to understand the foundation, or use the quick setup below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Setup\n",
        "If you're starting directly with Part 2, run this cell to import essential functions from Part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Part 2 setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Essential imports (if starting fresh)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Part 2 setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Design integration functions loaded successfully!\n",
            "\n",
            "‚úÖ All functions for Part 2 are now ready!\n"
          ]
        }
      ],
      "source": [
        "# Design Integration Functions\n",
        "\n",
        "def transform_clusters_to_insights():\n",
        "    \"\"\"\n",
        "    Transform technical clustering results into actionable innovation insights.\n",
        "    Generate comprehensive innovation dataset and apply clustering.\n",
        "    \"\"\"\n",
        "    print(\"üí° Transforming Clusters into Innovation Insights\\n\")\n",
        "    \n",
        "    # Generate comprehensive innovation dataset\n",
        "    n_innovations = 1000\n",
        "    n_features = 10\n",
        "    n_clusters = 5\n",
        "    \n",
        "    # Generate base data\n",
        "    from sklearn.datasets import make_blobs\n",
        "    X_innovation, y_true = make_blobs(n_samples=n_innovations, \n",
        "                                     n_features=n_features,\n",
        "                                     centers=n_clusters,\n",
        "                                     cluster_std=1.2,\n",
        "                                     random_state=42)\n",
        "    \n",
        "    # Feature names\n",
        "    feature_names = [\n",
        "        'Technical_Complexity', 'Market_Readiness', 'Investment_Required',\n",
        "        'User_Impact', 'Implementation_Time', 'Risk_Level',\n",
        "        'Innovation_Score', 'Scalability', 'Regulatory_Compliance', 'ROI_Potential'\n",
        "    ]\n",
        "    \n",
        "    # Standardize\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_innovation)\n",
        "    \n",
        "    # Apply clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    innovation_df = pd.DataFrame(X_innovation, columns=feature_names)\n",
        "    innovation_df['Cluster'] = labels\n",
        "    \n",
        "    # Visualize clusters in 2D using PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # Scatter plot\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n",
        "    for i in range(n_clusters):\n",
        "        mask = labels == i\n",
        "        ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
        "                   c=[colors[i]], s=30, alpha=0.6,\n",
        "                   label=f'Cluster {i+1}', edgecolors='black', linewidth=0.5)\n",
        "    \n",
        "    # Add cluster centers\n",
        "    centers_pca = pca.transform(kmeans.cluster_centers_)\n",
        "    ax1.scatter(centers_pca[:, 0], centers_pca[:, 1],\n",
        "               c='black', marker='*', s=300,\n",
        "               edgecolors='white', linewidth=2, zorder=10)\n",
        "    \n",
        "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
        "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
        "    ax1.set_title('Innovation Clusters (PCA Visualization)', fontsize=12, fontweight='bold')\n",
        "    ax1.legend(loc='best')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Cluster sizes\n",
        "    cluster_sizes = innovation_df['Cluster'].value_counts().sort_index()\n",
        "    ax2.bar(range(n_clusters), cluster_sizes.values, color=colors, alpha=0.7, edgecolor='black')\n",
        "    ax2.set_xlabel('Cluster', fontsize=11)\n",
        "    ax2.set_ylabel('Number of Innovations', fontsize=11)\n",
        "    ax2.set_title('Innovation Distribution Across Clusters', fontsize=12, fontweight='bold')\n",
        "    ax2.set_xticks(range(n_clusters))\n",
        "    ax2.set_xticklabels([f'Cluster {i+1}' for i in range(n_clusters)])\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, v in enumerate(cluster_sizes.values):\n",
        "        ax2.text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
        "    \n",
        "    plt.suptitle('Innovation Landscape Overview', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nüìä Innovation Clustering Results:\")\n",
        "    print(f\"Total Innovations: {n_innovations}\")\n",
        "    print(f\"Number of Clusters: {n_clusters}\")\n",
        "    print(f\"Average Cluster Size: {n_innovations/n_clusters:.0f}\")\n",
        "    print(f\"Silhouette Score: {silhouette_score(X_scaled, labels):.3f}\")\n",
        "    \n",
        "    return innovation_df, X_scaled, labels, kmeans\n",
        "\n",
        "\n",
        "def create_innovation_archetypes():\n",
        "    \"\"\"\n",
        "    Create innovation archetypes from clusters with detailed characterization.\n",
        "    Maps clusters to meaningful innovation personas.\n",
        "    \"\"\"\n",
        "    print(\"üé≠ Creating Innovation Archetypes\\n\")\n",
        "    \n",
        "    # Get data from previous function or generate new\n",
        "    innovation_df, X_scaled, labels, kmeans = transform_clusters_to_insights()\n",
        "    \n",
        "    n_clusters = len(np.unique(labels))\n",
        "    \n",
        "    # Define archetype characteristics\n",
        "    archetype_names = [\n",
        "        'Digital Pioneers',\n",
        "        'Market Disruptors', \n",
        "        'Efficiency Optimizers',\n",
        "        'Customer Champions',\n",
        "        'Platform Builders'\n",
        "    ]\n",
        "    \n",
        "    archetype_descriptions = [\n",
        "        'High-tech, high-risk innovations targeting early adopters',\n",
        "        'Game-changing solutions that redefine market dynamics',\n",
        "        'Process improvements focusing on cost and time savings',\n",
        "        'User-centric innovations prioritizing experience',\n",
        "        'Ecosystem solutions creating network effects'\n",
        "    ]\n",
        "    \n",
        "    # Analyze each cluster\n",
        "    archetypes = []\n",
        "    feature_names = innovation_df.columns[:-1]  # Exclude 'Cluster' column\n",
        "    \n",
        "    for cluster_id in range(n_clusters):\n",
        "        cluster_data = innovation_df[innovation_df['Cluster'] == cluster_id]\n",
        "        \n",
        "        # Calculate statistics\n",
        "        archetype = {\n",
        "            'Cluster': cluster_id + 1,\n",
        "            'Name': archetype_names[cluster_id % len(archetype_names)],\n",
        "            'Description': archetype_descriptions[cluster_id % len(archetype_descriptions)],\n",
        "            'Size': len(cluster_data),\n",
        "            'Percentage': f\"{len(cluster_data)/len(innovation_df)*100:.1f}%\"\n",
        "        }\n",
        "        \n",
        "        # Top features\n",
        "        feature_means = cluster_data[feature_names].mean()\n",
        "        top_features = feature_means.nlargest(3).index.tolist()\n",
        "        archetype['Top_Features'] = ', '.join(top_features)\n",
        "        \n",
        "        # Risk profile\n",
        "        if 'Risk_Level' in cluster_data.columns:\n",
        "            risk_level = cluster_data['Risk_Level'].mean()\n",
        "            if risk_level > 0.5:\n",
        "                archetype['Risk_Profile'] = 'High Risk'\n",
        "            elif risk_level > -0.5:\n",
        "                archetype['Risk_Profile'] = 'Medium Risk'\n",
        "            else:\n",
        "                archetype['Risk_Profile'] = 'Low Risk'\n",
        "        \n",
        "        archetypes.append(archetype)\n",
        "    \n",
        "    # Create archetype cards visualization\n",
        "    fig, axes = plt.subplots(1, n_clusters, figsize=(18, 6))\n",
        "    if n_clusters == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n",
        "    \n",
        "    for idx, archetype in enumerate(archetypes):\n",
        "        ax = plt.subplot(1, n_clusters, idx+1, projection='polar')\n",
        "        \n",
        "        # Create radar chart for each archetype\n",
        "        cluster_data = innovation_df[innovation_df['Cluster'] == idx]\n",
        "        feature_values = cluster_data[feature_names[:6]].mean().values\n",
        "        \n",
        "        # Normalize to 0-1 scale\n",
        "        feature_values = (feature_values - feature_values.min()) / (feature_values.max() - feature_values.min() + 1e-10)\n",
        "        \n",
        "        # Create radar chart\n",
        "        angles = np.linspace(0, 2*np.pi, len(feature_names[:6]), endpoint=False)\n",
        "        feature_values = np.concatenate((feature_values, [feature_values[0]]))\n",
        "        angles = np.concatenate((angles, [angles[0]]))\n",
        "        \n",
        "        ax.plot(angles, feature_values, 'o-', linewidth=2, color=colors[idx])\n",
        "        ax.fill(angles, feature_values, alpha=0.25, color=colors[idx])\n",
        "        ax.set_xticks(angles[:-1])\n",
        "        ax.set_xticklabels([f.replace('_', '\\n') for f in feature_names[:6]], fontsize=8)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title(f\"{archetype['Name']}\\n({archetype['Size']} innovations)\", \n",
        "                    fontsize=10, fontweight='bold', pad=20)\n",
        "        ax.grid(True)\n",
        "    \n",
        "    plt.suptitle('Innovation Archetype Profiles', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Display archetype summary\n",
        "    archetypes_df = pd.DataFrame(archetypes)\n",
        "    print(\"\\nüìã Innovation Archetype Summary:\")\n",
        "    display(archetypes_df[['Name', 'Size', 'Percentage', 'Risk_Profile', 'Top_Features']])\n",
        "    \n",
        "    print(\"\\nüí° How to Use Archetypes:\")\n",
        "    print(\"‚Ä¢ Tailor innovation strategies per archetype\")\n",
        "    print(\"‚Ä¢ Allocate resources based on archetype characteristics\")\n",
        "    print(\"‚Ä¢ Design specific support programs for each type\")\n",
        "    print(\"‚Ä¢ Track archetype evolution over time\")\n",
        "    \n",
        "    return archetypes_df, innovation_df\n",
        "\n",
        "\n",
        "def generate_opportunity_analysis():\n",
        "    \"\"\"\n",
        "    Generate comprehensive opportunity analysis with heatmaps and priority matrices.\n",
        "    Identifies white spaces and strategic opportunities.\n",
        "    \"\"\"\n",
        "    print(\"üî• Innovation Opportunity Analysis\\n\")\n",
        "    \n",
        "    # Get clustered data\n",
        "    innovation_df, X_scaled, labels, kmeans = transform_clusters_to_insights()\n",
        "    n_clusters = len(np.unique(labels))\n",
        "    \n",
        "    # Calculate opportunity scores\n",
        "    opportunity_dimensions = [\n",
        "        'Market_Size', 'Growth_Rate', 'Competition',\n",
        "        'Tech_Readiness', 'Investment_Need', 'Time_to_Market',\n",
        "        'Risk_Level', 'Regulatory', 'Customer_Demand'\n",
        "    ]\n",
        "    \n",
        "    # Create opportunity matrix\n",
        "    np.random.seed(42)\n",
        "    opportunity_matrix = np.random.randn(n_clusters, len(opportunity_dimensions))\n",
        "    \n",
        "    # Create heatmap\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Heatmap\n",
        "    im = ax1.imshow(opportunity_matrix, cmap='RdYlGn', aspect='auto', vmin=-2, vmax=2)\n",
        "    ax1.set_xticks(range(len(opportunity_dimensions)))\n",
        "    ax1.set_xticklabels(opportunity_dimensions, rotation=45, ha='right')\n",
        "    ax1.set_yticks(range(n_clusters))\n",
        "    ax1.set_yticklabels([f'Cluster {i+1}' for i in range(n_clusters)])\n",
        "    ax1.set_title('Innovation Opportunity Heatmap', fontsize=12, fontweight='bold')\n",
        "    \n",
        "    # Add values\n",
        "    for i in range(n_clusters):\n",
        "        for j in range(len(opportunity_dimensions)):\n",
        "            text = ax1.text(j, i, f'{opportunity_matrix[i, j]:.1f}',\n",
        "                           ha='center', va='center', color='black', fontsize=8)\n",
        "    \n",
        "    plt.colorbar(im, ax=ax1, label='Opportunity Score')\n",
        "    \n",
        "    # Priority matrix\n",
        "    cluster_sizes = innovation_df['Cluster'].value_counts().sort_index()\n",
        "    impact = innovation_df.groupby('Cluster')['User_Impact'].mean().values\n",
        "    effort = innovation_df.groupby('Cluster')['Implementation_Time'].mean().values\n",
        "    \n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n",
        "    \n",
        "    ax2.scatter(effort, impact, s=cluster_sizes.values*2, c=colors, \n",
        "               alpha=0.6, edgecolors='black', linewidth=2)\n",
        "    \n",
        "    for i in range(n_clusters):\n",
        "        ax2.annotate(f'C{i+1}', (effort[i], impact[i]),\n",
        "                    ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "    \n",
        "    # Add quadrant lines\n",
        "    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "    ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    # Add quadrant labels\n",
        "    ax2.text(1, 1, 'High Impact\\nHigh Effort', ha='center', va='center', \n",
        "            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
        "    ax2.text(-1, 1, 'High Impact\\nLow Effort', ha='center', va='center', \n",
        "            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
        "    ax2.text(-1, -1, 'Low Impact\\nLow Effort', ha='center', va='center', \n",
        "            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
        "    ax2.text(1, -1, 'Low Impact\\nHigh Effort', ha='center', va='center', \n",
        "            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.3))\n",
        "    \n",
        "    ax2.set_xlabel('Implementation Effort', fontsize=11)\n",
        "    ax2.set_ylabel('User Impact', fontsize=11)\n",
        "    ax2.set_title('Innovation Priority Matrix', fontsize=12, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüéØ Strategic Recommendations:\")\n",
        "    print(\"\\nüü¢ Quick Wins (High Impact, Low Effort):\")\n",
        "    print(\"  ‚Ä¢ Focus immediate resources here\")\n",
        "    print(\"  ‚Ä¢ Rapid prototyping and testing\")\n",
        "    print(\"\\nüü° Strategic Initiatives (High Impact, High Effort):\")\n",
        "    print(\"  ‚Ä¢ Long-term investment required\")\n",
        "    print(\"  ‚Ä¢ Build dedicated teams\")\n",
        "    print(\"\\nüîµ Fill-ins (Low Impact, Low Effort):\")\n",
        "    print(\"  ‚Ä¢ Good for learning and experimentation\")\n",
        "    print(\"  ‚Ä¢ Assign to junior teams\")\n",
        "    print(\"\\nüî¥ Avoid (Low Impact, High Effort):\")\n",
        "    print(\"  ‚Ä¢ Deprioritize or eliminate\")\n",
        "    print(\"  ‚Ä¢ Redirect resources elsewhere\")\n",
        "    \n",
        "    return opportunity_matrix, innovation_df\n",
        "\n",
        "\n",
        "def build_innovation_taxonomy():\n",
        "    \"\"\"\n",
        "    Build hierarchical innovation taxonomy using hierarchical clustering.\n",
        "    Shows relationships between innovation clusters.\n",
        "    \"\"\"\n",
        "    print(\"üå≥ Building Innovation Taxonomy\\n\")\n",
        "    \n",
        "    # Get cluster centers from k-means\n",
        "    _, X_scaled, labels, kmeans = transform_clusters_to_insights()\n",
        "    \n",
        "    # Use cluster centers for hierarchical clustering\n",
        "    from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "    \n",
        "    archetype_names = [\n",
        "        'Digital Pioneers',\n",
        "        'Market Disruptors',\n",
        "        'Efficiency Optimizers',\n",
        "        'Customer Champions',\n",
        "        'Platform Builders'\n",
        "    ]\n",
        "    \n",
        "    # Hierarchical clustering on centers\n",
        "    linkage_matrix = linkage(kmeans.cluster_centers_, method='ward')\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # Dendrogram\n",
        "    dendrogram(linkage_matrix, ax=ax1, labels=archetype_names,\n",
        "              color_threshold=0, above_threshold_color='gray')\n",
        "    ax1.set_title('Innovation Taxonomy Hierarchy', fontsize=12, fontweight='bold')\n",
        "    ax1.set_xlabel('Innovation Archetype')\n",
        "    ax1.set_ylabel('Distance')\n",
        "    \n",
        "    # Lifecycle stages\n",
        "    lifecycle_stages = ['Ideation', 'Validation', 'Development', 'Launch', 'Scale', 'Maturity']\n",
        "    n_clusters = len(kmeans.cluster_centers_)\n",
        "    stage_distribution = np.random.dirichlet(np.ones(len(lifecycle_stages)), size=n_clusters)\n",
        "    \n",
        "    # Stack bar chart for lifecycle\n",
        "    bottom = np.zeros(n_clusters)\n",
        "    stage_colors = plt.cm.coolwarm(np.linspace(0, 1, len(lifecycle_stages)))\n",
        "    \n",
        "    for stage_idx, stage in enumerate(lifecycle_stages):\n",
        "        values = stage_distribution[:, stage_idx]\n",
        "        ax2.bar(range(n_clusters), values, bottom=bottom, \n",
        "               color=stage_colors[stage_idx], label=stage, alpha=0.8)\n",
        "        bottom += values\n",
        "    \n",
        "    ax2.set_xlabel('Innovation Archetype', fontsize=11)\n",
        "    ax2.set_ylabel('Proportion', fontsize=11)\n",
        "    ax2.set_title('Innovation Lifecycle Distribution by Archetype', fontsize=12, fontweight='bold')\n",
        "    ax2.set_xticks(range(n_clusters))\n",
        "    ax2.set_xticklabels([name.split()[0] for name in archetype_names], rotation=45)\n",
        "    ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüìä Taxonomy Insights:\")\n",
        "    print(\"‚Ä¢ Digital Pioneers and Market Disruptors are closely related\")\n",
        "    print(\"‚Ä¢ Efficiency Optimizers form a distinct branch\")\n",
        "    print(\"‚Ä¢ Platform Builders bridge multiple categories\")\n",
        "    \n",
        "    return linkage_matrix, stage_distribution\n",
        "\n",
        "\n",
        "def create_innovation_ecosystem():\n",
        "    \"\"\"\n",
        "    Create innovation ecosystem network showing relationships between\n",
        "    archetypes and stakeholders.\n",
        "    \"\"\"\n",
        "    print(\"üåê Innovation Ecosystem Network\\n\")\n",
        "    \n",
        "    import networkx as nx\n",
        "    \n",
        "    # Create network graph\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    archetype_names = [\n",
        "        'Digital Pioneers',\n",
        "        'Market Disruptors',\n",
        "        'Efficiency Optimizers',\n",
        "        'Customer Champions',\n",
        "        'Platform Builders'\n",
        "    ]\n",
        "    \n",
        "    # Add nodes for archetypes\n",
        "    cluster_sizes = [200, 180, 150, 170, 200]  # Example sizes\n",
        "    for i, name in enumerate(archetype_names):\n",
        "        G.add_node(name, node_type='archetype', size=cluster_sizes[i])\n",
        "    \n",
        "    # Add stakeholder nodes\n",
        "    stakeholders = ['Customers', 'Partners', 'Investors', 'Regulators', 'Competitors']\n",
        "    for stakeholder in stakeholders:\n",
        "        G.add_node(stakeholder, node_type='stakeholder', size=100)\n",
        "    \n",
        "    # Add edges (connections)\n",
        "    connections = [\n",
        "        ('Digital Pioneers', 'Investors', 0.8),\n",
        "        ('Digital Pioneers', 'Partners', 0.6),\n",
        "        ('Market Disruptors', 'Competitors', 0.9),\n",
        "        ('Market Disruptors', 'Customers', 0.7),\n",
        "        ('Efficiency Optimizers', 'Partners', 0.8),\n",
        "        ('Efficiency Optimizers', 'Regulators', 0.5),\n",
        "        ('Customer Champions', 'Customers', 0.9),\n",
        "        ('Customer Champions', 'Partners', 0.6),\n",
        "        ('Platform Builders', 'Partners', 0.9),\n",
        "        ('Platform Builders', 'Investors', 0.7)\n",
        "    ]\n",
        "    \n",
        "    for source, target, weight in connections:\n",
        "        G.add_edge(source, target, weight=weight)\n",
        "    \n",
        "    # Visualize network\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # Layout\n",
        "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
        "    \n",
        "    # Draw nodes\n",
        "    archetype_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'archetype']\n",
        "    stakeholder_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'stakeholder']\n",
        "    \n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(archetype_nodes)))\n",
        "    \n",
        "    # Archetype nodes\n",
        "    nx.draw_networkx_nodes(G, pos, nodelist=archetype_nodes,\n",
        "                          node_color=colors,\n",
        "                          node_size=[G.nodes[n]['size']*5 for n in archetype_nodes],\n",
        "                          alpha=0.7, ax=ax)\n",
        "    \n",
        "    # Stakeholder nodes\n",
        "    nx.draw_networkx_nodes(G, pos, nodelist=stakeholder_nodes,\n",
        "                          node_color='lightgray',\n",
        "                          node_size=500,\n",
        "                          node_shape='s',\n",
        "                          alpha=0.8, ax=ax)\n",
        "    \n",
        "    # Draw edges\n",
        "    edges = G.edges()\n",
        "    weights = [G[u][v]['weight'] for u, v in edges]\n",
        "    nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights],\n",
        "                          alpha=0.5, ax=ax)\n",
        "    \n",
        "    # Labels\n",
        "    labels = {n: n.split()[0] if len(n.split()) > 1 else n for n in G.nodes()}\n",
        "    nx.draw_networkx_labels(G, pos, labels, font_size=10, font_weight='bold', ax=ax)\n",
        "    \n",
        "    ax.set_title('Innovation Ecosystem Network', fontsize=14, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Add legend\n",
        "    from matplotlib.patches import Rectangle, Circle\n",
        "    legend_elements = [\n",
        "        Circle((0, 0), 0.1, facecolor=colors[0], alpha=0.7, label='Innovation Archetypes'),\n",
        "        Rectangle((0, 0), 0.1, 0.1, facecolor='lightgray', alpha=0.8, label='Stakeholders')\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='upper right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüåê Ecosystem Insights:\")\n",
        "    print(\"‚Ä¢ Digital Pioneers have strong investor connections\")\n",
        "    print(\"‚Ä¢ Customer Champions directly connect with users\")\n",
        "    print(\"‚Ä¢ Platform Builders bridge multiple stakeholder groups\")\n",
        "    print(\"‚Ä¢ Market Disruptors create competitive tension\")\n",
        "    print(\"\\nüí° Use this network to:\")\n",
        "    print(\"‚Ä¢ Identify collaboration opportunities\")\n",
        "    print(\"‚Ä¢ Understand influence patterns\")\n",
        "    print(\"‚Ä¢ Design stakeholder engagement strategies\")\n",
        "    \n",
        "    return G\n",
        "\n",
        "print(\"Design integration functions loaded successfully!\")\n",
        "print(\"\\n‚úÖ All functions for Part 2 are now ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.3 Design Integration Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Algorithm Demonstration Functions",
        "def demonstrate_kmeans_step_by_step(X=None, n_clusters=3, n_iterations=5):",
        "    \"\"\"",
        "    Visualize K-means algorithm step by step.    Shows how centers converge to optimal positions.",
        "    \"\"\"",
        "    print(\"üéØ K-Means Clustering: Step-by-Step Process\\n\")",
        "    if X is None:",
        "        X, y_true = generate_blob_data(n_samples=300, centers=3, cluster_std=0.8)",
        "    np.random.seed(42)",
        "    # Initialize random centers",
        "    idx = np.random.choice(len(X), n_clusters, replace=False)",
        "    centers = X[idx].copy()",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))",
        "    axes = axes.flatten()",
        "    for iteration in range(n_iterations):",
        "    ax = axes[iteration]",
        "    # Assign points to nearest center",
        "    distances = np.zeros((len(X), n_clusters))",
        "    for k in range(n_clusters):",
        "            distances[:, k] = np.linalg.norm(X - centers[k], axis=1)",
        "    labels = np.argmin(distances, axis=1)",
        "    # Visualize current state",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']",
        "    for k in range(n_clusters):",
        "            mask = labels == k",
        "    ax.scatter(X[mask, 0], X[mask, 1], c=colors[k],",
        "               s=50, alpha=0.6, label=f'Cluster {k+1}')",
        "    # Plot centers",
        "    ax.scatter(centers[:, 0], centers[:, 1], c='red',",
        "           marker='*', s=300, edgecolors='black',",
        "           linewidth=2, label='Centers', zorder=10)",
        "    # Update centers",
        "    new_centers = np.zeros_like(centers)",
        "    for k in range(n_clusters):",
        "    if np.sum(labels == k) > 0:",
        "        new_centers[k] = X[labels == k].mean(axis=0)",
        "    else:",
        "        new_centers[k] = centers[k]",
        "    # Draw movement arrows",
        "    for k in range(n_clusters):",
        "    ax.arrow(centers[k, 0], centers[k, 1],",
        "            new_centers[k, 0] - centers[k, 0],",
        "            new_centers[k, 1] - centers[k, 1],",
        "            head_width=0.1, head_length=0.1,",
        "            fc='black', ec='black', alpha=0.5)",
        "    centers = new_centers.copy()",
        "    ax.set_title(f'Iteration {iteration + 1}', fontsize=12, fontweight='bold')",
        "    ax.set_xlabel('Feature 1')",
        "    ax.set_ylabel('Feature 2')",
        "    if iteration == 0:",
        "    ax.legend(loc='upper right', fontsize=8)",
        "    # Final result",
        "    ax = axes[5]",
        "    for k in range(n_clusters):",
        "        mask = labels == k",
        "    ax.scatter(X[mask, 0], X[mask, 1], c=colors[k],",
        "           s=50, alpha=0.6, label=f'Cluster {k+1}')",
        "    ax.scatter(centers[:, 0], centers[:, 1], c='red',",
        "               marker='*', s=300, edgecolors='black',",
        "               linewidth=2, label='Final Centers', zorder=10)",
        "    ax.set_title('Final Result', fontsize=12, fontweight='bold')",
        "    ax.set_xlabel('Feature 1')",
        "    ax.set_ylabel('Feature 2')",
        "    ax.legend(loc='upper right', fontsize=8)",
        "    plt.suptitle('K-Means Algorithm: Watch Centers Converge to Optimal Positions',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    silhouette = silhouette_score(X, labels)",
        "    print(f\"\\nüìä Algorithm Performance:\")",
        "    print(f\"Silhouette Score: {silhouette:.3f}\")",
        "    print(f\"Converged in 5 iterations\")",
        "    return centers, labels",
        "",
        "",
        "def demonstrate_kmeans_implementation():",
        "    \"\"\"",
        "    Hands-on K-Means implementation with different K values.    Shows impact of K on clustering quality.",
        "    \"\"\"",
        "    print(\"üîß Hands-on K-Means Implementation\\n\")",
        "    # Create innovation dataset",
        "    df, X_scaled, y_true = generate_innovation_data(n_samples=1000, n_features=10, n_clusters=4)",
        "    print(f\"Dataset shape: {X_scaled.shape}\")",
        "    print(f\"Features: {', '.join(df.columns[:10])}\\n\")",
        "    # Apply K-means with different K values",
        "    k_values = [2, 3, 4, 5, 6]",
        "    results = {}",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))",
        "    axes = axes.flatten()",
        "    # Use PCA for visualization",
        "    pca = PCA(n_components=2)",
        "    X_pca = pca.fit_transform(StandardScaler().fit_transform(X_scaled))",
        "    for idx, k in enumerate(k_values):",
        "    # Fit K-means",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)",
        "    labels = kmeans.fit_predict(X_scaled)",
        "    # Calculate metrics",
        "    silhouette = silhouette_score(X_scaled, labels)",
        "    inertia = kmeans.inertia_",
        "    results[k] = {",
        "            'labels': labels,",
        "            'centers': kmeans.cluster_centers_,",
        "            'silhouette': silhouette,",
        "            'inertia': inertia",
        "        }",
        "    # Visualize",
        "    ax = axes[idx]",
        "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels,",
        "             cmap='viridis', s=20, alpha=0.6)",
        "    # Plot centers in PCA space",
        "        centers_pca = pca.transform(StandardScaler().fit_transform(kmeans.cluster_centers_))",
        "    ax.scatter(centers_pca[:, 0], centers_pca[:, 1],",
        "          c='red', marker='*', s=300,",
        "           edgecolors='black', linewidth=2)",
        "    ax.set_title(f'K={k}, Silhouette={silhouette:.3f}',",
        "             fontsize=11, fontweight='bold')",
        "    ax.set_xlabel('First Principal Component')",
        "    ax.set_ylabel('Second Principal Component')",
        "    plt.colorbar(scatter, ax=ax)",
        "    # Hide extra subplot",
        "    axes[-1].set_visible(False)",
        "    plt.suptitle('K-Means with Different K Values (PCA Visualization)',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    # Print comparison",
        "    print(\"\\nüìä K-Means Results Comparison:\")",
        "    print(\"K | Silhouette | Inertia\")",
        "    print(\"-\" * 30)",
        "    for k, metrics in results.items():",
        "    print(f\"{k} | {metrics['silhouette']:.3f}      | {metrics['inertia']:.1f}\")",
        "    # Best K",
        "    best_k = max(results.keys(), key=lambda k: results[k]['silhouette'])",
        "    print(f\"\\n‚ú® Best K={best_k} with silhouette score {results[best_k]['silhouette']:.3f}\")",
        "    return results",
        "",
        "",
        "def implement_kmeans_from_scratch():",
        "    \"\"\"",
        "    Exercise: Implement K-means from scratch.    Compare with sklearn implementation.",
        "    \"\"\"",
        "    print(\"üéØ Exercise: Implement K-Means from Scratch\\n\")",
        "    class MyKMeans:    ",
        "    \"\"\"Simple K-Means implementation for learning\"\"\"",
        "",
        "",
        "def __init__(self, n_clusters=3, max_iters=100, tol=1e-4):",
        "            self.n_clusters = n_clusters",
        "            self.max_iters = max_iters",
        "            self.tol = tol",
        "            self.centers = None",
        "            self.labels = None",
        "",
        "",
        "def fit(self, X):",
        "    \"\"\"Fit K-means to data\"\"\"",
        "            n_samples = X.shape[0]",
        "    # Initialize centers randomly",
        "    idx = np.random.choice(n_samples, self.n_clusters, replace=False)",
        "            self.centers = X[idx].copy()",
        "    for iteration in range(self.max_iters):",
        "    # Assign points to nearest center",
        "    distances = np.zeros((n_samples, self.n_clusters))",
        "    for k in range(self.n_clusters):",
        "            distances[:, k] = np.linalg.norm(X - self.centers[k], axis=1)",
        "        self.labels = np.argmin(distances, axis=1)",
        "    # Update centers",
        "    new_centers = np.zeros_like(self.centers)",
        "    for k in range(self.n_clusters):",
        "    if np.sum(self.labels == k) > 0:",
        "        new_centers[k] = X[self.labels == k].mean(axis=0)",
        "    else:",
        "        new_centers[k] = self.centers[k]",
        "    # Check convergence",
        "    if np.linalg.norm(new_centers - self.centers) < self.tol:",
        "    print(f\"Converged at iteration {iteration + 1}\")",
        "            break",
        "        self.centers = new_centers",
        "    return self",
        "",
        "",
        "def predict(self, X):",
        "    \"\"\"Predict cluster labels\"\"\"",
        "    distances = np.zeros((X.shape[0], self.n_clusters))",
        "    for k in range(self.n_clusters):",
        "        distances[:, k] = np.linalg.norm(X - self.centers[k], axis=1)",
        "    return np.argmin(distances, axis=1)",
        "    # Test implementation",
        "    X_test, _ = generate_blob_data(n_samples=200, centers=3)",
        "    # Your implementation",
        "    my_kmeans = MyKMeans(n_clusters=3)",
        "    my_kmeans.fit(X_test)",
        "    my_labels = my_kmeans.labels",
        "    # Sklearn implementation",
        "    sklearn_kmeans = KMeans(n_clusters=3, random_state=42)",
        "    sklearn_labels = sklearn_kmeans.fit_predict(X_test)",
        "    # Compare results",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))",
        "    ax1.scatter(X_test[:, 0], X_test[:, 1], c=my_labels, cmap='viridis', s=50)",
        "    ax1.scatter(my_kmeans.centers[:, 0], my_kmeans.centers[:, 1],",
        "               c='red', marker='*', s=300, edgecolors='black', linewidth=2)",
        "    ax1.set_title('Your Implementation', fontsize=12, fontweight='bold')",
        "    ax1.set_xlabel('Feature 1')",
        "    ax1.set_ylabel('Feature 2')",
        "    ax2.scatter(X_test[:, 0], X_test[:, 1], c=sklearn_labels, cmap='viridis', s=50)",
        "    ax2.scatter(sklearn_kmeans.cluster_centers_[:, 0],",
        "    sklearn_kmeans.cluster_centers_[:, 1],",
        "               c='red', marker='*', s=300, edgecolors='black', linewidth=2)",
        "    ax2.set_title('Sklearn Implementation', fontsize=12, fontweight='bold')",
        "    ax2.set_xlabel('Feature 1')",
        "    ax2.set_ylabel('Feature 2')",
        "    plt.suptitle('K-Means Implementation Comparison', fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(f\"\\n‚úÖ Your implementation silhouette score: {silhouette_score(X_test, my_labels):.3f}\")",
        "    print(f\"‚úÖ Sklearn silhouette score: {silhouette_score(X_test, sklearn_labels):.3f}\")",
        "    return my_kmeans, sklearn_kmeans",
        "",
        "",
        "def find_optimal_k_elbow():",
        "    \"\"\"",
        "    Comprehensive elbow method analysis with multiple metrics.    Shows how to find the optimal number of clusters.",
        "    \"\"\"",
        "    print(\"üìà Finding Optimal K: The Elbow Method\\n\")",
        "    # Generate data with known clusters",
        "    X_elbow, y_true = generate_blob_data(n_samples=500, centers=4, cluster_std=1.0)",
        "    # Test range of K values",
        "    k_range = range(1, 11)",
        "    inertias = []",
        "    silhouettes = []",
        "    davies_bouldins = []",
        "    for k in k_range:",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)",
        "    labels = kmeans.fit_predict(X_elbow)",
        "    inertias.append(kmeans.inertia_)",
        "    if k > 1:  # Metrics need at least 2 clusters",
        "    silhouettes.append(silhouette_score(X_elbow, labels))",
        "    davies_bouldins.append(davies_bouldin_score(X_elbow, labels))",
        "    else:",
        "    silhouettes.append(0)",
        "    davies_bouldins.append(0)",
        "    # Calculate elbow point",
        "    deltas = np.diff(inertias)    delta_deltas = np.diff(deltas)",
        "    elbow_idx = np.argmax(np.abs(delta_deltas)) + 2  # +2 because of double diff",
        "    # Visualization",
        "    fig, axes = plt.subplots(1, 3, figsize=(14, 5))",
        "    # Inertia/Elbow plot",
        "    ax1 = axes[0]",
        "    ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)",
        "    ax1.axvline(x=list(k_range)[elbow_idx], color='red',",
        "        linestyle='--', alpha=0.7, label=f'Elbow at k={list(k_range)[elbow_idx]}')",
        "    ax1.set_xlabel('Number of Clusters (k)', fontsize=11)",
        "    ax1.set_ylabel('Inertia', fontsize=11)",
        "    ax1.set_title('Elbow Method', fontsize=12, fontweight='bold')",
        "    ax1.legend()",
        "    ax1.grid(True, alpha=0.3)",
        "    # Silhouette scores",
        "    ax2 = axes[1]",
        "    ax2.plot(k_range[1:], silhouettes[1:], 'go-', linewidth=2, markersize=8)",
        "    best_silhouette_k = list(k_range)[np.argmax(silhouettes) if silhouettes else 0]",
        "    ax2.axvline(x=best_silhouette_k, color='red', linestyle='--',",
        "        alpha=0.7, label=f'Best at k={best_silhouette_k}')",
        "    ax2.set_xlabel('Number of Clusters (k)', fontsize=11)",
        "    ax2.set_ylabel('Silhouette Score', fontsize=11)",
        "    ax2.set_title('Silhouette Analysis', fontsize=12, fontweight='bold')",
        "    ax2.legend()",
        "    ax2.grid(True, alpha=0.3)",
        "    # Davies-Bouldin Index (lower is better)",
        "    ax3 = axes[2]",
        "    ax3.plot(k_range[1:], davies_bouldins[1:], 'ro-', linewidth=2, markersize=8)",
        "    best_db_k = list(k_range)[np.argmin(davies_bouldins[1:]) + 1 if davies_bouldins[1:] else 0]",
        "    ax3.axvline(x=best_db_k, color='green', linestyle='--',",
        "        alpha=0.7, label=f'Best at k={best_db_k}')",
        "    ax3.set_xlabel('Number of Clusters (k)', fontsize=11)",
        "    ax3.set_ylabel('Davies-Bouldin Index', fontsize=11)",
        "    ax3.set_title('Davies-Bouldin Index (lower is better)', fontsize=12, fontweight='bold')",
        "    ax3.legend()",
        "    ax3.grid(True, alpha=0.3)",
        "    plt.suptitle('Multiple Methods for Finding Optimal K', fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(\"\\nüìä Optimal K Recommendations:\")",
        "    print(f\"Elbow Method: k={list(k_range)[elbow_idx]}\")",
        "    print(f\"Silhouette Score: k={best_silhouette_k}\")",
        "    print(f\"Davies-Bouldin Index: k={best_db_k}\")",
        "    print(f\"\\nTrue number of clusters: 4\")",
        "    print(\"\\nüí° Tip: When methods disagree, consider domain knowledge and use case!\")",
        "    return {'elbow_k': list(k_range)[elbow_idx], 'silhouette_k': best_silhouette_k, 'db_k': best_db_k}",
        "",
        "",
        "def demonstrate_dbscan_parameters():",
        "    \"\"\"",
        "    DBSCAN parameter exploration showing impact of eps and min_samples.    Helps understand how to tune DBSCAN for different datasets.",
        "    \"\"\"",
        "    print(\"üîç DBSCAN: Understanding eps and min_samples\\n\")",
        "    # Generate data with outliers",
        "    X_dbscan, _ = generate_blob_data(n_samples=300, centers=3, cluster_std=0.5)",
        "    # Add noise points",
        "    X_noise = np.random.uniform(-6, 6, (50, 2))",
        "    X_dbscan = np.vstack([X_dbscan, X_noise])",
        "    # Test different parameter combinations",
        "    eps_values = [0.3, 0.5, 0.7, 1.0]",
        "    min_samples_values = [3, 5, 10, 20]",
        "    fig, axes = plt.subplots(len(eps_values), len(min_samples_values),",
        "             figsize=(16, 12))",
        "    for i, eps in enumerate(eps_values):",
        "    for j, min_samples in enumerate(min_samples_values):",
        "    ax = axes[i, j]",
        "    # Apply DBSCAN",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)",
        "    labels = dbscan.fit_predict(X_dbscan)",
        "    # Count clusters and noise",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)",
        "    n_noise = list(labels).count(-1)",
        "    # Plot",
        "    unique_labels = set(labels)",
        "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))",
        "    for k, col in zip(unique_labels, colors):",
        "    if k == -1:",
        "            col = 'black'",
        "            marker = 'x'",
        "    else:",
        "            marker = 'o'",
        "        class_member_mask = (labels == k)",
        "        xy = X_dbscan[class_member_mask]",
        "    ax.scatter(xy[:, 0], xy[:, 1], c=[col],",
        "           marker=marker, s=30, alpha=0.7)",
        "    ax.set_title(f'eps={eps}, min={min_samples}\\nC={n_clusters}, N={n_noise}',",
        "        fontsize=9)",
        "    ax.set_xticks([])",
        "    ax.set_yticks([])",
        "    # Add labels",
        "    for i, eps in enumerate(eps_values):",
        "    axes[i, 0].set_ylabel(f'eps={eps}', fontsize=10, fontweight='bold')",
        "    for j, min_samples in enumerate(min_samples_values):",
        "    axes[0, j].set_xlabel(f'min_samples={min_samples}', fontsize=10, fontweight='bold')",
        "    axes[0, j].xaxis.set_label_position('top')",
        "    plt.suptitle('DBSCAN Parameter Grid: Impact of eps and min_samples\\n'",
        "        'C=Clusters, N=Noise points', fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(\"\\nüìö Parameter Guidelines:\")",
        "    print(\"‚Ä¢ eps: Maximum distance between points in same neighborhood\")",
        "    print(\"  - Too small: Many clusters, more noise\")",
        "    print(\"  - Too large: Few clusters, points merge\")",
        "    print(\"\\n‚Ä¢ min_samples: Minimum points to form dense region\")",
        "    print(\"  - Too small: More clusters, less noise\")",
        "    print(\"  - Too large: Fewer clusters, more noise\")",
        "    print(\"\\nüí° Start with min_samples = 2 * dimensions, adjust eps based on data\")",
        "    return X_dbscan",
        "",
        "",
        "def demonstrate_hierarchical_clustering():",
        "    \"\"\"",
        "    Hierarchical clustering demonstration with dendrograms.    Shows different linkage methods and how to cut the tree.",
        "    \"\"\"",
        "    print(\"üå≥ Hierarchical Clustering: Building Innovation Taxonomy\\n\")",
        "    # Generate hierarchical data",
        "    X_hier, y_hier = generate_blob_data(n_samples=100, centers=4, cluster_std=0.5)",
        "    # Different linkage methods",
        "    linkage_methods = ['ward', 'complete', 'average', 'single']",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))",
        "    axes = axes.flatten()",
        "    for idx, method in enumerate(linkage_methods):",
        "    ax = axes[idx]",
        "    # Perform hierarchical clustering",
        "    from scipy.cluster.hierarchy import dendrogram, linkage",
        "    linkage_matrix = linkage(X_hier, method=method)",
        "    # Plot dendrogram",
        "        dendrogram(linkage_matrix, ax=ax, truncate_mode='level',",
        "           p=5, color_threshold=0, above_threshold_color='gray')",
        "    ax.set_title(f'Linkage: {method.capitalize()}', fontsize=12, fontweight='bold')",
        "    ax.set_xlabel('Sample Index')",
        "    ax.set_ylabel('Distance')",
        "    plt.suptitle('Hierarchical Clustering with Different Linkage Methods',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(\"\\nüìä Linkage Method Comparison:\")",
        "    print(\"‚Ä¢ Ward: Minimizes within-cluster variance (most common)\")",
        "    print(\"‚Ä¢ Complete: Maximum distance between clusters\")",
        "    print(\"‚Ä¢ Average: Average distance between all pairs\")",
        "    print(\"‚Ä¢ Single: Minimum distance (can create chains)\")",
        "    return X_hier",
        "",
        "",
        "def demonstrate_gmm():",
        "    \"\"\"",
        "    Gaussian Mixture Models demonstration.    Shows soft clustering with probabilities.",
        "    \"\"\"",
        "    print(\"üîÆ Gaussian Mixture Models: Soft Clustering\\n\")",
        "    # Generate overlapping clusters",
        "    X_gmm, y_gmm = generate_blob_data(n_samples=400, centers=3, cluster_std=1.2)",
        "    # Fit GMM",
        "    from sklearn.mixture import GaussianMixture",
        "    gmm = GaussianMixture(n_components=3, random_state=42)",
        "    gmm.fit(X_gmm)",
        "    # Get predictions and probabilities",
        "    gmm_labels = gmm.predict(X_gmm)",
        "    gmm_probs = gmm.predict_proba(X_gmm)",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))",
        "    # Hard clustering",
        "    ax1 = axes[0]",
        "    scatter1 = ax1.scatter(X_gmm[:, 0], X_gmm[:, 1], c=gmm_labels,",
        "           cmap='viridis', s=30, alpha=0.7)",
        "    ax1.scatter(gmm.means_[:, 0], gmm.means_[:, 1],",
        "               c='red', marker='*', s=300, edgecolors='black', linewidth=2)",
        "    ax1.set_title('Hard Assignment', fontsize=11, fontweight='bold')",
        "    ax1.set_xlabel('Feature 1')",
        "    ax1.set_ylabel('Feature 2')",
        "    plt.colorbar(scatter1, ax=ax1)",
        "    # Soft clustering - show uncertainty",
        "    ax2 = axes[1]",
        "    uncertainty = -np.sum(gmm_probs * np.log(gmm_probs + 1e-10), axis=1)",
        "    scatter2 = ax2.scatter(X_gmm[:, 0], X_gmm[:, 1], c=uncertainty,",
        "           cmap='RdYlGn_r', s=30, alpha=0.7)",
        "    ax2.set_title('Uncertainty', fontsize=11, fontweight='bold')",
        "    ax2.set_xlabel('Feature 1')",
        "    ax2.set_ylabel('Feature 2')",
        "    plt.colorbar(scatter2, ax=ax2, label='Uncertainty')",
        "    # Probability contours",
        "    ax3 = axes[2]",
        "    x = np.linspace(X_gmm[:, 0].min() - 1, X_gmm[:, 0].max() + 1, 100)",
        "    y = np.linspace(X_gmm[:, 1].min() - 1, X_gmm[:, 1].max() + 1, 100)",
        "    X_grid, Y_grid = np.meshgrid(x, y)    XX = np.array([X_grid.ravel(), Y_grid.ravel()]).T    Z = -gmm.score_samples(XX)    Z = Z.reshape(X_grid.shape)",
        "    ax3.contour(X_grid, Y_grid, Z, levels=10, linewidths=0.5, colors='black', alpha=0.3)",
        "    ax3.contourf(X_grid, Y_grid, Z, levels=10, cmap='viridis', alpha=0.3)",
        "    ax3.scatter(X_gmm[:, 0], X_gmm[:, 1], c=gmm_labels,",
        "        cmap='viridis', s=30, alpha=0.7, edgecolors='black', linewidth=0.5)",
        "    ax3.scatter(gmm.means_[:, 0], gmm.means_[:, 1],",
        "               c='red', marker='*', s=300, edgecolors='white', linewidth=2)",
        "    ax3.set_title('Probability Contours', fontsize=11, fontweight='bold')",
        "    ax3.set_xlabel('Feature 1')",
        "    ax3.set_ylabel('Feature 2')",
        "    plt.suptitle('Gaussian Mixture Models: Soft vs Hard Clustering',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    # Show probability examples",
        "    print(\"\\nüìä Example: Innovation Probability Assignments\")",
        "    print(\"\\nSample 5 innovations and their cluster probabilities:\")",
        "    print(\"ID | Cluster 1 | Cluster 2 | Cluster 3 | Assigned\")",
        "    print(\"-\" * 50)",
        "    for i in range(5):",
        "        probs = gmm_probs[i]",
        "        assigned = gmm_labels[i]",
        "    print(f\"{i:2} | {probs[0]:.3f}    | {probs[1]:.3f}    | \"",
        "              f\"{probs[2]:.3f}    | Cluster {assigned+1}\")",
        "    print(\"\\nüí° GMM Benefits:\")",
        "    print(\"‚Ä¢ Shows uncertainty in cluster assignments\")",
        "    print(\"‚Ä¢ Handles overlapping clusters\")",
        "    print(\"‚Ä¢ Provides probability distributions\")",
        "    return gmm, X_gmm, gmm_labels",
        "",
        "",
        "def compare_all_algorithms():",
        "    \"\"\"",
        "    Comprehensive comparison of all clustering algorithms.    Shows strengths and weaknesses of each method.",
        "    \"\"\"",
        "    print(\"‚öñÔ∏è Clustering Algorithm Comparison\\n\")",
        "    # Generate test dataset",
        "    X_compare, y_compare = generate_blob_data(n_samples=500, centers=4, cluster_std=1.0)",
        "    # Add some noise",
        "    X_noise = np.random.uniform(X_compare.min(), X_compare.max(), (50, 2))",
        "    X_compare = np.vstack([X_compare, X_noise])    y_compare = np.hstack([y_compare, [-1] * 50])",
        "    # Standardize",
        "    from sklearn.preprocessing import StandardScaler    scaler = StandardScaler()",
        "    X_scaled = scaler.fit_transform(X_compare)",
        "    # Define algorithms",
        "    from sklearn.mixture import GaussianMixture    algorithms = [",
        "        ('K-Means', KMeans(n_clusters=4, random_state=42)),",
        "        ('DBSCAN', DBSCAN(eps=0.3, min_samples=5)),",
        "        ('Hierarchical', AgglomerativeClustering(n_clusters=4)),",
        "        ('GMM', GaussianMixture(n_components=4, random_state=42))    ]",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))",
        "    # Store results    comparison_results = []",
        "    for idx, (name, algorithm) in enumerate(algorithms):",
        "    # Fit algorithm",
        "    if hasattr(algorithm, 'fit_predict'):",
        "    labels = algorithm.fit_predict(X_scaled)",
        "    else:",
        "    labels = algorithm.fit(X_scaled).predict(X_scaled)",
        "    # Calculate metrics",
        "    unique_labels = np.unique(labels[labels != -1])",
        "    n_clusters = len(unique_labels)",
        "    n_noise = np.sum(labels == -1)",
        "    if n_clusters > 1:",
        "    silhouette = silhouette_score(X_scaled, labels)",
        "            db_index = davies_bouldin_score(X_scaled, labels)",
        "    else:",
        "    silhouette = -1",
        "            db_index = np.inf",
        "        comparison_results.append({",
        "            'Algorithm': name,",
        "            'Clusters': n_clusters,",
        "            'Noise': n_noise,",
        "            'Silhouette': silhouette,",
        "            'Davies-Bouldin': db_index",
        "        })",
        "    # Visualization",
        "    ax1 = axes[0, idx]",
        "    ax2 = axes[1, idx]",
        "    # Plot clusters",
        "    for label in unique_labels:",
        "    if label == -1:",
        "        mask = labels == label",
        "    ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],",
        "           c='black', marker='x', s=30, alpha=0.5, label='Noise')",
        "    else:",
        "        mask = labels == label",
        "    ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],",
        "           s=30, alpha=0.7, label=f'C{label}')",
        "    ax1.set_title(f'{name}\\nClusters: {n_clusters}, Noise: {n_noise}',",
        "              fontsize=10, fontweight='bold')",
        "    ax1.set_xticks([])",
        "    ax1.set_yticks([])",
        "    # Metrics bar chart",
        "        metrics = ['Silhouette', 'DB Index\\n(inverted)']",
        "        values = [silhouette, -db_index/10]  # Normalize for display",
        "    colors_bar = ['green' if v > 0 else 'red' for v in values]",
        "        bars = ax2.bar(range(2), values, color=colors_bar, alpha=0.7)",
        "    ax2.set_xticks(range(2))",
        "    ax2.set_xticklabels(metrics, fontsize=8)",
        "    ax2.set_title(f'{name} Metrics', fontsize=10, fontweight='bold')",
        "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)",
        "    plt.suptitle('Clustering Algorithm Comparison', fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    # Comparison table    comparison_df = pd.DataFrame(comparison_results)",
        "    print(\"\\nüìä Performance Comparison:\")    display(comparison_df)",
        "    print(\"\\nüéØ Algorithm Selection Guide:\")",
        "    print(\"\\nüìå K-Means: Fast, simple, spherical clusters\")",
        "    print(\"üìå DBSCAN: Arbitrary shapes, identifies outliers\")",
        "    print(\"üìå Hierarchical: Creates taxonomy, no K needed upfront\")",
        "    print(\"üìå GMM: Soft clustering, overlapping clusters\")",
        "    return comparison_df",
        "",
        "",
        "def demonstrate_common_mistakes():",
        "    \"\"\"",
        "    Show common clustering mistakes and how to fix them.    Educational visualization of pitfalls.",
        "    \"\"\"",
        "    print(\"‚ö†Ô∏è Common Clustering Mistakes and How to Fix Them\\n\")",
        "    # Generate data",
        "    X_mistakes, y_mistakes = generate_blob_data(n_samples=300, centers=3)",
        "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))",
        "    # Mistake 1: Not scaling features",
        "    ax1 = axes[0, 0]",
        "    X_unscaled = X_mistakes.copy()",
        "    X_unscaled[:, 1] *= 100  # Make one feature much larger    kmeans_unscaled = KMeans(n_clusters=3, random_state=42)    labels_unscaled = kmeans_unscaled.fit_predict(X_unscaled)",
        "    ax1.scatter(X_unscaled[:, 0], X_unscaled[:, 1], c=labels_unscaled,",
        "        cmap='viridis', s=30, alpha=0.7)",
        "    ax1.set_title('‚ùå Mistake: Unscaled Features', fontsize=10, fontweight='bold', color='red')",
        "    ax1.set_xlabel('Feature 1 (0-10)')",
        "    ax1.set_ylabel('Feature 2 (0-1000)')",
        "    # Fix 1: Scale features",
        "    ax2 = axes[0, 1]    scaler = StandardScaler()",
        "    X_scaled_fix = scaler.fit_transform(X_unscaled)    kmeans_scaled = KMeans(n_clusters=3, random_state=42)    labels_scaled = kmeans_scaled.fit_predict(X_scaled_fix)",
        "    ax2.scatter(X_scaled_fix[:, 0], X_scaled_fix[:, 1], c=labels_scaled,",
        "        cmap='viridis', s=30, alpha=0.7)",
        "    ax2.set_title('‚úÖ Fix: Scaled Features', fontsize=10, fontweight='bold', color='green')",
        "    ax2.set_xlabel('Feature 1 (standardized)')",
        "    ax2.set_ylabel('Feature 2 (standardized)')",
        "    # Mistake 2: Wrong number of clusters",
        "    ax3 = axes[0, 2]    kmeans_wrong_k = KMeans(n_clusters=10, random_state=42)    labels_wrong_k = kmeans_wrong_k.fit_predict(X_mistakes)",
        "    ax3.scatter(X_mistakes[:, 0], X_mistakes[:, 1], c=labels_wrong_k,",
        "        cmap='tab10', s=30, alpha=0.7)",
        "    silhouette_wrong = silhouette_score(X_mistakes, labels_wrong_k)",
        "    ax3.set_title(f'‚ùå Too Many Clusters (K=10)\\nSilhouette: {silhouette_wrong:.3f}',",
        "          fontsize=10, fontweight='bold', color='red')",
        "    # Fix 2: Use elbow method",
        "    ax4 = axes[1, 0]    kmeans_correct_k = KMeans(n_clusters=3, random_state=42)    labels_correct_k = kmeans_correct_k.fit_predict(X_mistakes)",
        "    ax4.scatter(X_mistakes[:, 0], X_mistakes[:, 1], c=labels_correct_k,",
        "        cmap='viridis', s=30, alpha=0.7)",
        "    silhouette_correct = silhouette_score(X_mistakes, labels_correct_k)",
        "    ax4.set_title(f'‚úÖ Optimal K=3\\nSilhouette: {silhouette_correct:.3f}',",
        "          fontsize=10, fontweight='bold', color='green')",
        "    # Mistake 3: Ignoring outliers",
        "    ax5 = axes[1, 1]",
        "    X_with_outliers = X_mistakes.copy()    outliers = np.random.uniform(-15, 15, (20, 2))",
        "    X_with_outliers = np.vstack([X_with_outliers, outliers])    kmeans_outliers = KMeans(n_clusters=3, random_state=42)    labels_outliers = kmeans_outliers.fit_predict(X_with_outliers)",
        "    ax5.scatter(X_with_outliers[:, 0], X_with_outliers[:, 1],",
        "        c=labels_outliers, cmap='viridis', s=30, alpha=0.7)",
        "    ax5.set_title('‚ùå K-Means with Outliers', fontsize=10, fontweight='bold', color='red')",
        "    # Fix 3: Use DBSCAN",
        "    ax6 = axes[1, 2]    dbscan_fix = DBSCAN(eps=1.5, min_samples=5)    labels_dbscan = dbscan_fix.fit_predict(X_with_outliers)",
        "    unique_labels = set(labels_dbscan)",
        "    colors_db = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))",
        "    for k, col in zip(unique_labels, colors_db):",
        "    if k == -1:",
        "            col = 'black'",
        "            marker = 'x'",
        "    else:",
        "            marker = 'o'",
        "        class_member_mask = (labels_dbscan == k)",
        "        xy = X_with_outliers[class_member_mask]",
        "    ax6.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, s=30, alpha=0.7)",
        "    ax6.set_title('‚úÖ DBSCAN Handles Outliers', fontsize=10, fontweight='bold', color='green')",
        "    plt.suptitle('Common Clustering Mistakes and Solutions', fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(\"\\nüìö Summary of Common Mistakes:\")",
        "    print(\"\\n1. üìè Not Scaling Features:\")",
        "    print(\"   Problem: Features with larger scales dominate\")",
        "    print(\"   Solution: Always standardize or normalize\")",
        "    print(\"\\n2. üî¢ Wrong Number of Clusters:\")",
        "    print(\"   Problem: Too many/few clusters\")",
        "    print(\"   Solution: Use elbow method, silhouette analysis\")",
        "    print(\"\\n3. üîç Ignoring Outliers:\")",
        "    print(\"   Problem: K-means is sensitive to outliers\")",
        "    print(\"   Solution: Use DBSCAN or remove outliers first\")print(\"Algorithm demonstration functions loaded successfully!\")# ============================================================================# ADDITIONAL ALGORITHM FUNCTIONS# ============================================================================",
        "",
        "",
        "def demonstrate_silhouette_analysis():",
        "    \"\"\"",
        "    Detailed silhouette analysis for cluster validation.    Shows how to interpret silhouette scores.",
        "    \"\"\"",
        "    print(\"üìä Silhouette Analysis: Understanding Cluster Quality\\n\")",
        "    # Generate data with varying cluster quality",
        "    X_good, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)",
        "    X_bad, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.0, random_state=42)",
        "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))",
        "    for row, (X, quality) in enumerate([(X_good, 'Good'), (X_bad, 'Poor')]):",
        "    # Standardize",
        "    X_scaled = StandardScaler().fit_transform(X)",
        "    # Apply K-means",
        "    kmeans = KMeans(n_clusters=3, random_state=42)",
        "    labels = kmeans.fit_predict(X_scaled)",
        "    # Calculate silhouette scores",
        "    from sklearn.metrics import silhouette_samples",
        "    silhouette_vals = silhouette_samples(X_scaled, labels)",
        "    silhouette_avg = silhouette_score(X_scaled, labels)",
        "    # Plot clusters",
        "    ax1 = axes[row, 0]",
        "    scatter = ax1.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30, alpha=0.7)",
        "    ax1.set_title(f'{quality} Clustering\\nSilhouette: {silhouette_avg:.3f}',",
        "              fontweight='bold')",
        "    ax1.set_xlabel('Feature 1')",
        "    ax1.set_ylabel('Feature 2')",
        "    # Silhouette plot",
        "    ax2 = axes[row, 1]",
        "        y_lower = 10",
        "    for i in range(3):",
        "            cluster_silhouette_vals = silhouette_vals[labels == i]",
        "            cluster_silhouette_vals.sort()",
        "        size_cluster_i = cluster_silhouette_vals.shape[0]",
        "            y_upper = y_lower + size_cluster_i",
        "        color = plt.cm.viridis(i / 3)",
        "    ax2.fill_betweenx(np.arange(y_lower, y_upper), 0,",
        "              cluster_silhouette_vals,",
        "              facecolor=color, edgecolor=color, alpha=0.7)",
        "    ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))",
        "            y_lower = y_upper + 10",
        "    ax2.set_xlabel('Silhouette Coefficient')",
        "    ax2.set_ylabel('Cluster Label')",
        "    ax2.set_title('Silhouette Plot', fontweight='bold')",
        "    ax2.axvline(x=silhouette_avg, color='red', linestyle='--',",
        "            label=f'Average: {silhouette_avg:.3f}')",
        "    ax2.legend()",
        "    # Distribution",
        "    ax3 = axes[row, 2]",
        "    ax3.hist(silhouette_vals, bins=20, alpha=0.7, color='blue', edgecolor='black')",
        "    ax3.axvline(x=silhouette_avg, color='red', linestyle='--', linewidth=2)",
        "    ax3.set_xlabel('Silhouette Score')",
        "    ax3.set_ylabel('Frequency')",
        "    ax3.set_title('Score Distribution', fontweight='bold')",
        "    plt.suptitle('Silhouette Analysis: Good vs Poor Clustering',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(\"\\nüìö Interpretation Guide:\")",
        "    print(\"‚Ä¢ Score > 0.7: Strong clustering\")",
        "    print(\"‚Ä¢ Score 0.5-0.7: Reasonable clustering\")",
        "    print(\"‚Ä¢ Score 0.25-0.5: Weak clustering\")",
        "    print(\"‚Ä¢ Score < 0.25: Poor/artificial clustering\")",
        "",
        "",
        "def demonstrate_clustering_stability():",
        "    \"\"\"",
        "    Test clustering stability with different initializations.    Shows importance of n_init parameter.",
        "    \"\"\"",
        "    print(\"üîÑ Clustering Stability Analysis\\n\")",
        "    # Generate data    X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)",
        "    X_scaled = StandardScaler().fit_transform(X)",
        "    # Test different n_init values    n_init_values = [1, 5, 10, 20]    n_runs = 10",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))",
        "    axes = axes.flatten()",
        "    for idx, n_init in enumerate(n_init_values):",
        "    ax = axes[idx]",
        "    # Run clustering multiple times",
        "        all_labels = []",
        "        all_scores = []",
        "    for run in range(n_runs):",
        "    kmeans = KMeans(n_clusters=3, n_init=n_init, random_state=run)",
        "    labels = kmeans.fit_predict(X_scaled)",
        "            score = silhouette_score(X_scaled, labels)",
        "            all_labels.append(labels)",
        "            all_scores.append(score)",
        "    # Plot all results overlaid",
        "    for labels, alpha in zip(all_labels, np.linspace(0.2, 0.8, n_runs)):",
        "    ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis',",
        "               s=20, alpha=alpha)",
        "    # Statistics",
        "        mean_score = np.mean(all_scores)",
        "        std_score = np.std(all_scores)",
        "    ax.set_title(f'n_init={n_init}\\nScore: {mean_score:.3f} ¬± {std_score:.3f}',",
        "             fontweight='bold')",
        "    ax.set_xlabel('Feature 1')",
        "    ax.set_ylabel('Feature 2')",
        "    plt.suptitle('Clustering Stability: Impact of n_init Parameter',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(\"\\nüìä Stability Results:\")",
        "    print(\"‚Ä¢ n_init=1: Unstable, varies with initialization\")",
        "    print(\"‚Ä¢ n_init=5: Better stability\")",
        "    print(\"‚Ä¢ n_init=10: Good stability (default)\")",
        "    print(\"‚Ä¢ n_init=20: Marginal improvement\")",
        "    print(\"\\nüí° Recommendation: Use n_init=10 for production\")",
        "",
        "",
        "def compare_distance_metrics():",
        "    \"\"\"",
        "    Compare different distance metrics for clustering.    Shows when to use each metric.",
        "    \"\"\"",
        "    print(\"üìè Distance Metrics Comparison\\n\")",
        "    # Generate data with different characteristics",
        "    X_normal, _ = make_blobs(n_samples=200, centers=3, random_state=42)",
        "    # Create elongated clusters    transformation = [[0.5, -0.5], [-0.8, 0.8]]",
        "    X_elongated = np.dot(X_normal, transformation)",
        "    # Different metrics    metrics = ['euclidean', 'manhattan', 'cosine']",
        "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))",
        "    for row, (X, data_type) in enumerate([(X_normal, 'Normal'),",
        "           (X_elongated, 'Elongated')]):",
        "    X_scaled = StandardScaler().fit_transform(X)",
        "    for col, metric in enumerate(metrics):",
        "    ax = axes[row, col]",
        "    # Apply clustering with different metrics",
        "    if metric == 'cosine':",
        "    # Use AgglomerativeClustering for cosine metric",
        "    from sklearn.cluster import AgglomerativeClustering",
        "        clustering = AgglomerativeClustering(n_clusters=3,",
        "             metric=metric,",
        "            linkage='average')",
        "    labels = clustering.fit_predict(X_scaled)",
        "    else:",
        "    # KMeans doesn't support custom metrics, use AgglomerativeClustering",
        "        clustering = AgglomerativeClustering(n_clusters=3,",
        "             metric=metric,",
        "            linkage='average')",
        "    labels = clustering.fit_predict(X_scaled)",
        "    # Plot",
        "    scatter = ax.scatter(X[:, 0], X[:, 1], c=labels,",
        "        cmap='viridis', s=30, alpha=0.7)",
        "    # Calculate score",
        "            score = silhouette_score(X_scaled, labels, metric=metric)",
        "    ax.set_title(f'{data_type} Data\\n{metric.capitalize()}\\nScore: {score:.3f}',",
        "         fontweight='bold')",
        "    ax.set_xlabel('Feature 1')",
        "    ax.set_ylabel('Feature 2')",
        "    plt.suptitle('Distance Metrics: Impact on Clustering Results',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(\"\\nüìö When to Use Each Metric:\")",
        "    print(\"‚Ä¢ Euclidean: General purpose, spherical clusters\")",
        "    print(\"‚Ä¢ Manhattan: Grid-like data, robust to outliers\")",
        "    print(\"‚Ä¢ Cosine: Text data, direction matters more than magnitude\")",
        "    print(\"‚Ä¢ Correlation: Time series, pattern similarity\")",
        "",
        "",
        "def demonstrate_feature_scaling_impact():",
        "    \"\"\"",
        "    Show the critical importance of feature scaling.    Demonstrates what happens with and without scaling.",
        "    \"\"\"",
        "    print(\"‚öñÔ∏è Feature Scaling: Critical for Success\\n\")",
        "    # Create data with different scales",
        "    np.random.seed(42)    n_samples = 300",
        "    # Feature 1: Scale 0-1    feature1 = np.random.uniform(0, 1, n_samples)",
        "    # Feature 2: Scale 0-1000    feature2 = np.random.uniform(0, 1000, n_samples)",
        "    # Create 3 true clusters    true_labels = np.repeat([0, 1, 2], 100)    feature1[true_labels == 0] += 0.5    feature1[true_labels == 1] -= 0.3    feature2[true_labels == 0] += 500    feature2[true_labels == 1] -= 300",
        "    X_unscaled = np.column_stack([feature1, feature2])",
        "    # Scale the data    scaler = StandardScaler()",
        "    X_scaled = scaler.fit_transform(X_unscaled)",
        "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))",
        "    # Unscaled clustering    kmeans_unscaled = KMeans(n_clusters=3, random_state=42)    labels_unscaled = kmeans_unscaled.fit_predict(X_unscaled)",
        "    # Scaled clustering    kmeans_scaled = KMeans(n_clusters=3, random_state=42)    labels_scaled = kmeans_scaled.fit_predict(X_scaled)",
        "    # Plot unscaled",
        "    axes[0, 0].scatter(X_unscaled[:, 0], X_unscaled[:, 1],",
        "               c=true_labels, cmap='viridis', s=30, alpha=0.7)",
        "    axes[0, 0].set_title('True Clusters\\n(Unscaled View)', fontweight='bold')",
        "    axes[0, 0].set_xlabel('Feature 1 (0-1)')",
        "    axes[0, 0].set_ylabel('Feature 2 (0-1000)')",
        "    axes[0, 1].scatter(X_unscaled[:, 0], X_unscaled[:, 1],",
        "               c=labels_unscaled, cmap='viridis', s=30, alpha=0.7)",
        "    axes[0, 1].set_title('K-Means (No Scaling)\\n‚ùå Dominated by Feature 2',",
        "         fontweight='bold', color='red')",
        "    axes[0, 1].set_xlabel('Feature 1 (0-1)')",
        "    axes[0, 1].set_ylabel('Feature 2 (0-1000)')",
        "    # Feature importance (unscaled)",
        "    axes[0, 2].bar(['Feature 1', 'Feature 2'],",
        "            [np.std(X_unscaled[:, 0]), np.std(X_unscaled[:, 1])],",
        "           color=['blue', 'red'])",
        "    axes[0, 2].set_title('Feature \"Importance\"\\n(Unscaled)', fontweight='bold')",
        "    axes[0, 2].set_ylabel('Standard Deviation')",
        "    # Plot scaled",
        "    axes[1, 0].scatter(X_scaled[:, 0], X_scaled[:, 1],",
        "               c=true_labels, cmap='viridis', s=30, alpha=0.7)",
        "    axes[1, 0].set_title('True Clusters\\n(Scaled View)', fontweight='bold')",
        "    axes[1, 0].set_xlabel('Feature 1 (scaled)')",
        "    axes[1, 0].set_ylabel('Feature 2 (scaled)')",
        "    axes[1, 1].scatter(X_scaled[:, 0], X_scaled[:, 1],",
        "               c=labels_scaled, cmap='viridis', s=30, alpha=0.7)",
        "    axes[1, 1].set_title('K-Means (With Scaling)\\n‚úÖ Balanced Features',",
        "         fontweight='bold', color='green')",
        "    axes[1, 1].set_xlabel('Feature 1 (scaled)')",
        "    axes[1, 1].set_ylabel('Feature 2 (scaled)')",
        "    # Feature importance (scaled)",
        "    axes[1, 2].bar(['Feature 1', 'Feature 2'],",
        "            [np.std(X_scaled[:, 0]), np.std(X_scaled[:, 1])],",
        "           color=['blue', 'blue'])",
        "    axes[1, 2].set_title('Feature Importance\\n(Scaled)', fontweight='bold')",
        "    axes[1, 2].set_ylabel('Standard Deviation')",
        "    axes[1, 2].set_ylim([0, 2])",
        "    plt.suptitle('Feature Scaling: Essential for Correct Clustering',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    # Performance comparison    score_unscaled = silhouette_score(X_unscaled, labels_unscaled)    score_scaled = silhouette_score(X_scaled, labels_scaled)",
        "    print(f\"\\nüìä Performance Impact:\")",
        "    print(f\"Without scaling: Silhouette = {score_unscaled:.3f}\")",
        "    print(f\"With scaling: Silhouette = {score_scaled:.3f}\")",
        "    print(f\"Improvement: {(score_scaled - score_unscaled) / abs(score_unscaled) * 100:.1f}%\")",
        "    print(\"\\nüö® Always scale your features before clustering!\")",
        "",
        "",
        "def demonstrate_cluster_initialization():",
        "    \"\"\"",
        "    Show different initialization methods for K-means.    Compare random vs k-means++ initialization.",
        "    \"\"\"",
        "    print(\"üé≤ Cluster Initialization Methods\\n\")",
        "    # Generate challenging data    X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.2, random_state=42)",
        "    X_scaled = StandardScaler().fit_transform(X)",
        "    # Different initialization methods    init_methods = ['random', 'k-means++']    n_runs = 5",
        "    fig, axes = plt.subplots(2, n_runs, figsize=(16, 6))",
        "    for row, init_method in enumerate(init_methods):",
        "        scores = []",
        "    inertias = []",
        "    for col in range(n_runs):",
        "    ax = axes[row, col]",
        "    # Apply K-means with different initialization",
        "    kmeans = KMeans(n_clusters=4, init=init_method,",
        "           n_init=1, random_state=col)",
        "    labels = kmeans.fit_predict(X_scaled)",
        "    # Calculate metrics",
        "            score = silhouette_score(X_scaled, labels)",
        "            scores.append(score)",
        "    inertias.append(kmeans.inertia_)",
        "    # Plot",
        "    scatter = ax.scatter(X[:, 0], X[:, 1], c=labels,",
        "        cmap='viridis', s=20, alpha=0.7)",
        "    ax.scatter(kmeans.cluster_centers_[:, 0] * np.std(X[:, 0]) + np.mean(X[:, 0]),",
        "              kmeans.cluster_centers_[:, 1] * np.std(X[:, 1]) + np.mean(X[:, 1]),",
        "              c='red', marker='*', s=200, edgecolors='black', linewidth=1.5)",
        "    ax.set_title(f'Run {col+1}\\nScore: {score:.3f}', fontsize=10)",
        "    ax.set_xticks([])",
        "    ax.set_yticks([])",
        "    if col == 0:",
        "    ax.set_ylabel(f'{init_method}', fontsize=12, fontweight='bold')",
        "    # Add statistics",
        "        mean_score = np.mean(scores)",
        "        std_score = np.std(scores)",
        "    print(f\"{init_method:12} - Mean Score: {mean_score:.3f} ¬± {std_score:.3f}\")",
        "    plt.suptitle('K-Means Initialization: Random vs K-Means++',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(\"\\nüìö Key Insights:\")",
        "    print(\"‚Ä¢ Random: Varies significantly between runs\")",
        "    print(\"‚Ä¢ K-means++: More consistent, better results\")",
        "    print(\"‚Ä¢ K-means++ chooses centers far apart\")",
        "    print(\"‚Ä¢ Default in sklearn is k-means++\")",
        "",
        "",
        "def demonstrate_incremental_clustering():",
        "    \"\"\"",
        "    Show incremental/online clustering with MiniBatchKMeans.    Useful for large datasets or streaming data.",
        "    \"\"\"",
        "    print(\"üìà Incremental Clustering for Large Data\\n\")",
        "    # Simulate streaming data    n_batches = 5    batch_size = 200",
        "    n_clusters = 3",
        "    # Generate full dataset",
        "    X_full, y_full = make_blobs(n_samples=n_batches * batch_size,",
        "        centers=n_clusters, cluster_std=0.7,",
        "        random_state=42)",
        "    X_full_scaled = StandardScaler().fit_transform(X_full)",
        "    # Compare regular KMeans vs MiniBatchKMeans",
        "    from sklearn.cluster import MiniBatchKMeans",
        "    fig, axes = plt.subplots(2, n_batches + 1, figsize=(16, 6))",
        "    # Initialize models",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)    mbkmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42,",
        "               batch_size=batch_size)",
        "    # Process batches",
        "    for batch_idx in range(n_batches):",
        "        start_idx = batch_idx * batch_size",
        "        end_idx = (batch_idx + 1) * batch_size",
        "    X_batch = X_full_scaled[start_idx:end_idx]",
        "    # MiniBatch update",
        "        mbkmeans.partial_fit(X_batch)",
        "    # Visualize current state",
        "    ax_mb = axes[1, batch_idx]",
        "    # Show data seen so far",
        "    X_seen = X_full_scaled[:end_idx]",
        "        labels_mb = mbkmeans.predict(X_seen)",
        "    ax_mb.scatter(X_full[:end_idx, 0], X_full[:end_idx, 1],",
        "              c=labels_mb, cmap='viridis', s=20, alpha=0.6)",
        "    ax_mb.set_title(f'Batch {batch_idx+1}\\n({end_idx} points)', fontsize=10)",
        "    ax_mb.set_xticks([])",
        "    ax_mb.set_yticks([])",
        "    if batch_idx == 0:",
        "    ax_mb.set_ylabel('MiniBatch\\nK-Means', fontsize=11, fontweight='bold')",
        "    # Final comparison",
        "    # Regular K-means on full data    labels_full = kmeans.fit_predict(X_full_scaled)",
        "    ax_full = axes[0, n_batches]",
        "    ax_full.scatter(X_full[:, 0], X_full[:, 1], c=labels_full,",
        "            cmap='viridis', s=20, alpha=0.6)",
        "    ax_full.set_title(f'Final\\nFull K-Means', fontsize=10, fontweight='bold')",
        "    ax_full.set_xticks([])",
        "    ax_full.set_yticks([])",
        "    # MiniBatch final    labels_mb_final = mbkmeans.predict(X_full_scaled)",
        "    ax_mb_final = axes[1, n_batches]",
        "    ax_mb_final.scatter(X_full[:, 0], X_full[:, 1], c=labels_mb_final,",
        "        cmap='viridis', s=20, alpha=0.6)",
        "    ax_mb_final.set_title(f'Final\\nMiniBatch', fontsize=10, fontweight='bold')",
        "    ax_mb_final.set_xticks([])",
        "    ax_mb_final.set_yticks([])",
        "    # Hide first row except last",
        "    for i in range(n_batches):",
        "    axes[0, i].axis('off')",
        "    plt.suptitle('Incremental Clustering: Processing Data in Batches',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    # Performance comparison    import time",
        "    # Time regular K-means    start = time.time()    kmeans.fit(X_full_scaled)    kmeans_time = time.time() - start",
        "    # Time MiniBatchKMeans    start = time.time()    mbkmeans_new = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)    mbkmeans_new.fit(X_full_scaled)    mb_time = time.time() - start",
        "    print(f\"\\n‚è±Ô∏è Performance Comparison:\")",
        "    print(f\"Regular K-Means: {kmeans_time:.4f}s\")",
        "    print(f\"MiniBatch K-Means: {mb_time:.4f}s\")",
        "    print(f\"Speedup: {kmeans_time/mb_time:.2f}x\")",
        "    print(f\"\\nüí° MiniBatch is ideal for large datasets or streaming data!\")# ============================================================================# ADVANCED CLUSTERING TECHNIQUES# ============================================================================",
        "",
        "",
        "def demonstrate_spectral_clustering():",
        "    \"\"\"",
        "    Show spectral clustering for non-convex shapes.    Handles complex cluster boundaries.",
        "    \"\"\"",
        "    print(\"üåà Spectral Clustering: Beyond Simple Shapes\\n\")",
        "    from sklearn.cluster import SpectralClustering",
        "    from sklearn.datasets import make_moons, make_circles",
        "    # Generate non-convex datasets    datasets = [",
        "        ('Two Moons', make_moons(n_samples=300, noise=0.1, random_state=42)),",
        "        ('Concentric Circles', make_circles(n_samples=300, noise=0.05,",
        "            factor=0.5, random_state=42))    ]",
        "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))",
        "    for row, (name, (X, y_true)) in enumerate(datasets):",
        "    # K-Means (fails on these shapes)",
        "    kmeans = KMeans(n_clusters=2, random_state=42)",
        "        kmeans_labels = kmeans.fit_predict(X)",
        "    # Spectral Clustering (works well)",
        "        spectral = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',",
        "             n_neighbors=10, random_state=42)",
        "        spectral_labels = spectral.fit_predict(X)",
        "    # Plot true labels",
        "    axes[row, 0].scatter(X[:, 0], X[:, 1], c=y_true,",
        "            cmap='viridis', s=30, alpha=0.7)",
        "    axes[row, 0].set_title(f'{name}\\nTrue Clusters', fontweight='bold')",
        "    # Plot K-Means result",
        "    axes[row, 1].scatter(X[:, 0], X[:, 1], c=kmeans_labels,",
        "            cmap='viridis', s=30, alpha=0.7)",
        "        score_km = silhouette_score(X, kmeans_labels)",
        "    axes[row, 1].set_title(f'K-Means\\nScore: {score_km:.3f}', fontweight='bold')",
        "    # Plot Spectral result",
        "    axes[row, 2].scatter(X[:, 0], X[:, 1], c=spectral_labels,",
        "            cmap='viridis', s=30, alpha=0.7)",
        "        score_sp = silhouette_score(X, spectral_labels)",
        "    axes[row, 2].set_title(f'Spectral\\nScore: {score_sp:.3f}', fontweight='bold')",
        "    plt.suptitle('Spectral Clustering: Handling Non-Convex Shapes',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(\"\\nüìö When to Use Spectral Clustering:\")",
        "    print(\"‚Ä¢ Non-convex cluster shapes\")",
        "    print(\"‚Ä¢ Image segmentation\")",
        "    print(\"‚Ä¢ Social network analysis\")",
        "    print(\"‚Ä¢ When connectivity matters more than distance\")",
        "",
        "",
        "def demonstrate_mean_shift():",
        "    \"\"\"",
        "    Show Mean Shift clustering - finds modes automatically.    No need to specify number of clusters.",
        "    \"\"\"",
        "    print(\"üéØ Mean Shift: Automatic Mode Finding\\n\")",
        "    from sklearn.cluster import MeanShift, estimate_bandwidth",
        "    # Generate data with varying densities    X1 = np.random.randn(100, 2) * 0.5 + [2, 2]    X2 = np.random.randn(150, 2) * 0.7 + [-2, -1]    X3 = np.random.randn(80, 2) * 0.3 + [1, -2]    X = np.vstack([X1, X2, X3])",
        "    # Estimate bandwidth    bandwidth = estimate_bandwidth(X, quantile=0.2)",
        "    # Apply Mean Shift    ms = MeanShift(bandwidth=bandwidth)",
        "    labels = ms.fit_predict(X)    cluster_centers = ms.cluster_centers_",
        "    n_clusters = len(cluster_centers)",
        "    # Compare with K-means (needs K specified)",
        "    fig, axes = plt.subplots(1, 3, figsize=(14, 5))",
        "    # Mean Shift result",
        "    axes[0].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30, alpha=0.7)",
        "    axes[0].scatter(cluster_centers[:, 0], cluster_centers[:, 1],",
        "           c='red', marker='*', s=300, edgecolors='black', linewidth=2)",
        "    axes[0].set_title(f'Mean Shift\\nFound {n_clusters} clusters automatically',",
        "              fontweight='bold')",
        "    # K-means with correct K    kmeans_correct = KMeans(n_clusters=3, random_state=42)    labels_correct = kmeans_correct.fit_predict(X)",
        "    axes[1].scatter(X[:, 0], X[:, 1], c=labels_correct,",
        "            cmap='viridis', s=30, alpha=0.7)",
        "    axes[1].set_title('K-Means (K=3)\\nCorrect K', fontweight='bold')",
        "    # K-means with wrong K    kmeans_wrong = KMeans(n_clusters=5, random_state=42)    labels_wrong = kmeans_wrong.fit_predict(X)",
        "    axes[2].scatter(X[:, 0], X[:, 1], c=labels_wrong,",
        "            cmap='viridis', s=30, alpha=0.7)",
        "    axes[2].set_title('K-Means (K=5)\\nWrong K', fontweight='bold')",
        "    for ax in axes:",
        "    ax.set_xlabel('Feature 1')",
        "    ax.set_ylabel('Feature 2')",
        "    plt.suptitle('Mean Shift: Automatic Cluster Discovery',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(f\"\\nüéØ Mean Shift Results:\")",
        "    print(f\"‚Ä¢ Automatically found {n_clusters} clusters\")",
        "    print(f\"‚Ä¢ No need to specify K beforehand\")",
        "    print(f\"‚Ä¢ Bandwidth parameter controls granularity\")",
        "    print(\"‚Ä¢ Works well for density-based clustering\")",
        "",
        "",
        "def demonstrate_affinity_propagation():",
        "    \"\"\"",
        "    Show Affinity Propagation - finds exemplars automatically.    Good for finding representative examples.",
        "    \"\"\"",
        "    print(\"üìç Affinity Propagation: Finding Exemplars\\n\")",
        "    from sklearn.cluster import AffinityPropagation",
        "    # Generate data    X, _ = make_blobs(n_samples=200, centers=4, cluster_std=0.5, random_state=42)",
        "    # Apply Affinity Propagation    af = AffinityPropagation(random_state=42, damping=0.9)",
        "    labels = af.fit_predict(X)    cluster_centers_indices = af.cluster_centers_indices_",
        "    n_clusters = len(cluster_centers_indices)",
        "    # Visualize",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))",
        "    # Plot clusters",
        "    ax1 = axes[0]",
        "    colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))",
        "    for k, col in zip(range(n_clusters), colors):",
        "        class_members = labels == k",
        "        cluster_center = X[cluster_centers_indices[k]]",
        "    ax1.plot(X[class_members, 0], X[class_members, 1], 'o',",
        "         markerfacecolor=col, markeredgecolor='k', markersize=8, alpha=0.6)",
        "    ax1.plot(cluster_center[0], cluster_center[1], 'o',",
        "         markerfacecolor=col, markeredgecolor='red', markersize=15,",
        "         markeredgewidth=2)",
        "    # Draw lines from exemplar to members",
        "    for x in X[class_members]:",
        "    ax1.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]],",
        "             col, alpha=0.2, linewidth=0.5)",
        "    ax1.set_title(f'Affinity Propagation\\nFound {n_clusters} exemplars',",
        "          fontweight='bold')",
        "    ax1.set_xlabel('Feature 1')",
        "    ax1.set_ylabel('Feature 2')",
        "    # Compare cluster sizes",
        "    ax2 = axes[1]    unique, counts = np.unique(labels, return_counts=True)",
        "    ax2.bar(range(len(unique)), counts, color=colors)",
        "    ax2.set_xlabel('Cluster ID')",
        "    ax2.set_ylabel('Number of Points')",
        "    ax2.set_title('Cluster Size Distribution', fontweight='bold')",
        "    plt.suptitle('Affinity Propagation: Automatic Exemplar Selection',",
        "         fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    print(f\"\\nüìç Key Features:\")",
        "    print(f\"‚Ä¢ Found {n_clusters} exemplars (actual data points)\")",
        "    print(\"‚Ä¢ No need to specify number of clusters\")",
        "    print(\"‚Ä¢ Each exemplar represents its cluster\")",
        "    print(\"‚Ä¢ Useful for selecting representative samples\")# ============================================================================# HELPER UTILITIES# ============================================================================",
        "",
        "",
        "def generate_complex_data(n_samples=500, pattern='spiral'):",
        "    \"\"\"",
        "    Generate complex data patterns for testing algorithms.    Includes spiral, swiss roll, and other challenging shapes.",
        "    \"\"\"",
        "    if pattern == 'spiral':",
        "        theta = np.sqrt(np.random.rand(n_samples)) * 4 * np.pi",
        "        r = theta",
        "    x = r * np.cos(theta) + np.random.randn(n_samples) * 0.5",
        "    y = r * np.sin(theta) + np.random.randn(n_samples) * 0.5",
        "        X = np.column_stack([x, y])",
        "            elif pattern == 'swiss_roll':",
        "    from sklearn.datasets import make_swiss_roll",
        "        X, _ = make_swiss_roll(n_samples, noise=0.5, random_state=42)",
        "        X = X[:, [0, 2]]  # Use 2D projection",
        "            elif pattern == 'anisotropic':",
        "        X, _ = make_blobs(n_samples=n_samples, centers=3, random_state=42)",
        "        transformation = [[0.6, -0.6], [-0.4, 0.8]]",
        "        X = np.dot(X, transformation)",
        "    else:",
        "        X, _ = make_blobs(n_samples=n_samples, centers=4, random_state=42)",
        "    return StandardScaler().fit_transform(X)",
        "",
        "",
        "def evaluate_clustering_comprehensive(X, labels, algorithm_name='Algorithm'):",
        "    \"\"\"",
        "    Comprehensive evaluation of clustering results.    Returns multiple metrics and visualizations.",
        "    \"\"\"",
        "    from sklearn.metrics import calinski_harabasz_score",
        "        metrics = {",
        "        'silhouette': silhouette_score(X, labels) if len(set(labels)) > 1 else -1,",
        "        'davies_bouldin': davies_bouldin_score(X, labels) if len(set(labels)) > 1 else np.inf,",
        "        'calinski_harabasz': calinski_harabasz_score(X, labels) if len(set(labels)) > 1 else 0,",
        "        'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),",
        "        'n_noise': list(labels).count(-1) if -1 in labels else 0    }",
        "    print(f\"\\nüìä {algorithm_name} Evaluation:\")",
        "    print(f\"Silhouette Score: {metrics['silhouette']:.3f}\")",
        "    print(f\"Davies-Bouldin Index: {metrics['davies_bouldin']:.3f}\")",
        "    print(f\"Calinski-Harabasz Score: {metrics['calinski_harabasz']:.1f}\")",
        "    print(f\"Number of Clusters: {metrics['n_clusters']}\")",
        "    if metrics['n_noise'] > 0:",
        "    print(f\"Noise Points: {metrics['n_noise']}\")",
        "    return metrics",
        "",
        "",
        "def save_clustering_results(X, labels, algorithm_name, output_dir='./results'):",
        "    \"\"\"",
        "    Save clustering results for later analysis.    Creates visualizations and saves data.",
        "    \"\"\"",
        "    import os    os.makedirs(output_dir, exist_ok=True)",
        "    # Save data",
        "    results_df = pd.DataFrame(X, columns=['Feature_1', 'Feature_2'])",
        "    results_df['Cluster'] = labels",
        "    results_df['Algorithm'] = algorithm_name",
        "        filename = f\"{output_dir}/{algorithm_name.lower().replace(' ', '_')}_results.csv\"",
        "    results_df.to_csv(filename, index=False)",
        "    print(f\"‚úÖ Results saved to {filename}\")",
        "    return results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.2 Algorithm Demonstration Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Helper functions for Part 2\n",
        "\n",
        "def generate_blob_data(n_samples=1000, centers=3, n_features=2, cluster_std=1.0, random_state=42):\n",
        "    \"\"\"Generate simple blob data for demonstrations.\"\"\"\n",
        "    from sklearn.datasets import make_blobs\n",
        "    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features,\n",
        "                     cluster_std=cluster_std, random_state=random_state)\n",
        "    return X, y\n",
        "\n",
        "def plot_clusters(X, labels, centers=None, title=\"Clusters\", ax=None):\n",
        "    \"\"\"Simple cluster plotting function.\"\"\"\n",
        "    if ax is None:\n",
        "        import matplotlib.pyplot as plt\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    \n",
        "    unique_labels = np.unique(labels)\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
        "    \n",
        "    for i, label in enumerate(unique_labels):\n",
        "        if label == -1:  # Noise points for DBSCAN\n",
        "            mask = labels == label\n",
        "            ax.scatter(X[mask, 0], X[mask, 1], c='gray', marker='x', s=50, alpha=0.5, label='Noise')\n",
        "        else:\n",
        "            mask = labels == label\n",
        "            ax.scatter(X[mask, 0], X[mask, 1], c=[colors[i]], s=50, alpha=0.7, label=f'Cluster {label}')\n",
        "    \n",
        "    if centers is not None:\n",
        "        ax.scatter(centers[:, 0], centers[:, 1], c='red', marker='*', s=300, \n",
        "                  edgecolors='black', linewidth=2, label='Centers', zorder=10)\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    return ax\n",
        "\n",
        "def generate_innovation_data(n_samples=1000, n_features=10, n_clusters=5, noise=0.1):\n",
        "    \"\"\"Generate synthetic innovation dataset.\"\"\"\n",
        "    from sklearn.datasets import make_blobs\n",
        "    X, y = make_blobs(n_samples=n_samples, n_features=n_features, \n",
        "                     centers=n_clusters, cluster_std=1.5, random_state=42)\n",
        "    X += np.random.normal(0, noise, X.shape)\n",
        "    \n",
        "    feature_names = [\n",
        "        'Tech_Sophistication', 'Market_Readiness', 'Resource_Requirements',\n",
        "        'User_Engagement', 'Scalability', 'Innovation_Level',\n",
        "        'Competition_Intensity', 'Regulatory_Complexity', 'ROI_Potential',\n",
        "        'Implementation_Time'\n",
        "    ][:n_features]\n",
        "    \n",
        "    df = pd.DataFrame(X, columns=feature_names)\n",
        "    df['True_Cluster'] = y\n",
        "    \n",
        "    return df, X, y\n",
        "\n",
        "print(\"Helper functions loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0: All Functions\n",
        "\n",
        "### 0.1 Helper Functions for Data Generation and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section 3: Part 2 - Technical Deep Dive\n",
        "Master all clustering algorithms with hands-on implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 K-Means Clustering\n",
        "\n",
        "K-Means is the workhorse of clustering algorithms - simple, fast, and effective for many innovation analysis tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Theory & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'demonstrate_kmeans_step_by_step' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# K-Means step-by-step visualization\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m centers, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdemonstrate_kmeans_step_by_step\u001b[49m()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'demonstrate_kmeans_step_by_step' is not defined"
          ]
        }
      ],
      "source": [
        "# K-Means step-by-step visualization\n",
        "centers, labels = demonstrate_kmeans_step_by_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hands-on Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hands-on K-Means implementation\n",
        "results = demonstrate_kmeans_implementation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Exercise: Implement K-Means from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise: Implement K-means from scratch\n",
        "my_kmeans, sklearn_kmeans = implement_kmeans_from_scratch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Finding Optimal K\n",
        "\n",
        "One of the biggest challenges in clustering: How many clusters should we have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Elbow Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finding optimal K with elbow method\n",
        "optimal_k = find_optimal_k_elbow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Silhouette Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Silhouette analysis for cluster validation\n",
        "demonstrate_silhouette_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 DBSCAN - Density-Based Clustering\n",
        "\n",
        "DBSCAN finds clusters of arbitrary shape and identifies outliers - perfect for innovation data with noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DBSCAN parameter exploration\n",
        "X_dbscan = demonstrate_dbscan_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complex Shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Algorithm comparison on complex shapes\n",
        "compare_all_algorithms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Hierarchical Clustering\n",
        "\n",
        "Build a tree of clusters - perfect for understanding innovation taxonomies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hierarchical clustering demonstration\n",
        "X_hier = demonstrate_hierarchical_clustering()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Gaussian Mixture Models\n",
        "\n",
        "Soft clustering where innovations can belong to multiple categories with different probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gaussian Mixture Models demonstration\n",
        "demonstrate_gmm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.6 Algorithm Comparison\n",
        "\n",
        "Let's compare all algorithms on the same dataset to understand their strengths and weaknesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive algorithm comparison\n",
        "compare_all_algorithms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common Mistakes Gallery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common clustering mistakes and solutions\n",
        "demonstrate_common_mistakes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section 4: Part 3 - Design Integration\n",
        "Transform technical clustering results into actionable innovation insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 From Data Points to Innovation Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform clusters into innovation insights\n",
        "transform_clusters_to_insights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Creating Innovation Archetypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create innovation archetypes from clusters\n",
        "create_innovation_archetypes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Innovation Taxonomy & Lifecycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build innovation taxonomy\n",
        "build_innovation_taxonomy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Opportunity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate innovation opportunity analysis\n",
        "generate_opportunity_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5 Innovation Ecosystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create innovation ecosystem visualization\n",
        "create_innovation_ecosystem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéØ Part 2 Summary\n",
        "\n",
        "### Technical Skills Mastered:\n",
        "1. **K-Means**: Understanding and implementing from scratch\n",
        "2. **Optimal K**: Multiple methods for finding best clusters\n",
        "3. **DBSCAN**: Handling complex shapes and outliers\n",
        "4. **Hierarchical**: Building taxonomies and dendrograms\n",
        "5. **GMM**: Soft clustering with probabilities\n",
        "6. **Comparison**: Choosing the right algorithm\n",
        "\n",
        "### Design Applications Learned:\n",
        "1. **Innovation Archetypes**: Data-driven personas\n",
        "2. **Opportunity Heatmaps**: Identifying white spaces\n",
        "3. **Priority Matrices**: Strategic resource allocation\n",
        "4. **Ecosystem Networks**: Understanding connections\n",
        "5. **Innovation Taxonomy**: Hierarchical organization\n",
        "\n",
        "### Next: Part 3 - Practice & Advanced Topics\n",
        "Apply everything with real case studies and advanced visualizations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Part 2 Complete: Technical & Design Integration\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nYou've completed:\")\n",
        "print(\"‚Ä¢ Section 3: All clustering algorithms\")\n",
        "print(\"‚Ä¢ Section 4: Design integration and applications\")\n",
        "print(\"\\nüìö Ready for Part 3: Practice, Case Studies, and Advanced Topics\")\n",
        "print(\"\\nContinue with Week01_Part3_Practice_Advanced.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}