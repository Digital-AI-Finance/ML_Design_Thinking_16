{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 1: Clustering for Innovation - Part 2\n## Technical Implementation & Design Integration\n\nThis notebook continues from Part 1, covering the technical deep dive and design applications.\n\n**Part 2 Contents:**\n- Section 0: Complete Setup & ALL Functions (50+ total)\n- Section 3: Technical Deep Dive (function calls only)\n- Section 4: Design Integration (function calls only)\n\n**Note:** All code is organized as functions at the beginning for modularity and reusability.\n**Prerequisites:** Run Part 1 first to understand the foundation, or use the quick setup below.",
   "id": "17cf2d1a-3298-4fdd-8dcb-36fbc8b66c0b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Setup\n",
    "If you're starting directly with Part 2, run this cell to import essential functions from Part 1."
   ],
   "id": "d210f70d-40af-44ef-b79c-36d56066596f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports (if starting fresh)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Part 2 setup complete!\")"
   ],
   "id": "c1a3df20-cece-41b1-be00-1225a245ac56"
  },
  {
   "cell_type": "code",
   "source": "# Design Integration Functions\n\ndef transform_clusters_to_insights():\n    \"\"\"\n    Transform technical clustering results into actionable innovation insights.\n    Generate comprehensive innovation dataset and apply clustering.\n    \"\"\"\n    print(\"üí° Transforming Clusters into Innovation Insights\\n\")\n    \n    # Generate comprehensive innovation dataset\n    n_innovations = 1000\n    n_features = 10\n    n_clusters = 5\n    \n    # Generate base data\n    from sklearn.datasets import make_blobs\n    X_innovation, y_true = make_blobs(n_samples=n_innovations, \n                                     n_features=n_features,\n                                     centers=n_clusters,\n                                     cluster_std=1.2,\n                                     random_state=42)\n    \n    # Feature names\n    feature_names = [\n        'Technical_Complexity', 'Market_Readiness', 'Investment_Required',\n        'User_Impact', 'Implementation_Time', 'Risk_Level',\n        'Innovation_Score', 'Scalability', 'Regulatory_Compliance', 'ROI_Potential'\n    ]\n    \n    # Standardize\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_innovation)\n    \n    # Apply clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n    \n    # Create DataFrame\n    innovation_df = pd.DataFrame(X_innovation, columns=feature_names)\n    innovation_df['Cluster'] = labels\n    \n    # Visualize clusters in 2D using PCA\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X_scaled)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Scatter plot\n    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n    for i in range(n_clusters):\n        mask = labels == i\n        ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], \n                   c=[colors[i]], s=30, alpha=0.6,\n                   label=f'Cluster {i+1}', edgecolors='black', linewidth=0.5)\n    \n    # Add cluster centers\n    centers_pca = pca.transform(kmeans.cluster_centers_)\n    ax1.scatter(centers_pca[:, 0], centers_pca[:, 1],\n               c='black', marker='*', s=300,\n               edgecolors='white', linewidth=2, zorder=10)\n    \n    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n    ax1.set_title('Innovation Clusters (PCA Visualization)', fontsize=12, fontweight='bold')\n    ax1.legend(loc='best')\n    ax1.grid(True, alpha=0.3)\n    \n    # Cluster sizes\n    cluster_sizes = innovation_df['Cluster'].value_counts().sort_index()\n    ax2.bar(range(n_clusters), cluster_sizes.values, color=colors, alpha=0.7, edgecolor='black')\n    ax2.set_xlabel('Cluster', fontsize=11)\n    ax2.set_ylabel('Number of Innovations', fontsize=11)\n    ax2.set_title('Innovation Distribution Across Clusters', fontsize=12, fontweight='bold')\n    ax2.set_xticks(range(n_clusters))\n    ax2.set_xticklabels([f'Cluster {i+1}' for i in range(n_clusters)])\n    \n    # Add value labels\n    for i, v in enumerate(cluster_sizes.values):\n        ax2.text(i, v + 5, str(v), ha='center', fontweight='bold')\n    \n    plt.suptitle('Innovation Landscape Overview', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nüìä Innovation Clustering Results:\")\n    print(f\"Total Innovations: {n_innovations}\")\n    print(f\"Number of Clusters: {n_clusters}\")\n    print(f\"Average Cluster Size: {n_innovations/n_clusters:.0f}\")\n    print(f\"Silhouette Score: {silhouette_score(X_scaled, labels):.3f}\")\n    \n    return innovation_df, X_scaled, labels, kmeans\n\n\ndef create_innovation_archetypes():\n    \"\"\"\n    Create innovation archetypes from clusters with detailed characterization.\n    Maps clusters to meaningful innovation personas.\n    \"\"\"\n    print(\"üé≠ Creating Innovation Archetypes\\n\")\n    \n    # Get data from previous function or generate new\n    innovation_df, X_scaled, labels, kmeans = transform_clusters_to_insights()\n    \n    n_clusters = len(np.unique(labels))\n    \n    # Define archetype characteristics\n    archetype_names = [\n        'Digital Pioneers',\n        'Market Disruptors', \n        'Efficiency Optimizers',\n        'Customer Champions',\n        'Platform Builders'\n    ]\n    \n    archetype_descriptions = [\n        'High-tech, high-risk innovations targeting early adopters',\n        'Game-changing solutions that redefine market dynamics',\n        'Process improvements focusing on cost and time savings',\n        'User-centric innovations prioritizing experience',\n        'Ecosystem solutions creating network effects'\n    ]\n    \n    # Analyze each cluster\n    archetypes = []\n    feature_names = innovation_df.columns[:-1]  # Exclude 'Cluster' column\n    \n    for cluster_id in range(n_clusters):\n        cluster_data = innovation_df[innovation_df['Cluster'] == cluster_id]\n        \n        # Calculate statistics\n        archetype = {\n            'Cluster': cluster_id + 1,\n            'Name': archetype_names[cluster_id % len(archetype_names)],\n            'Description': archetype_descriptions[cluster_id % len(archetype_descriptions)],\n            'Size': len(cluster_data),\n            'Percentage': f\"{len(cluster_data)/len(innovation_df)*100:.1f}%\"\n        }\n        \n        # Top features\n        feature_means = cluster_data[feature_names].mean()\n        top_features = feature_means.nlargest(3).index.tolist()\n        archetype['Top_Features'] = ', '.join(top_features)\n        \n        # Risk profile\n        if 'Risk_Level' in cluster_data.columns:\n            risk_level = cluster_data['Risk_Level'].mean()\n            if risk_level > 0.5:\n                archetype['Risk_Profile'] = 'High Risk'\n            elif risk_level > -0.5:\n                archetype['Risk_Profile'] = 'Medium Risk'\n            else:\n                archetype['Risk_Profile'] = 'Low Risk'\n        \n        archetypes.append(archetype)\n    \n    # Create archetype cards visualization\n    fig, axes = plt.subplots(1, n_clusters, figsize=(18, 6))\n    if n_clusters == 1:\n        axes = [axes]\n    \n    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n    \n    for idx, archetype in enumerate(archetypes):\n        ax = plt.subplot(1, n_clusters, idx+1, projection='polar')\n        \n        # Create radar chart for each archetype\n        cluster_data = innovation_df[innovation_df['Cluster'] == idx]\n        feature_values = cluster_data[feature_names[:6]].mean().values\n        \n        # Normalize to 0-1 scale\n        feature_values = (feature_values - feature_values.min()) / (feature_values.max() - feature_values.min() + 1e-10)\n        \n        # Create radar chart\n        angles = np.linspace(0, 2*np.pi, len(feature_names[:6]), endpoint=False)\n        feature_values = np.concatenate((feature_values, [feature_values[0]]))\n        angles = np.concatenate((angles, [angles[0]]))\n        \n        ax.plot(angles, feature_values, 'o-', linewidth=2, color=colors[idx])\n        ax.fill(angles, feature_values, alpha=0.25, color=colors[idx])\n        ax.set_xticks(angles[:-1])\n        ax.set_xticklabels([f.replace('_', '\\n') for f in feature_names[:6]], fontsize=8)\n        ax.set_ylim(0, 1)\n        ax.set_title(f\"{archetype['Name']}\\n({archetype['Size']} innovations)\", \n                    fontsize=10, fontweight='bold', pad=20)\n        ax.grid(True)\n    \n    plt.suptitle('Innovation Archetype Profiles', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Display archetype summary\n    archetypes_df = pd.DataFrame(archetypes)\n    print(\"\\nüìã Innovation Archetype Summary:\")\n    display(archetypes_df[['Name', 'Size', 'Percentage', 'Risk_Profile', 'Top_Features']])\n    \n    print(\"\\nüí° How to Use Archetypes:\")\n    print(\"‚Ä¢ Tailor innovation strategies per archetype\")\n    print(\"‚Ä¢ Allocate resources based on archetype characteristics\")\n    print(\"‚Ä¢ Design specific support programs for each type\")\n    print(\"‚Ä¢ Track archetype evolution over time\")\n    \n    return archetypes_df, innovation_df\n\n\ndef generate_opportunity_analysis():\n    \"\"\"\n    Generate comprehensive opportunity analysis with heatmaps and priority matrices.\n    Identifies white spaces and strategic opportunities.\n    \"\"\"\n    print(\"üî• Innovation Opportunity Analysis\\n\")\n    \n    # Get clustered data\n    innovation_df, X_scaled, labels, kmeans = transform_clusters_to_insights()\n    n_clusters = len(np.unique(labels))\n    \n    # Calculate opportunity scores\n    opportunity_dimensions = [\n        'Market_Size', 'Growth_Rate', 'Competition',\n        'Tech_Readiness', 'Investment_Need', 'Time_to_Market',\n        'Risk_Level', 'Regulatory', 'Customer_Demand'\n    ]\n    \n    # Create opportunity matrix\n    np.random.seed(42)\n    opportunity_matrix = np.random.randn(n_clusters, len(opportunity_dimensions))\n    \n    # Create heatmap\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Heatmap\n    im = ax1.imshow(opportunity_matrix, cmap='RdYlGn', aspect='auto', vmin=-2, vmax=2)\n    ax1.set_xticks(range(len(opportunity_dimensions)))\n    ax1.set_xticklabels(opportunity_dimensions, rotation=45, ha='right')\n    ax1.set_yticks(range(n_clusters))\n    ax1.set_yticklabels([f'Cluster {i+1}' for i in range(n_clusters)])\n    ax1.set_title('Innovation Opportunity Heatmap', fontsize=12, fontweight='bold')\n    \n    # Add values\n    for i in range(n_clusters):\n        for j in range(len(opportunity_dimensions)):\n            text = ax1.text(j, i, f'{opportunity_matrix[i, j]:.1f}',\n                           ha='center', va='center', color='black', fontsize=8)\n    \n    plt.colorbar(im, ax=ax1, label='Opportunity Score')\n    \n    # Priority matrix\n    cluster_sizes = innovation_df['Cluster'].value_counts().sort_index()\n    impact = innovation_df.groupby('Cluster')['User_Impact'].mean().values\n    effort = innovation_df.groupby('Cluster')['Implementation_Time'].mean().values\n    \n    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n    \n    ax2.scatter(effort, impact, s=cluster_sizes.values*2, c=colors, \n               alpha=0.6, edgecolors='black', linewidth=2)\n    \n    for i in range(n_clusters):\n        ax2.annotate(f'C{i+1}', (effort[i], impact[i]),\n                    ha='center', va='center', fontsize=9, fontweight='bold')\n    \n    # Add quadrant lines\n    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n    ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n    \n    # Add quadrant labels\n    ax2.text(1, 1, 'High Impact\\nHigh Effort', ha='center', va='center', \n            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n    ax2.text(-1, 1, 'High Impact\\nLow Effort', ha='center', va='center', \n            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n    ax2.text(-1, -1, 'Low Impact\\nLow Effort', ha='center', va='center', \n            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n    ax2.text(1, -1, 'Low Impact\\nHigh Effort', ha='center', va='center', \n            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.3))\n    \n    ax2.set_xlabel('Implementation Effort', fontsize=11)\n    ax2.set_ylabel('User Impact', fontsize=11)\n    ax2.set_title('Innovation Priority Matrix', fontsize=12, fontweight='bold')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüéØ Strategic Recommendations:\")\n    print(\"\\nüü¢ Quick Wins (High Impact, Low Effort):\")\n    print(\"  ‚Ä¢ Focus immediate resources here\")\n    print(\"  ‚Ä¢ Rapid prototyping and testing\")\n    print(\"\\nüü° Strategic Initiatives (High Impact, High Effort):\")\n    print(\"  ‚Ä¢ Long-term investment required\")\n    print(\"  ‚Ä¢ Build dedicated teams\")\n    print(\"\\nüîµ Fill-ins (Low Impact, Low Effort):\")\n    print(\"  ‚Ä¢ Good for learning and experimentation\")\n    print(\"  ‚Ä¢ Assign to junior teams\")\n    print(\"\\nüî¥ Avoid (Low Impact, High Effort):\")\n    print(\"  ‚Ä¢ Deprioritize or eliminate\")\n    print(\"  ‚Ä¢ Redirect resources elsewhere\")\n    \n    return opportunity_matrix, innovation_df\n\n\ndef build_innovation_taxonomy():\n    \"\"\"\n    Build hierarchical innovation taxonomy using hierarchical clustering.\n    Shows relationships between innovation clusters.\n    \"\"\"\n    print(\"üå≥ Building Innovation Taxonomy\\n\")\n    \n    # Get cluster centers from k-means\n    _, X_scaled, labels, kmeans = transform_clusters_to_insights()\n    \n    # Use cluster centers for hierarchical clustering\n    from scipy.cluster.hierarchy import dendrogram, linkage\n    \n    archetype_names = [\n        'Digital Pioneers',\n        'Market Disruptors',\n        'Efficiency Optimizers',\n        'Customer Champions',\n        'Platform Builders'\n    ]\n    \n    # Hierarchical clustering on centers\n    linkage_matrix = linkage(kmeans.cluster_centers_, method='ward')\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Dendrogram\n    dendrogram(linkage_matrix, ax=ax1, labels=archetype_names,\n              color_threshold=0, above_threshold_color='gray')\n    ax1.set_title('Innovation Taxonomy Hierarchy', fontsize=12, fontweight='bold')\n    ax1.set_xlabel('Innovation Archetype')\n    ax1.set_ylabel('Distance')\n    \n    # Lifecycle stages\n    lifecycle_stages = ['Ideation', 'Validation', 'Development', 'Launch', 'Scale', 'Maturity']\n    n_clusters = len(kmeans.cluster_centers_)\n    stage_distribution = np.random.dirichlet(np.ones(len(lifecycle_stages)), size=n_clusters)\n    \n    # Stack bar chart for lifecycle\n    bottom = np.zeros(n_clusters)\n    stage_colors = plt.cm.coolwarm(np.linspace(0, 1, len(lifecycle_stages)))\n    \n    for stage_idx, stage in enumerate(lifecycle_stages):\n        values = stage_distribution[:, stage_idx]\n        ax2.bar(range(n_clusters), values, bottom=bottom, \n               color=stage_colors[stage_idx], label=stage, alpha=0.8)\n        bottom += values\n    \n    ax2.set_xlabel('Innovation Archetype', fontsize=11)\n    ax2.set_ylabel('Proportion', fontsize=11)\n    ax2.set_title('Innovation Lifecycle Distribution by Archetype', fontsize=12, fontweight='bold')\n    ax2.set_xticks(range(n_clusters))\n    ax2.set_xticklabels([name.split()[0] for name in archetype_names], rotation=45)\n    ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìä Taxonomy Insights:\")\n    print(\"‚Ä¢ Digital Pioneers and Market Disruptors are closely related\")\n    print(\"‚Ä¢ Efficiency Optimizers form a distinct branch\")\n    print(\"‚Ä¢ Platform Builders bridge multiple categories\")\n    \n    return linkage_matrix, stage_distribution\n\n\ndef create_innovation_ecosystem():\n    \"\"\"\n    Create innovation ecosystem network showing relationships between\n    archetypes and stakeholders.\n    \"\"\"\n    print(\"üåê Innovation Ecosystem Network\\n\")\n    \n    import networkx as nx\n    \n    # Create network graph\n    G = nx.Graph()\n    \n    archetype_names = [\n        'Digital Pioneers',\n        'Market Disruptors',\n        'Efficiency Optimizers',\n        'Customer Champions',\n        'Platform Builders'\n    ]\n    \n    # Add nodes for archetypes\n    cluster_sizes = [200, 180, 150, 170, 200]  # Example sizes\n    for i, name in enumerate(archetype_names):\n        G.add_node(name, node_type='archetype', size=cluster_sizes[i])\n    \n    # Add stakeholder nodes\n    stakeholders = ['Customers', 'Partners', 'Investors', 'Regulators', 'Competitors']\n    for stakeholder in stakeholders:\n        G.add_node(stakeholder, node_type='stakeholder', size=100)\n    \n    # Add edges (connections)\n    connections = [\n        ('Digital Pioneers', 'Investors', 0.8),\n        ('Digital Pioneers', 'Partners', 0.6),\n        ('Market Disruptors', 'Competitors', 0.9),\n        ('Market Disruptors', 'Customers', 0.7),\n        ('Efficiency Optimizers', 'Partners', 0.8),\n        ('Efficiency Optimizers', 'Regulators', 0.5),\n        ('Customer Champions', 'Customers', 0.9),\n        ('Customer Champions', 'Partners', 0.6),\n        ('Platform Builders', 'Partners', 0.9),\n        ('Platform Builders', 'Investors', 0.7)\n    ]\n    \n    for source, target, weight in connections:\n        G.add_edge(source, target, weight=weight)\n    \n    # Visualize network\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    # Layout\n    pos = nx.spring_layout(G, k=2, iterations=50)\n    \n    # Draw nodes\n    archetype_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'archetype']\n    stakeholder_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'stakeholder']\n    \n    colors = plt.cm.Set3(np.linspace(0, 1, len(archetype_nodes)))\n    \n    # Archetype nodes\n    nx.draw_networkx_nodes(G, pos, nodelist=archetype_nodes,\n                          node_color=colors,\n                          node_size=[G.nodes[n]['size']*5 for n in archetype_nodes],\n                          alpha=0.7, ax=ax)\n    \n    # Stakeholder nodes\n    nx.draw_networkx_nodes(G, pos, nodelist=stakeholder_nodes,\n                          node_color='lightgray',\n                          node_size=500,\n                          node_shape='s',\n                          alpha=0.8, ax=ax)\n    \n    # Draw edges\n    edges = G.edges()\n    weights = [G[u][v]['weight'] for u, v in edges]\n    nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights],\n                          alpha=0.5, ax=ax)\n    \n    # Labels\n    labels = {n: n.split()[0] if len(n.split()) > 1 else n for n in G.nodes()}\n    nx.draw_networkx_labels(G, pos, labels, font_size=10, font_weight='bold', ax=ax)\n    \n    ax.set_title('Innovation Ecosystem Network', fontsize=14, fontweight='bold')\n    ax.axis('off')\n    \n    # Add legend\n    from matplotlib.patches import Rectangle, Circle\n    legend_elements = [\n        Circle((0, 0), 0.1, facecolor=colors[0], alpha=0.7, label='Innovation Archetypes'),\n        Rectangle((0, 0), 0.1, 0.1, facecolor='lightgray', alpha=0.8, label='Stakeholders')\n    ]\n    ax.legend(handles=legend_elements, loc='upper right')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüåê Ecosystem Insights:\")\n    print(\"‚Ä¢ Digital Pioneers have strong investor connections\")\n    print(\"‚Ä¢ Customer Champions directly connect with users\")\n    print(\"‚Ä¢ Platform Builders bridge multiple stakeholder groups\")\n    print(\"‚Ä¢ Market Disruptors create competitive tension\")\n    print(\"\\nüí° Use this network to:\")\n    print(\"‚Ä¢ Identify collaboration opportunities\")\n    print(\"‚Ä¢ Understand influence patterns\")\n    print(\"‚Ä¢ Design stakeholder engagement strategies\")\n    \n    return G\n\nprint(\"Design integration functions loaded successfully!\")\nprint(\"\\n‚úÖ All functions for Part 2 are now ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cd7760ad-cab6-4ebd-a3c5-46252bf01049"
  },
  {
   "cell_type": "markdown",
   "source": "### 0.3 Design Integration Functions",
   "metadata": {},
   "id": "7585dee6-8b11-4af9-b1ae-e47b4a73db86"
  },
  {
   "cell_type": "code",
   "source": "# Algorithm Demonstration Functions\n\ndef demonstrate_kmeans_step_by_step(X=None, n_clusters=3, n_iterations=5):\n    \"\"\"\n    Visualize K-means algorithm step by step.\n    Shows how centers converge to optimal positions.\n    \"\"\"\n    print(\"üéØ K-Means Clustering: Step-by-Step Process\\n\")\n    \n    if X is None:\n        X, y_true = generate_blob_data(n_samples=300, centers=3, cluster_std=0.8)\n    \n    np.random.seed(42)\n    \n    # Initialize random centers\n    idx = np.random.choice(len(X), n_clusters, replace=False)\n    centers = X[idx].copy()\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n    \n    for iteration in range(n_iterations):\n        ax = axes[iteration]\n        \n        # Assign points to nearest center\n        distances = np.zeros((len(X), n_clusters))\n        for k in range(n_clusters):\n            distances[:, k] = np.linalg.norm(X - centers[k], axis=1)\n        labels = np.argmin(distances, axis=1)\n        \n        # Visualize current state\n        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n        for k in range(n_clusters):\n            mask = labels == k\n            ax.scatter(X[mask, 0], X[mask, 1], c=colors[k], \n                      s=50, alpha=0.6, label=f'Cluster {k+1}')\n        \n        # Plot centers\n        ax.scatter(centers[:, 0], centers[:, 1], c='red', \n                  marker='*', s=300, edgecolors='black', \n                  linewidth=2, label='Centers', zorder=10)\n        \n        # Update centers\n        new_centers = np.zeros_like(centers)\n        for k in range(n_clusters):\n            if np.sum(labels == k) > 0:\n                new_centers[k] = X[labels == k].mean(axis=0)\n            else:\n                new_centers[k] = centers[k]\n        \n        # Draw movement arrows\n        for k in range(n_clusters):\n            ax.arrow(centers[k, 0], centers[k, 1],\n                    new_centers[k, 0] - centers[k, 0],\n                    new_centers[k, 1] - centers[k, 1],\n                    head_width=0.1, head_length=0.1,\n                    fc='black', ec='black', alpha=0.5)\n        \n        centers = new_centers.copy()\n        \n        ax.set_title(f'Iteration {iteration + 1}', fontsize=12, fontweight='bold')\n        ax.set_xlabel('Feature 1')\n        ax.set_ylabel('Feature 2')\n        if iteration == 0:\n            ax.legend(loc='upper right', fontsize=8)\n    \n    # Final result\n    ax = axes[5]\n    for k in range(n_clusters):\n        mask = labels == k\n        ax.scatter(X[mask, 0], X[mask, 1], c=colors[k], \n                  s=50, alpha=0.6, label=f'Cluster {k+1}')\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', \n              marker='*', s=300, edgecolors='black', \n              linewidth=2, label='Final Centers', zorder=10)\n    ax.set_title('Final Result', fontsize=12, fontweight='bold')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.legend(loc='upper right', fontsize=8)\n    \n    plt.suptitle('K-Means Algorithm: Watch Centers Converge to Optimal Positions', \n                fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    silhouette = silhouette_score(X, labels)\n    print(f\"\\nüìä Algorithm Performance:\")\n    print(f\"Silhouette Score: {silhouette:.3f}\")\n    print(f\"Converged in 5 iterations\")\n    \n    return centers, labels\n\n\ndef demonstrate_kmeans_implementation():\n    \"\"\"\n    Hands-on K-Means implementation with different K values.\n    Shows impact of K on clustering quality.\n    \"\"\"\n    print(\"üîß Hands-on K-Means Implementation\\n\")\n    \n    # Create innovation dataset\n    df, X_scaled, y_true = generate_innovation_data(n_samples=1000, n_features=10, n_clusters=4)\n    \n    print(f\"Dataset shape: {X_scaled.shape}\")\n    print(f\"Features: {', '.join(df.columns[:10])}\\n\")\n    \n    # Apply K-means with different K values\n    k_values = [2, 3, 4, 5, 6]\n    results = {}\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n    \n    # Use PCA for visualization\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(StandardScaler().fit_transform(X_scaled))\n    \n    for idx, k in enumerate(k_values):\n        # Fit K-means\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        labels = kmeans.fit_predict(X_scaled)\n        \n        # Calculate metrics\n        silhouette = silhouette_score(X_scaled, labels)\n        inertia = kmeans.inertia_\n        \n        results[k] = {\n            'labels': labels,\n            'centers': kmeans.cluster_centers_,\n            'silhouette': silhouette,\n            'inertia': inertia\n        }\n        \n        # Visualize\n        ax = axes[idx]\n        scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n                            cmap='viridis', s=20, alpha=0.6)\n        \n        # Plot centers in PCA space\n        centers_pca = pca.transform(StandardScaler().fit_transform(kmeans.cluster_centers_))\n        ax.scatter(centers_pca[:, 0], centers_pca[:, 1],\n                  c='red', marker='*', s=300, \n                  edgecolors='black', linewidth=2)\n        \n        ax.set_title(f'K={k}, Silhouette={silhouette:.3f}', \n                    fontsize=11, fontweight='bold')\n        ax.set_xlabel('First Principal Component')\n        ax.set_ylabel('Second Principal Component')\n        plt.colorbar(scatter, ax=ax)\n    \n    # Hide extra subplot\n    axes[-1].set_visible(False)\n    \n    plt.suptitle('K-Means with Different K Values (PCA Visualization)', \n                fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Print comparison\n    print(\"\\nüìä K-Means Results Comparison:\")\n    print(\"K | Silhouette | Inertia\")\n    print(\"-\" * 30)\n    for k, metrics in results.items():\n        print(f\"{k} | {metrics['silhouette']:.3f}      | {metrics['inertia']:.1f}\")\n    \n    # Best K\n    best_k = max(results.keys(), key=lambda k: results[k]['silhouette'])\n    print(f\"\\n‚ú® Best K={best_k} with silhouette score {results[best_k]['silhouette']:.3f}\")\n    \n    return results\n\n\ndef implement_kmeans_from_scratch():\n    \"\"\"\n    Exercise: Implement K-means from scratch.\n    Compare with sklearn implementation.\n    \"\"\"\n    print(\"üéØ Exercise: Implement K-Means from Scratch\\n\")\n    \n    class MyKMeans:\n        \"\"\"Simple K-Means implementation for learning\"\"\"\n        \n        def __init__(self, n_clusters=3, max_iters=100, tol=1e-4):\n            self.n_clusters = n_clusters\n            self.max_iters = max_iters\n            self.tol = tol\n            self.centers = None\n            self.labels = None\n        \n        def fit(self, X):\n            \"\"\"Fit K-means to data\"\"\"\n            n_samples = X.shape[0]\n            \n            # Initialize centers randomly\n            idx = np.random.choice(n_samples, self.n_clusters, replace=False)\n            self.centers = X[idx].copy()\n            \n            for iteration in range(self.max_iters):\n                # Assign points to nearest center\n                distances = np.zeros((n_samples, self.n_clusters))\n                for k in range(self.n_clusters):\n                    distances[:, k] = np.linalg.norm(X - self.centers[k], axis=1)\n                self.labels = np.argmin(distances, axis=1)\n                \n                # Update centers\n                new_centers = np.zeros_like(self.centers)\n                for k in range(self.n_clusters):\n                    if np.sum(self.labels == k) > 0:\n                        new_centers[k] = X[self.labels == k].mean(axis=0)\n                    else:\n                        new_centers[k] = self.centers[k]\n                \n                # Check convergence\n                if np.linalg.norm(new_centers - self.centers) < self.tol:\n                    print(f\"Converged at iteration {iteration + 1}\")\n                    break\n                \n                self.centers = new_centers\n            \n            return self\n        \n        def predict(self, X):\n            \"\"\"Predict cluster labels\"\"\"\n            distances = np.zeros((X.shape[0], self.n_clusters))\n            for k in range(self.n_clusters):\n                distances[:, k] = np.linalg.norm(X - self.centers[k], axis=1)\n            return np.argmin(distances, axis=1)\n    \n    # Test implementation\n    X_test, _ = generate_blob_data(n_samples=200, centers=3)\n    \n    # Your implementation\n    my_kmeans = MyKMeans(n_clusters=3)\n    my_kmeans.fit(X_test)\n    my_labels = my_kmeans.labels\n    \n    # Sklearn implementation\n    sklearn_kmeans = KMeans(n_clusters=3, random_state=42)\n    sklearn_labels = sklearn_kmeans.fit_predict(X_test)\n    \n    # Compare results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    ax1.scatter(X_test[:, 0], X_test[:, 1], c=my_labels, cmap='viridis', s=50)\n    ax1.scatter(my_kmeans.centers[:, 0], my_kmeans.centers[:, 1],\n               c='red', marker='*', s=300, edgecolors='black', linewidth=2)\n    ax1.set_title('Your Implementation', fontsize=12, fontweight='bold')\n    ax1.set_xlabel('Feature 1')\n    ax1.set_ylabel('Feature 2')\n    \n    ax2.scatter(X_test[:, 0], X_test[:, 1], c=sklearn_labels, cmap='viridis', s=50)\n    ax2.scatter(sklearn_kmeans.cluster_centers_[:, 0], \n               sklearn_kmeans.cluster_centers_[:, 1],\n               c='red', marker='*', s=300, edgecolors='black', linewidth=2)\n    ax2.set_title('Sklearn Implementation', fontsize=12, fontweight='bold')\n    ax2.set_xlabel('Feature 1')\n    ax2.set_ylabel('Feature 2')\n    \n    plt.suptitle('K-Means Implementation Comparison', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\n‚úÖ Your implementation silhouette score: {silhouette_score(X_test, my_labels):.3f}\")\n    print(f\"‚úÖ Sklearn silhouette score: {silhouette_score(X_test, sklearn_labels):.3f}\")\n    \n    return my_kmeans, sklearn_kmeans\n\n\ndef find_optimal_k_elbow():\n    \"\"\"\n    Comprehensive elbow method analysis with multiple metrics.\n    Shows how to find the optimal number of clusters.\n    \"\"\"\n    print(\"üìà Finding Optimal K: The Elbow Method\\n\")\n    \n    # Generate data with known clusters\n    X_elbow, y_true = generate_blob_data(n_samples=500, centers=4, cluster_std=1.0)\n    \n    # Test range of K values\n    k_range = range(1, 11)\n    inertias = []\n    silhouettes = []\n    davies_bouldins = []\n    \n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        labels = kmeans.fit_predict(X_elbow)\n        \n        inertias.append(kmeans.inertia_)\n        \n        if k > 1:  # Metrics need at least 2 clusters\n            silhouettes.append(silhouette_score(X_elbow, labels))\n            davies_bouldins.append(davies_bouldin_score(X_elbow, labels))\n        else:\n            silhouettes.append(0)\n            davies_bouldins.append(0)\n    \n    # Calculate elbow point\n    deltas = np.diff(inertias)\n    delta_deltas = np.diff(deltas)\n    elbow_idx = np.argmax(np.abs(delta_deltas)) + 2  # +2 because of double diff\n    \n    # Visualization\n    fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n    \n    # Inertia/Elbow plot\n    ax1 = axes[0]\n    ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n    ax1.axvline(x=list(k_range)[elbow_idx], color='red', \n               linestyle='--', alpha=0.7, label=f'Elbow at k={list(k_range)[elbow_idx]}')\n    ax1.set_xlabel('Number of Clusters (k)', fontsize=11)\n    ax1.set_ylabel('Inertia', fontsize=11)\n    ax1.set_title('Elbow Method', fontsize=12, fontweight='bold')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Silhouette scores\n    ax2 = axes[1]\n    ax2.plot(k_range[1:], silhouettes[1:], 'go-', linewidth=2, markersize=8)\n    best_silhouette_k = list(k_range)[np.argmax(silhouettes) if silhouettes else 0]\n    ax2.axvline(x=best_silhouette_k, color='red', linestyle='--', \n               alpha=0.7, label=f'Best at k={best_silhouette_k}')\n    ax2.set_xlabel('Number of Clusters (k)', fontsize=11)\n    ax2.set_ylabel('Silhouette Score', fontsize=11)\n    ax2.set_title('Silhouette Analysis', fontsize=12, fontweight='bold')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Davies-Bouldin Index (lower is better)\n    ax3 = axes[2]\n    ax3.plot(k_range[1:], davies_bouldins[1:], 'ro-', linewidth=2, markersize=8)\n    best_db_k = list(k_range)[np.argmin(davies_bouldins[1:]) + 1 if davies_bouldins[1:] else 0]\n    ax3.axvline(x=best_db_k, color='green', linestyle='--', \n               alpha=0.7, label=f'Best at k={best_db_k}')\n    ax3.set_xlabel('Number of Clusters (k)', fontsize=11)\n    ax3.set_ylabel('Davies-Bouldin Index', fontsize=11)\n    ax3.set_title('Davies-Bouldin Index (lower is better)', fontsize=12, fontweight='bold')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    plt.suptitle('Multiple Methods for Finding Optimal K', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìä Optimal K Recommendations:\")\n    print(f\"Elbow Method: k={list(k_range)[elbow_idx]}\")\n    print(f\"Silhouette Score: k={best_silhouette_k}\")\n    print(f\"Davies-Bouldin Index: k={best_db_k}\")\n    print(f\"\\nTrue number of clusters: 4\")\n    print(\"\\nüí° Tip: When methods disagree, consider domain knowledge and use case!\")\n    \n    return {'elbow_k': list(k_range)[elbow_idx], 'silhouette_k': best_silhouette_k, 'db_k': best_db_k}\n\n\ndef demonstrate_dbscan_parameters():\n    \"\"\"\n    DBSCAN parameter exploration showing impact of eps and min_samples.\n    Helps understand how to tune DBSCAN for different datasets.\n    \"\"\"\n    print(\"üîç DBSCAN: Understanding eps and min_samples\\n\")\n    \n    # Generate data with outliers\n    X_dbscan, _ = generate_blob_data(n_samples=300, centers=3, cluster_std=0.5)\n    # Add noise points\n    X_noise = np.random.uniform(-6, 6, (50, 2))\n    X_dbscan = np.vstack([X_dbscan, X_noise])\n    \n    # Test different parameter combinations\n    eps_values = [0.3, 0.5, 0.7, 1.0]\n    min_samples_values = [3, 5, 10, 20]\n    \n    fig, axes = plt.subplots(len(eps_values), len(min_samples_values), \n                            figsize=(16, 12))\n    \n    for i, eps in enumerate(eps_values):\n        for j, min_samples in enumerate(min_samples_values):\n            ax = axes[i, j]\n            \n            # Apply DBSCAN\n            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n            labels = dbscan.fit_predict(X_dbscan)\n            \n            # Count clusters and noise\n            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n            n_noise = list(labels).count(-1)\n            \n            # Plot\n            unique_labels = set(labels)\n            colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n            \n            for k, col in zip(unique_labels, colors):\n                if k == -1:\n                    col = 'black'\n                    marker = 'x'\n                else:\n                    marker = 'o'\n                \n                class_member_mask = (labels == k)\n                xy = X_dbscan[class_member_mask]\n                ax.scatter(xy[:, 0], xy[:, 1], c=[col], \n                          marker=marker, s=30, alpha=0.7)\n            \n            ax.set_title(f'eps={eps}, min={min_samples}\\nC={n_clusters}, N={n_noise}',\n                        fontsize=9)\n            ax.set_xticks([])\n            ax.set_yticks([])\n    \n    # Add labels\n    for i, eps in enumerate(eps_values):\n        axes[i, 0].set_ylabel(f'eps={eps}', fontsize=10, fontweight='bold')\n    for j, min_samples in enumerate(min_samples_values):\n        axes[0, j].set_xlabel(f'min_samples={min_samples}', fontsize=10, fontweight='bold')\n        axes[0, j].xaxis.set_label_position('top')\n    \n    plt.suptitle('DBSCAN Parameter Grid: Impact of eps and min_samples\\n'\n                'C=Clusters, N=Noise points', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìö Parameter Guidelines:\")\n    print(\"‚Ä¢ eps: Maximum distance between points in same neighborhood\")\n    print(\"  - Too small: Many clusters, more noise\")\n    print(\"  - Too large: Few clusters, points merge\")\n    print(\"\\n‚Ä¢ min_samples: Minimum points to form dense region\")\n    print(\"  - Too small: More clusters, less noise\")\n    print(\"  - Too large: Fewer clusters, more noise\")\n    print(\"\\nüí° Start with min_samples = 2 * dimensions, adjust eps based on data\")\n    \n    return X_dbscan\n\n\ndef demonstrate_hierarchical_clustering():\n    \"\"\"\n    Hierarchical clustering demonstration with dendrograms.\n    Shows different linkage methods and how to cut the tree.\n    \"\"\"\n    print(\"üå≥ Hierarchical Clustering: Building Innovation Taxonomy\\n\")\n    \n    # Generate hierarchical data\n    X_hier, y_hier = generate_blob_data(n_samples=100, centers=4, cluster_std=0.5)\n    \n    # Different linkage methods\n    linkage_methods = ['ward', 'complete', 'average', 'single']\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.flatten()\n    \n    for idx, method in enumerate(linkage_methods):\n        ax = axes[idx]\n        \n        # Perform hierarchical clustering\n        from scipy.cluster.hierarchy import dendrogram, linkage\n        linkage_matrix = linkage(X_hier, method=method)\n        \n        # Plot dendrogram\n        dendrogram(linkage_matrix, ax=ax, truncate_mode='level', \n                  p=5, color_threshold=0, above_threshold_color='gray')\n        \n        ax.set_title(f'Linkage: {method.capitalize()}', fontsize=12, fontweight='bold')\n        ax.set_xlabel('Sample Index')\n        ax.set_ylabel('Distance')\n    \n    plt.suptitle('Hierarchical Clustering with Different Linkage Methods', \n                fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìä Linkage Method Comparison:\")\n    print(\"‚Ä¢ Ward: Minimizes within-cluster variance (most common)\")\n    print(\"‚Ä¢ Complete: Maximum distance between clusters\")\n    print(\"‚Ä¢ Average: Average distance between all pairs\")\n    print(\"‚Ä¢ Single: Minimum distance (can create chains)\")\n    \n    return X_hier\n\n\ndef demonstrate_gmm():\n    \"\"\"\n    Gaussian Mixture Models demonstration.\n    Shows soft clustering with probabilities.\n    \"\"\"\n    print(\"üîÆ Gaussian Mixture Models: Soft Clustering\\n\")\n    \n    # Generate overlapping clusters\n    X_gmm, y_gmm = generate_blob_data(n_samples=400, centers=3, cluster_std=1.2)\n    \n    # Fit GMM\n    from sklearn.mixture import GaussianMixture\n    gmm = GaussianMixture(n_components=3, random_state=42)\n    gmm.fit(X_gmm)\n    \n    # Get predictions and probabilities\n    gmm_labels = gmm.predict(X_gmm)\n    gmm_probs = gmm.predict_proba(X_gmm)\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Hard clustering\n    ax1 = axes[0]\n    scatter1 = ax1.scatter(X_gmm[:, 0], X_gmm[:, 1], c=gmm_labels, \n                          cmap='viridis', s=30, alpha=0.7)\n    ax1.scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n               c='red', marker='*', s=300, edgecolors='black', linewidth=2)\n    ax1.set_title('Hard Assignment', fontsize=11, fontweight='bold')\n    ax1.set_xlabel('Feature 1')\n    ax1.set_ylabel('Feature 2')\n    plt.colorbar(scatter1, ax=ax1)\n    \n    # Soft clustering - show uncertainty\n    ax2 = axes[1]\n    uncertainty = -np.sum(gmm_probs * np.log(gmm_probs + 1e-10), axis=1)\n    scatter2 = ax2.scatter(X_gmm[:, 0], X_gmm[:, 1], c=uncertainty, \n                          cmap='RdYlGn_r', s=30, alpha=0.7)\n    ax2.set_title('Uncertainty', fontsize=11, fontweight='bold')\n    ax2.set_xlabel('Feature 1')\n    ax2.set_ylabel('Feature 2')\n    plt.colorbar(scatter2, ax=ax2, label='Uncertainty')\n    \n    # Probability contours\n    ax3 = axes[2]\n    x = np.linspace(X_gmm[:, 0].min() - 1, X_gmm[:, 0].max() + 1, 100)\n    y = np.linspace(X_gmm[:, 1].min() - 1, X_gmm[:, 1].max() + 1, 100)\n    X_grid, Y_grid = np.meshgrid(x, y)\n    XX = np.array([X_grid.ravel(), Y_grid.ravel()]).T\n    Z = -gmm.score_samples(XX)\n    Z = Z.reshape(X_grid.shape)\n    \n    ax3.contour(X_grid, Y_grid, Z, levels=10, linewidths=0.5, colors='black', alpha=0.3)\n    ax3.contourf(X_grid, Y_grid, Z, levels=10, cmap='viridis', alpha=0.3)\n    ax3.scatter(X_gmm[:, 0], X_gmm[:, 1], c=gmm_labels, \n               cmap='viridis', s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n    ax3.scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n               c='red', marker='*', s=300, edgecolors='white', linewidth=2)\n    ax3.set_title('Probability Contours', fontsize=11, fontweight='bold')\n    ax3.set_xlabel('Feature 1')\n    ax3.set_ylabel('Feature 2')\n    \n    plt.suptitle('Gaussian Mixture Models: Soft vs Hard Clustering', \n                fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Show probability examples\n    print(\"\\nüìä Example: Innovation Probability Assignments\")\n    print(\"\\nSample 5 innovations and their cluster probabilities:\")\n    print(\"ID | Cluster 1 | Cluster 2 | Cluster 3 | Assigned\")\n    print(\"-\" * 50)\n    for i in range(5):\n        probs = gmm_probs[i]\n        assigned = gmm_labels[i]\n        print(f\"{i:2} | {probs[0]:.3f}    | {probs[1]:.3f}    | \"\n              f\"{probs[2]:.3f}    | Cluster {assigned+1}\")\n    \n    print(\"\\nüí° GMM Benefits:\")\n    print(\"‚Ä¢ Shows uncertainty in cluster assignments\")\n    print(\"‚Ä¢ Handles overlapping clusters\")\n    print(\"‚Ä¢ Provides probability distributions\")\n    \n    return gmm, X_gmm, gmm_labels\n\n\ndef compare_all_algorithms():\n    \"\"\"\n    Comprehensive comparison of all clustering algorithms.\n    Shows strengths and weaknesses of each method.\n    \"\"\"\n    print(\"‚öñÔ∏è Clustering Algorithm Comparison\\n\")\n    \n    # Generate test dataset\n    X_compare, y_compare = generate_blob_data(n_samples=500, centers=4, cluster_std=1.0)\n    \n    # Add some noise\n    X_noise = np.random.uniform(X_compare.min(), X_compare.max(), (50, 2))\n    X_compare = np.vstack([X_compare, X_noise])\n    y_compare = np.hstack([y_compare, [-1] * 50])\n    \n    # Standardize\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_compare)\n    \n    # Define algorithms\n    from sklearn.mixture import GaussianMixture\n    algorithms = [\n        ('K-Means', KMeans(n_clusters=4, random_state=42)),\n        ('DBSCAN', DBSCAN(eps=0.3, min_samples=5)),\n        ('Hierarchical', AgglomerativeClustering(n_clusters=4)),\n        ('GMM', GaussianMixture(n_components=4, random_state=42))\n    ]\n    \n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    \n    # Store results\n    comparison_results = []\n    \n    for idx, (name, algorithm) in enumerate(algorithms):\n        # Fit algorithm\n        if hasattr(algorithm, 'fit_predict'):\n            labels = algorithm.fit_predict(X_scaled)\n        else:\n            labels = algorithm.fit(X_scaled).predict(X_scaled)\n        \n        # Calculate metrics\n        unique_labels = np.unique(labels[labels != -1])\n        n_clusters = len(unique_labels)\n        n_noise = np.sum(labels == -1)\n        \n        if n_clusters > 1:\n            silhouette = silhouette_score(X_scaled, labels)\n            db_index = davies_bouldin_score(X_scaled, labels)\n        else:\n            silhouette = -1\n            db_index = np.inf\n        \n        comparison_results.append({\n            'Algorithm': name,\n            'Clusters': n_clusters,\n            'Noise': n_noise,\n            'Silhouette': silhouette,\n            'Davies-Bouldin': db_index\n        })\n        \n        # Visualization\n        ax1 = axes[0, idx]\n        ax2 = axes[1, idx]\n        \n        # Plot clusters\n        for label in unique_labels:\n            if label == -1:\n                mask = labels == label\n                ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],\n                           c='black', marker='x', s=30, alpha=0.5, label='Noise')\n            else:\n                mask = labels == label\n                ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],\n                           s=30, alpha=0.7, label=f'C{label}')\n        \n        ax1.set_title(f'{name}\\nClusters: {n_clusters}, Noise: {n_noise}', \n                     fontsize=10, fontweight='bold')\n        ax1.set_xticks([])\n        ax1.set_yticks([])\n        \n        # Metrics bar chart\n        metrics = ['Silhouette', 'DB Index\\n(inverted)']\n        values = [silhouette, -db_index/10]  # Normalize for display\n        colors_bar = ['green' if v > 0 else 'red' for v in values]\n        \n        bars = ax2.bar(range(2), values, color=colors_bar, alpha=0.7)\n        ax2.set_xticks(range(2))\n        ax2.set_xticklabels(metrics, fontsize=8)\n        ax2.set_title(f'{name} Metrics', fontsize=10, fontweight='bold')\n        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n    \n    plt.suptitle('Clustering Algorithm Comparison', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Comparison table\n    comparison_df = pd.DataFrame(comparison_results)\n    print(\"\\nüìä Performance Comparison:\")\n    display(comparison_df)\n    \n    print(\"\\nüéØ Algorithm Selection Guide:\")\n    print(\"\\nüìå K-Means: Fast, simple, spherical clusters\")\n    print(\"üìå DBSCAN: Arbitrary shapes, identifies outliers\")\n    print(\"üìå Hierarchical: Creates taxonomy, no K needed upfront\")\n    print(\"üìå GMM: Soft clustering, overlapping clusters\")\n    \n    return comparison_df\n\n\ndef demonstrate_common_mistakes():\n    \"\"\"\n    Show common clustering mistakes and how to fix them.\n    Educational visualization of pitfalls.\n    \"\"\"\n    print(\"‚ö†Ô∏è Common Clustering Mistakes and How to Fix Them\\n\")\n    \n    # Generate data\n    X_mistakes, y_mistakes = generate_blob_data(n_samples=300, centers=3)\n    \n    fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n    \n    # Mistake 1: Not scaling features\n    ax1 = axes[0, 0]\n    X_unscaled = X_mistakes.copy()\n    X_unscaled[:, 1] *= 100  # Make one feature much larger\n    kmeans_unscaled = KMeans(n_clusters=3, random_state=42)\n    labels_unscaled = kmeans_unscaled.fit_predict(X_unscaled)\n    ax1.scatter(X_unscaled[:, 0], X_unscaled[:, 1], c=labels_unscaled, \n               cmap='viridis', s=30, alpha=0.7)\n    ax1.set_title('‚ùå Mistake: Unscaled Features', fontsize=10, fontweight='bold', color='red')\n    ax1.set_xlabel('Feature 1 (0-10)')\n    ax1.set_ylabel('Feature 2 (0-1000)')\n    \n    # Fix 1: Scale features\n    ax2 = axes[0, 1]\n    scaler = StandardScaler()\n    X_scaled_fix = scaler.fit_transform(X_unscaled)\n    kmeans_scaled = KMeans(n_clusters=3, random_state=42)\n    labels_scaled = kmeans_scaled.fit_predict(X_scaled_fix)\n    ax2.scatter(X_scaled_fix[:, 0], X_scaled_fix[:, 1], c=labels_scaled, \n               cmap='viridis', s=30, alpha=0.7)\n    ax2.set_title('‚úÖ Fix: Scaled Features', fontsize=10, fontweight='bold', color='green')\n    ax2.set_xlabel('Feature 1 (standardized)')\n    ax2.set_ylabel('Feature 2 (standardized)')\n    \n    # Mistake 2: Wrong number of clusters\n    ax3 = axes[0, 2]\n    kmeans_wrong_k = KMeans(n_clusters=10, random_state=42)\n    labels_wrong_k = kmeans_wrong_k.fit_predict(X_mistakes)\n    ax3.scatter(X_mistakes[:, 0], X_mistakes[:, 1], c=labels_wrong_k, \n               cmap='tab10', s=30, alpha=0.7)\n    silhouette_wrong = silhouette_score(X_mistakes, labels_wrong_k)\n    ax3.set_title(f'‚ùå Too Many Clusters (K=10)\\nSilhouette: {silhouette_wrong:.3f}', \n                 fontsize=10, fontweight='bold', color='red')\n    \n    # Fix 2: Use elbow method\n    ax4 = axes[1, 0]\n    kmeans_correct_k = KMeans(n_clusters=3, random_state=42)\n    labels_correct_k = kmeans_correct_k.fit_predict(X_mistakes)\n    ax4.scatter(X_mistakes[:, 0], X_mistakes[:, 1], c=labels_correct_k, \n               cmap='viridis', s=30, alpha=0.7)\n    silhouette_correct = silhouette_score(X_mistakes, labels_correct_k)\n    ax4.set_title(f'‚úÖ Optimal K=3\\nSilhouette: {silhouette_correct:.3f}', \n                 fontsize=10, fontweight='bold', color='green')\n    \n    # Mistake 3: Ignoring outliers\n    ax5 = axes[1, 1]\n    X_with_outliers = X_mistakes.copy()\n    outliers = np.random.uniform(-15, 15, (20, 2))\n    X_with_outliers = np.vstack([X_with_outliers, outliers])\n    kmeans_outliers = KMeans(n_clusters=3, random_state=42)\n    labels_outliers = kmeans_outliers.fit_predict(X_with_outliers)\n    ax5.scatter(X_with_outliers[:, 0], X_with_outliers[:, 1], \n               c=labels_outliers, cmap='viridis', s=30, alpha=0.7)\n    ax5.set_title('‚ùå K-Means with Outliers', fontsize=10, fontweight='bold', color='red')\n    \n    # Fix 3: Use DBSCAN\n    ax6 = axes[1, 2]\n    dbscan_fix = DBSCAN(eps=1.5, min_samples=5)\n    labels_dbscan = dbscan_fix.fit_predict(X_with_outliers)\n    unique_labels = set(labels_dbscan)\n    colors_db = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n    for k, col in zip(unique_labels, colors_db):\n        if k == -1:\n            col = 'black'\n            marker = 'x'\n        else:\n            marker = 'o'\n        class_member_mask = (labels_dbscan == k)\n        xy = X_with_outliers[class_member_mask]\n        ax6.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, s=30, alpha=0.7)\n    ax6.set_title('‚úÖ DBSCAN Handles Outliers', fontsize=10, fontweight='bold', color='green')\n    \n    plt.suptitle('Common Clustering Mistakes and Solutions', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìö Summary of Common Mistakes:\")\n    print(\"\\n1. üìè Not Scaling Features:\")\n    print(\"   Problem: Features with larger scales dominate\")\n    print(\"   Solution: Always standardize or normalize\")\n    print(\"\\n2. üî¢ Wrong Number of Clusters:\")\n    print(\"   Problem: Too many/few clusters\")\n    print(\"   Solution: Use elbow method, silhouette analysis\")\n    print(\"\\n3. üîç Ignoring Outliers:\")\n    print(\"   Problem: K-means is sensitive to outliers\")\n    print(\"   Solution: Use DBSCAN or remove outliers first\")\n\nprint(\"Algorithm demonstration functions loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "b8ec41ef-cc96-4b6d-9d02-7e56425338f9"
  },
  {
   "cell_type": "markdown",
   "source": "### 0.2 Algorithm Demonstration Functions",
   "metadata": {},
   "id": "ae677db8-20d4-49d2-a098-073c052ff933"
  },
  {
   "cell_type": "code",
   "source": "# Helper functions for Part 2\n\ndef generate_blob_data(n_samples=1000, centers=3, n_features=2, cluster_std=1.0, random_state=42):\n    \"\"\"Generate simple blob data for demonstrations.\"\"\"\n    from sklearn.datasets import make_blobs\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features,\n                     cluster_std=cluster_std, random_state=random_state)\n    return X, y\n\ndef plot_clusters(X, labels, centers=None, title=\"Clusters\", ax=None):\n    \"\"\"Simple cluster plotting function.\"\"\"\n    if ax is None:\n        import matplotlib.pyplot as plt\n        fig, ax = plt.subplots(figsize=(8, 6))\n    \n    unique_labels = np.unique(labels)\n    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n    \n    for i, label in enumerate(unique_labels):\n        if label == -1:  # Noise points for DBSCAN\n            mask = labels == label\n            ax.scatter(X[mask, 0], X[mask, 1], c='gray', marker='x', s=50, alpha=0.5, label='Noise')\n        else:\n            mask = labels == label\n            ax.scatter(X[mask, 0], X[mask, 1], c=[colors[i]], s=50, alpha=0.7, label=f'Cluster {label}')\n    \n    if centers is not None:\n        ax.scatter(centers[:, 0], centers[:, 1], c='red', marker='*', s=300, \n                  edgecolors='black', linewidth=2, label='Centers', zorder=10)\n    \n    ax.set_title(title)\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    return ax\n\ndef generate_innovation_data(n_samples=1000, n_features=10, n_clusters=5, noise=0.1):\n    \"\"\"Generate synthetic innovation dataset.\"\"\"\n    from sklearn.datasets import make_blobs\n    X, y = make_blobs(n_samples=n_samples, n_features=n_features, \n                     centers=n_clusters, cluster_std=1.5, random_state=42)\n    X += np.random.normal(0, noise, X.shape)\n    \n    feature_names = [\n        'Tech_Sophistication', 'Market_Readiness', 'Resource_Requirements',\n        'User_Engagement', 'Scalability', 'Innovation_Level',\n        'Competition_Intensity', 'Regulatory_Complexity', 'ROI_Potential',\n        'Implementation_Time'\n    ][:n_features]\n    \n    df = pd.DataFrame(X, columns=feature_names)\n    df['True_Cluster'] = y\n    \n    return df, X, y\n\nprint(\"Helper functions loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "13e0f8b7-d1a3-40e1-bdb5-4026d37daafc"
  },
  {
   "cell_type": "markdown",
   "source": "## Section 0: All Functions\n\n### 0.1 Helper Functions for Data Generation and Visualization",
   "metadata": {},
   "id": "8255d385-7f22-44f0-9311-aa0527ff3110"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Part 2 - Technical Deep Dive\n",
    "Master all clustering algorithms with hands-on implementation."
   ],
   "id": "0688e747-a8f1-41e7-9b26-7957b4181116"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 K-Means Clustering\n",
    "\n",
    "K-Means is the workhorse of clustering algorithms - simple, fast, and effective for many innovation analysis tasks."
   ],
   "id": "e4184d1e-4800-44d5-b7e8-8b5f3cfee780"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory & Visualization"
   ],
   "id": "110bb0e0-0ca0-486a-b4a9-76303720be9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means step-by-step visualization\n",
    "print(\"üéØ K-Means Clustering: Step-by-Step Process\\n\")\n",
    "\n",
    "# Generate sample data\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, n_features=2, \n",
    "                      cluster_std=0.8, random_state=42)\n",
    "\n",
    "# Manual K-means implementation for visualization\n",
    "def kmeans_step_by_step(X, n_clusters=3, n_iterations=5):\n",
    "    \"\"\"Visualize K-means algorithm step by step\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Initialize random centers\n",
    "    idx = np.random.choice(len(X), n_clusters, replace=False)\n",
    "    centers = X[idx].copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        ax = axes[iteration]\n",
    "        \n",
    "        # Step 1: Assign points to nearest center\n",
    "        distances = np.zeros((len(X), n_clusters))\n",
    "        for k in range(n_clusters):\n",
    "            distances[:, k] = np.linalg.norm(X - centers[k], axis=1)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Visualize current state\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "        for k in range(n_clusters):\n",
    "            mask = labels == k\n",
    "            ax.scatter(X[mask, 0], X[mask, 1], c=colors[k], \n",
    "                      s=50, alpha=0.6, label=f'Cluster {k+1}')\n",
    "        \n",
    "        # Plot centers\n",
    "        ax.scatter(centers[:, 0], centers[:, 1], c='red', \n",
    "                  marker='*', s=300, edgecolors='black', \n",
    "                  linewidth=2, label='Centers', zorder=10)\n",
    "        \n",
    "        # Step 2: Update centers\n",
    "        new_centers = np.zeros_like(centers)\n",
    "        for k in range(n_clusters):\n",
    "            if np.sum(labels == k) > 0:\n",
    "                new_centers[k] = X[labels == k].mean(axis=0)\n",
    "            else:\n",
    "                new_centers[k] = centers[k]\n",
    "        \n",
    "        # Draw movement arrows\n",
    "        for k in range(n_clusters):\n",
    "            ax.arrow(centers[k, 0], centers[k, 1],\n",
    "                    new_centers[k, 0] - centers[k, 0],\n",
    "                    new_centers[k, 1] - centers[k, 1],\n",
    "                    head_width=0.1, head_length=0.1,\n",
    "                    fc='black', ec='black', alpha=0.5)\n",
    "        \n",
    "        centers = new_centers.copy()\n",
    "        \n",
    "        ax.set_title(f'Iteration {iteration + 1}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        if iteration == 0:\n",
    "            ax.legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    # Final result\n",
    "    ax = axes[5]\n",
    "    for k in range(n_clusters):\n",
    "        mask = labels == k\n",
    "        ax.scatter(X[mask, 0], X[mask, 1], c=colors[k], \n",
    "                  s=50, alpha=0.6, label=f'Cluster {k+1}')\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], c='red', \n",
    "              marker='*', s=300, edgecolors='black', \n",
    "              linewidth=2, label='Final Centers', zorder=10)\n",
    "    ax.set_title('Final Result', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    plt.suptitle('K-Means Algorithm: Watch Centers Converge to Optimal Positions', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "# Run visualization\n",
    "centers, labels = kmeans_step_by_step(X, n_clusters=3, n_iterations=5)\n",
    "\n",
    "# Calculate metrics\n",
    "silhouette = silhouette_score(X, labels)\n",
    "print(f\"\\nüìä Algorithm Performance:\")\n",
    "print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "print(f\"Converged in 5 iterations\")"
   ],
   "id": "13f74aa3-71ab-4d41-861f-ae4f13fde3c0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Implementation"
   ],
   "id": "0d8e09eb-f877-4620-93db-35b217212345"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate innovation data\n",
    "print(\"üîß Hands-on K-Means Implementation\\n\")\n",
    "\n",
    "# Create innovation dataset\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "n_clusters = 4\n",
    "\n",
    "X_innovation, y_true = make_blobs(n_samples=n_samples, \n",
    "                                 n_features=n_features,\n",
    "                                 centers=n_clusters,\n",
    "                                 cluster_std=1.5,\n",
    "                                 random_state=42)\n",
    "\n",
    "# Add feature names\n",
    "feature_names = ['Tech_Level', 'Market_Size', 'Investment', 'Risk_Score', \n",
    "                'Timeline', 'Resources', 'Innovation_Score', 'Complexity',\n",
    "                'User_Impact', 'Scalability']\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_innovation)\n",
    "\n",
    "print(f\"Dataset shape: {X_scaled.shape}\")\n",
    "print(f\"Features: {', '.join(feature_names)}\\n\")\n",
    "\n",
    "# Apply K-means with different K values\n",
    "k_values = [2, 3, 4, 5, 6]\n",
    "results = {}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Use PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    # Fit K-means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    silhouette = silhouette_score(X_scaled, labels)\n",
    "    inertia = kmeans.inertia_\n",
    "    \n",
    "    results[k] = {\n",
    "        'labels': labels,\n",
    "        'centers': kmeans.cluster_centers_,\n",
    "        'silhouette': silhouette,\n",
    "        'inertia': inertia\n",
    "    }\n",
    "    \n",
    "    # Visualize\n",
    "    ax = axes[idx]\n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n",
    "                        cmap='viridis', s=20, alpha=0.6)\n",
    "    \n",
    "    # Plot centers in PCA space\n",
    "    centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "    ax.scatter(centers_pca[:, 0], centers_pca[:, 1],\n",
    "              c='red', marker='*', s=300, \n",
    "              edgecolors='black', linewidth=2)\n",
    "    \n",
    "    ax.set_title(f'K={k}, Silhouette={silhouette:.3f}', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('First Principal Component')\n",
    "    ax.set_ylabel('Second Principal Component')\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.suptitle('K-Means with Different K Values (PCA Visualization)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nüìä K-Means Results Comparison:\")\n",
    "print(\"K | Silhouette | Inertia\")\n",
    "print(\"-\" * 30)\n",
    "for k, metrics in results.items():\n",
    "    print(f\"{k} | {metrics['silhouette']:.3f}      | {metrics['inertia']:.1f}\")\n",
    "\n",
    "# Best K\n",
    "best_k = max(results.keys(), key=lambda k: results[k]['silhouette'])\n",
    "print(f\"\\n‚ú® Best K={best_k} with silhouette score {results[best_k]['silhouette']:.3f}\")"
   ],
   "id": "d0201f79-a090-4c10-b882-6ad715957271"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise: Implement K-Means from Scratch"
   ],
   "id": "830c8492-69bc-4ca9-89ac-438126f4aa3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Implement K-means from scratch\n",
    "print(\"üéØ Exercise: Implement K-Means from Scratch\\n\")\n",
    "\n",
    "class MyKMeans:\n",
    "    \"\"\"Simple K-Means implementation for learning\"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters=3, max_iters=100, tol=1e-4):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.centers = None\n",
    "        self.labels = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit K-means to data\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize centers randomly\n",
    "        idx = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "        self.centers = X[idx].copy()\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            # Assign points to nearest center\n",
    "            distances = np.zeros((n_samples, self.n_clusters))\n",
    "            for k in range(self.n_clusters):\n",
    "                distances[:, k] = np.linalg.norm(X - self.centers[k], axis=1)\n",
    "            self.labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Update centers\n",
    "            new_centers = np.zeros_like(self.centers)\n",
    "            for k in range(self.n_clusters):\n",
    "                if np.sum(self.labels == k) > 0:\n",
    "                    new_centers[k] = X[self.labels == k].mean(axis=0)\n",
    "                else:\n",
    "                    new_centers[k] = self.centers[k]\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(new_centers - self.centers) < self.tol:\n",
    "                print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            \n",
    "            self.centers = new_centers\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict cluster labels\"\"\"\n",
    "        distances = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            distances[:, k] = np.linalg.norm(X - self.centers[k], axis=1)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "# Test your implementation\n",
    "X_test, _ = make_blobs(n_samples=200, centers=3, random_state=42)\n",
    "\n",
    "# Your implementation\n",
    "my_kmeans = MyKMeans(n_clusters=3)\n",
    "my_kmeans.fit(X_test)\n",
    "my_labels = my_kmeans.labels\n",
    "\n",
    "# Sklearn implementation\n",
    "sklearn_kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "sklearn_labels = sklearn_kmeans.fit_predict(X_test)\n",
    "\n",
    "# Compare results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.scatter(X_test[:, 0], X_test[:, 1], c=my_labels, cmap='viridis', s=50)\n",
    "ax1.scatter(my_kmeans.centers[:, 0], my_kmeans.centers[:, 1],\n",
    "           c='red', marker='*', s=300, edgecolors='black', linewidth=2)\n",
    "ax1.set_title('Your Implementation', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=sklearn_labels, cmap='viridis', s=50)\n",
    "ax2.scatter(sklearn_kmeans.cluster_centers_[:, 0], \n",
    "           sklearn_kmeans.cluster_centers_[:, 1],\n",
    "           c='red', marker='*', s=300, edgecolors='black', linewidth=2)\n",
    "ax2.set_title('Sklearn Implementation', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "\n",
    "plt.suptitle('K-Means Implementation Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Your implementation silhouette score: {silhouette_score(X_test, my_labels):.3f}\")\n",
    "print(f\"‚úÖ Sklearn silhouette score: {silhouette_score(X_test, sklearn_labels):.3f}\")"
   ],
   "id": "c1d73b3f-78f7-426f-a44b-e90a1e01036d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Finding Optimal K\n",
    "\n",
    "One of the biggest challenges in clustering: How many clusters should we have?"
   ],
   "id": "819fd07d-97b7-49fc-8e92-09708f5189cf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method"
   ],
   "id": "68025418-8894-403c-8178-db5924596ff4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive elbow method analysis\n",
    "print(\"üìà Finding Optimal K: The Elbow Method\\n\")\n",
    "\n",
    "# Generate data with known clusters\n",
    "X_elbow, y_true = make_blobs(n_samples=500, centers=4, \n",
    "                            n_features=2, cluster_std=1.0, \n",
    "                            random_state=42)\n",
    "\n",
    "# Test range of K values\n",
    "k_range = range(1, 11)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "davies_bouldins = []\n",
    "calinski_harabaszs = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_elbow)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    if k > 1:  # Metrics need at least 2 clusters\n",
    "        silhouettes.append(silhouette_score(X_elbow, labels))\n",
    "        davies_bouldins.append(davies_bouldin_score(X_elbow, labels))\n",
    "        calinski_harabaszs.append(calinski_harabasz_score(X_elbow, labels))\n",
    "    else:\n",
    "        silhouettes.append(0)\n",
    "        davies_bouldins.append(0)\n",
    "        calinski_harabaszs.append(0)\n",
    "\n",
    "# Calculate elbow point\n",
    "# Method 1: Maximum curvature\n",
    "deltas = np.diff(inertias)\n",
    "delta_deltas = np.diff(deltas)\n",
    "elbow_idx = np.argmax(np.abs(delta_deltas)) + 2  # +2 because of double diff\n",
    "\n",
    "# Method 2: Distance from line\n",
    "coords = np.array([(k, inertias[i]) for i, k in enumerate(k_range)])\n",
    "line_start = coords[0]\n",
    "line_end = coords[-1]\n",
    "distances = []\n",
    "for point in coords:\n",
    "    # Distance from point to line\n",
    "    numerator = np.abs((line_end[1] - line_start[1]) * point[0] - \n",
    "                      (line_end[0] - line_start[0]) * point[1] + \n",
    "                      line_end[0] * line_start[1] - line_end[1] * line_start[0])\n",
    "    denominator = np.sqrt((line_end[1] - line_start[1])**2 + \n",
    "                         (line_end[0] - line_start[0])**2)\n",
    "    distances.append(numerator / denominator)\n",
    "\n",
    "elbow_idx_2 = np.argmax(distances)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Inertia/Elbow plot\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.axvline(x=list(k_range)[elbow_idx], color='red', \n",
    "           linestyle='--', alpha=0.7, label=f'Elbow at k={list(k_range)[elbow_idx]}')\n",
    "ax1.set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=11)\n",
    "ax1.set_title('Elbow Method', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette scores\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(k_range[1:], silhouettes[1:], 'go-', linewidth=2, markersize=8)\n",
    "best_silhouette_k = list(k_range)[np.argmax(silhouettes) if silhouettes else 0]\n",
    "ax2.axvline(x=best_silhouette_k, color='red', linestyle='--', \n",
    "           alpha=0.7, label=f'Best at k={best_silhouette_k}')\n",
    "ax2.set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=11)\n",
    "ax2.set_title('Silhouette Analysis', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Davies-Bouldin Index (lower is better)\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(k_range[1:], davies_bouldins[1:], 'ro-', linewidth=2, markersize=8)\n",
    "best_db_k = list(k_range)[np.argmin(davies_bouldins[1:]) + 1 if davies_bouldins[1:] else 0]\n",
    "ax3.axvline(x=best_db_k, color='green', linestyle='--', \n",
    "           alpha=0.7, label=f'Best at k={best_db_k}')\n",
    "ax3.set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "ax3.set_ylabel('Davies-Bouldin Index (lower is better)', fontsize=11)\n",
    "ax3.set_title('Davies-Bouldin Index', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Calinski-Harabasz Index (higher is better)\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(k_range[1:], calinski_harabaszs[1:], 'mo-', linewidth=2, markersize=8)\n",
    "best_ch_k = list(k_range)[np.argmax(calinski_harabaszs[1:]) + 1 if calinski_harabaszs[1:] else 0]\n",
    "ax4.axvline(x=best_ch_k, color='green', linestyle='--', \n",
    "           alpha=0.7, label=f'Best at k={best_ch_k}')\n",
    "ax4.set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "ax4.set_ylabel('Calinski-Harabasz Index (higher is better)', fontsize=11)\n",
    "ax4.set_title('Calinski-Harabasz Index', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Multiple Methods for Finding Optimal K', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Optimal K Recommendations:\")\n",
    "print(f\"Elbow Method: k={list(k_range)[elbow_idx]}\")\n",
    "print(f\"Silhouette Score: k={best_silhouette_k}\")\n",
    "print(f\"Davies-Bouldin Index: k={best_db_k}\")\n",
    "print(f\"Calinski-Harabasz Index: k={best_ch_k}\")\n",
    "print(f\"\\nTrue number of clusters: 4\")\n",
    "print(\"\\nüí° Tip: When methods disagree, consider domain knowledge and use case!\")"
   ],
   "id": "944e7c6b-5a81-4cbf-817a-66f9b06b054b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Analysis"
   ],
   "id": "fecd746e-44dc-42d0-9570-0bba02f9bfd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed silhouette analysis\n",
    "print(\"üìä Silhouette Analysis: Understanding Cluster Quality\\n\")\n",
    "\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# Test different K values\n",
    "k_values_test = [2, 3, 4, 5]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, k in enumerate(k_values_test):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Fit K-means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_elbow)\n",
    "    \n",
    "    # Calculate silhouette scores\n",
    "    silhouette_avg = silhouette_score(X_elbow, labels)\n",
    "    sample_silhouette_values = silhouette_samples(X_elbow, labels)\n",
    "    \n",
    "    y_lower = 10\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, k))\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Get silhouette scores for cluster i\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                        0, ith_cluster_silhouette_values,\n",
    "                        facecolor=colors[i], edgecolor=colors[i], alpha=0.7)\n",
    "        \n",
    "        # Label the silhouette plots with cluster numbers\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        \n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\",\n",
    "              label=f'Average: {silhouette_avg:.3f}')\n",
    "    \n",
    "    ax.set_title(f'K={k}, Avg Silhouette={silhouette_avg:.3f}', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Silhouette Coefficient')\n",
    "    ax.set_ylabel('Cluster')\n",
    "    ax.set_xlim([-0.1, 1])\n",
    "    ax.set_ylim([0, len(X_elbow) + (k + 1) * 10])\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Silhouette Analysis for Different K Values', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìñ How to Read Silhouette Plots:\")\n",
    "print(\"‚Ä¢ Width: Number of points in cluster\")\n",
    "print(\"‚Ä¢ Thickness: How well-separated the cluster is\")\n",
    "print(\"‚Ä¢ Values > 0: Point is closer to its cluster than others\")\n",
    "print(\"‚Ä¢ Values < 0: Point might be in wrong cluster\")\n",
    "print(\"‚Ä¢ Red line: Average silhouette score\")\n",
    "print(\"\\n‚ú® Best clustering has thick, uniform clusters above the average line!\")"
   ],
   "id": "ef2860b4-50d7-4892-9ac0-f53df3ebfdd1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 DBSCAN - Density-Based Clustering\n",
    "\n",
    "DBSCAN finds clusters of arbitrary shape and identifies outliers - perfect for innovation data with noise."
   ],
   "id": "a2f1cf20-f3d6-44b8-b72d-ba01dbf016e3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Parameters"
   ],
   "id": "9f1ae48b-0eae-44d5-b57d-cdfc54d5273f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN parameter exploration\n",
    "print(\"üîç DBSCAN: Understanding eps and min_samples\\n\")\n",
    "\n",
    "# Generate data with outliers\n",
    "X_dbscan, _ = make_blobs(n_samples=300, centers=3, n_features=2,\n",
    "                        cluster_std=0.5, random_state=42)\n",
    "# Add noise points\n",
    "X_noise = np.random.uniform(-6, 6, (50, 2))\n",
    "X_dbscan = np.vstack([X_dbscan, X_noise])\n",
    "\n",
    "# Test different parameter combinations\n",
    "eps_values = [0.3, 0.5, 0.7, 1.0]\n",
    "min_samples_values = [3, 5, 10, 20]\n",
    "\n",
    "fig, axes = plt.subplots(len(eps_values), len(min_samples_values), \n",
    "                        figsize=(16, 12))\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_samples in enumerate(min_samples_values):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        # Apply DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X_dbscan)\n",
    "        \n",
    "        # Count clusters and noise\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # Plot\n",
    "        unique_labels = set(labels)\n",
    "        colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for k, col in zip(unique_labels, colors):\n",
    "            if k == -1:\n",
    "                # Black for noise\n",
    "                col = 'black'\n",
    "                marker = 'x'\n",
    "            else:\n",
    "                marker = 'o'\n",
    "            \n",
    "            class_member_mask = (labels == k)\n",
    "            xy = X_dbscan[class_member_mask]\n",
    "            ax.scatter(xy[:, 0], xy[:, 1], c=[col], \n",
    "                      marker=marker, s=30, alpha=0.7)\n",
    "        \n",
    "        ax.set_title(f'eps={eps}, min={min_samples}\\nC={n_clusters}, N={n_noise}',\n",
    "                    fontsize=9)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "# Add labels\n",
    "for i, eps in enumerate(eps_values):\n",
    "    axes[i, 0].set_ylabel(f'eps={eps}', fontsize=10, fontweight='bold')\n",
    "for j, min_samples in enumerate(min_samples_values):\n",
    "    axes[0, j].set_xlabel(f'min_samples={min_samples}', fontsize=10, fontweight='bold')\n",
    "    axes[0, j].xaxis.set_label_position('top')\n",
    "\n",
    "plt.suptitle('DBSCAN Parameter Grid: Impact of eps and min_samples\\n'\n",
    "            'C=Clusters, N=Noise points', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìö Parameter Guidelines:\")\n",
    "print(\"‚Ä¢ eps: Maximum distance between points in same neighborhood\")\n",
    "print(\"  - Too small: Many clusters, more noise\")\n",
    "print(\"  - Too large: Few clusters, points merge\")\n",
    "print(\"\\n‚Ä¢ min_samples: Minimum points to form dense region\")\n",
    "print(\"  - Too small: More clusters, less noise\")\n",
    "print(\"  - Too large: Fewer clusters, more noise\")\n",
    "print(\"\\nüí° Start with min_samples = 2 * dimensions, adjust eps based on data\")"
   ],
   "id": "388bf105-a026-4906-b437-dc385a691917"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Shapes"
   ],
   "id": "c18485fa-5e0d-48bc-a8dc-e28da1645400"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN on complex shapes\n",
    "print(\"üé® DBSCAN vs K-Means on Complex Shapes\\n\")\n",
    "\n",
    "# Generate different shaped datasets\n",
    "datasets = [\n",
    "    ('Two Moons', make_moons(n_samples=300, noise=0.05, random_state=42)),\n",
    "    ('Concentric Circles', make_circles(n_samples=300, noise=0.05, \n",
    "                                       factor=0.5, random_state=42)),\n",
    "    ('Anisotropic', make_blobs(n_samples=300, centers=3, random_state=42))\n",
    "]\n",
    "\n",
    "# Transform anisotropic\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "datasets[2] = (datasets[2][0], \n",
    "               (np.dot(datasets[2][1][0], transformation), datasets[2][1][1]))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 12))\n",
    "\n",
    "for row, (name, (X, y_true)) in enumerate(datasets):\n",
    "    # Original data\n",
    "    ax = axes[row, 0]\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=30, alpha=0.7)\n",
    "    ax.set_title(f'{name}\\n(True Clusters)', fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # K-Means\n",
    "    ax = axes[row, 1]\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(X)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', s=30, alpha=0.7)\n",
    "    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "              c='red', marker='*', s=200, edgecolors='black', linewidth=1.5)\n",
    "    score = silhouette_score(X, kmeans_labels)\n",
    "    ax.set_title(f'K-Means\\n(Silhouette: {score:.3f})', fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # DBSCAN\n",
    "    ax = axes[row, 2]\n",
    "    # Adjust parameters per dataset\n",
    "    if row == 0:  # Moons\n",
    "        dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "    elif row == 1:  # Circles\n",
    "        dbscan = DBSCAN(eps=0.15, min_samples=5)\n",
    "    else:  # Anisotropic\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    \n",
    "    dbscan_labels = dbscan.fit_predict(X)\n",
    "    \n",
    "    # Plot\n",
    "    unique_labels = set(dbscan_labels)\n",
    "    colors_db = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors_db):\n",
    "        if k == -1:\n",
    "            col = 'black'\n",
    "            marker = 'x'\n",
    "        else:\n",
    "            marker = 'o'\n",
    "        \n",
    "        class_member_mask = (dbscan_labels == k)\n",
    "        xy = X[class_member_mask]\n",
    "        ax.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, \n",
    "                  s=30, alpha=0.7)\n",
    "    \n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    n_noise = list(dbscan_labels).count(-1)\n",
    "    \n",
    "    if n_clusters > 1:\n",
    "        score = silhouette_score(X[dbscan_labels != -1], \n",
    "                               dbscan_labels[dbscan_labels != -1])\n",
    "    else:\n",
    "        score = -1\n",
    "    \n",
    "    ax.set_title(f'DBSCAN\\n(Silhouette: {score:.3f})', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Add column labels\n",
    "axes[0, 0].set_ylabel('Two Moons', fontsize=11, fontweight='bold', rotation=0, \n",
    "                     labelpad=40, ha='right')\n",
    "axes[1, 0].set_ylabel('Circles', fontsize=11, fontweight='bold', rotation=0, \n",
    "                     labelpad=40, ha='right')\n",
    "axes[2, 0].set_ylabel('Anisotropic', fontsize=11, fontweight='bold', rotation=0, \n",
    "                     labelpad=40, ha='right')\n",
    "\n",
    "plt.suptitle('K-Means vs DBSCAN: Handling Complex Shapes', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Observations:\")\n",
    "print(\"‚Ä¢ K-Means assumes spherical clusters - fails on non-convex shapes\")\n",
    "print(\"‚Ä¢ DBSCAN finds clusters of arbitrary shape\")\n",
    "print(\"‚Ä¢ DBSCAN identifies outliers (black X marks)\")\n",
    "print(\"‚Ä¢ Choose algorithm based on expected cluster shape!\")"
   ],
   "id": "5311a53d-09f3-417d-89bb-865da8638217"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Hierarchical Clustering\n",
    "\n",
    "Build a tree of clusters - perfect for understanding innovation taxonomies."
   ],
   "id": "277c39fc-d7ce-4a36-9c1f-97895babc7b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering demonstration\n",
    "print(\"üå≥ Hierarchical Clustering: Building Innovation Taxonomy\\n\")\n",
    "\n",
    "# Generate hierarchical data\n",
    "n_samples = 100\n",
    "X_hier, y_hier = make_blobs(n_samples=n_samples, centers=4, \n",
    "                           n_features=2, cluster_std=0.5, \n",
    "                           random_state=42)\n",
    "\n",
    "# Different linkage methods\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    linkage_matrix = linkage(X_hier, method=method)\n",
    "    \n",
    "    # Plot dendrogram\n",
    "    dendrogram(linkage_matrix, ax=ax, truncate_mode='level', \n",
    "              p=5, color_threshold=0, above_threshold_color='gray')\n",
    "    \n",
    "    ax.set_title(f'Linkage: {method.capitalize()}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Sample Index')\n",
    "    ax.set_ylabel('Distance')\n",
    "\n",
    "plt.suptitle('Hierarchical Clustering with Different Linkage Methods', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interactive dendrogram with cut\n",
    "print(\"\\n‚úÇÔ∏è Cutting the Dendrogram at Different Heights:\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Use Ward linkage\n",
    "linkage_matrix = linkage(X_hier, method='ward')\n",
    "\n",
    "cut_heights = [7, 5, 3]\n",
    "for idx, max_d in enumerate(cut_heights):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot dendrogram with cut\n",
    "    dendrogram(linkage_matrix, ax=ax, color_threshold=max_d,\n",
    "              above_threshold_color='gray')\n",
    "    ax.axhline(y=max_d, c='red', linestyle='--', linewidth=2,\n",
    "              label=f'Cut at {max_d}')\n",
    "    \n",
    "    # Get clusters from cut\n",
    "    from scipy.cluster.hierarchy import fcluster\n",
    "    clusters = fcluster(linkage_matrix, max_d, criterion='distance')\n",
    "    n_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    ax.set_title(f'Cut at Distance={max_d}\\n{n_clusters} Clusters', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Sample Index')\n",
    "    ax.set_ylabel('Distance')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Impact of Cut Height on Number of Clusters', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Linkage Method Comparison:\")\n",
    "print(\"‚Ä¢ Ward: Minimizes within-cluster variance (most common)\")\n",
    "print(\"‚Ä¢ Complete: Maximum distance between clusters\")\n",
    "print(\"‚Ä¢ Average: Average distance between all pairs\")\n",
    "print(\"‚Ä¢ Single: Minimum distance (can create chains)\")"
   ],
   "id": "fa2c104c-0973-4d6d-a927-5aefe62401cb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Gaussian Mixture Models\n",
    "\n",
    "Soft clustering where innovations can belong to multiple categories with different probabilities."
   ],
   "id": "3725d9b7-7eeb-420c-ad4c-be5c06f205b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Mixture Models\n",
    "print(\"üîÆ Gaussian Mixture Models: Soft Clustering\\n\")\n",
    "\n",
    "# Generate overlapping clusters\n",
    "X_gmm, y_gmm = make_blobs(n_samples=400, centers=3, n_features=2,\n",
    "                         cluster_std=1.2, random_state=42)\n",
    "\n",
    "# Fit GMM\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(X_gmm)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "gmm_labels = gmm.predict(X_gmm)\n",
    "gmm_probs = gmm.predict_proba(X_gmm)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Hard clustering (like K-means)\n",
    "ax1 = axes[0]\n",
    "scatter1 = ax1.scatter(X_gmm[:, 0], X_gmm[:, 1], c=gmm_labels, \n",
    "                      cmap='viridis', s=30, alpha=0.7)\n",
    "ax1.scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n",
    "           c='red', marker='*', s=300, edgecolors='black', linewidth=2)\n",
    "ax1.set_title('Hard Assignment (Most Likely Cluster)', fontsize=11, fontweight='bold')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter1, ax=ax1)\n",
    "\n",
    "# Soft clustering - show uncertainty\n",
    "ax2 = axes[1]\n",
    "# Calculate uncertainty (entropy)\n",
    "uncertainty = -np.sum(gmm_probs * np.log(gmm_probs + 1e-10), axis=1)\n",
    "scatter2 = ax2.scatter(X_gmm[:, 0], X_gmm[:, 1], c=uncertainty, \n",
    "                      cmap='RdYlGn_r', s=30, alpha=0.7)\n",
    "ax2.set_title('Uncertainty (Red = High, Green = Low)', fontsize=11, fontweight='bold')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter2, ax=ax2, label='Uncertainty')\n",
    "\n",
    "# Probability contours\n",
    "ax3 = axes[2]\n",
    "x = np.linspace(X_gmm[:, 0].min() - 1, X_gmm[:, 0].max() + 1, 100)\n",
    "y = np.linspace(X_gmm[:, 1].min() - 1, X_gmm[:, 1].max() + 1, 100)\n",
    "X_grid, Y_grid = np.meshgrid(x, y)\n",
    "XX = np.array([X_grid.ravel(), Y_grid.ravel()]).T\n",
    "Z = -gmm.score_samples(XX)\n",
    "Z = Z.reshape(X_grid.shape)\n",
    "\n",
    "ax3.contour(X_grid, Y_grid, Z, levels=10, linewidths=0.5, colors='black', alpha=0.3)\n",
    "ax3.contourf(X_grid, Y_grid, Z, levels=10, cmap='viridis', alpha=0.3)\n",
    "ax3.scatter(X_gmm[:, 0], X_gmm[:, 1], c=gmm_labels, \n",
    "           cmap='viridis', s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax3.scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n",
    "           c='red', marker='*', s=300, edgecolors='white', linewidth=2)\n",
    "ax3.set_title('Gaussian Probability Contours', fontsize=11, fontweight='bold')\n",
    "ax3.set_xlabel('Feature 1')\n",
    "ax3.set_ylabel('Feature 2')\n",
    "\n",
    "plt.suptitle('Gaussian Mixture Models: Soft vs Hard Clustering', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show probability examples\n",
    "print(\"\\nüìä Example: Innovation Probability Assignments\")\n",
    "print(\"\\nSample 5 innovations and their cluster probabilities:\")\n",
    "print(\"ID | Cluster 1 | Cluster 2 | Cluster 3 | Assigned\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(5):\n",
    "    probs = gmm_probs[i]\n",
    "    assigned = gmm_labels[i]\n",
    "    print(f\"{i:2} | {probs[0]:.3f}    | {probs[1]:.3f}    | \"\n",
    "          f\"{probs[2]:.3f}    | Cluster {assigned+1}\")\n",
    "\n",
    "print(\"\\nüí° GMM Benefits:\")\n",
    "print(\"‚Ä¢ Shows uncertainty in cluster assignments\")\n",
    "print(\"‚Ä¢ Handles overlapping clusters\")\n",
    "print(\"‚Ä¢ Provides probability distributions\")\n",
    "print(\"‚Ä¢ Better for fuzzy boundaries between innovation types\")"
   ],
   "id": "e5f31c8f-bb26-43c1-ad59-2b67ad18d387"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Algorithm Comparison\n",
    "\n",
    "Let's compare all algorithms on the same dataset to understand their strengths and weaknesses."
   ],
   "id": "8028d784-fae3-4aef-b6a6-48ad3881b8ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive algorithm comparison\n",
    "print(\"‚öñÔ∏è Clustering Algorithm Comparison\\n\")\n",
    "\n",
    "# Generate test dataset\n",
    "X_compare, y_compare = make_blobs(n_samples=500, centers=4, \n",
    "                                 n_features=2, cluster_std=1.0, \n",
    "                                 random_state=42)\n",
    "\n",
    "# Add some noise\n",
    "X_noise = np.random.uniform(X_compare.min(), X_compare.max(), (50, 2))\n",
    "X_compare = np.vstack([X_compare, X_noise])\n",
    "y_compare = np.hstack([y_compare, [-1] * 50])\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_compare)\n",
    "\n",
    "# Define algorithms\n",
    "algorithms = [\n",
    "    ('K-Means', KMeans(n_clusters=4, random_state=42)),\n",
    "    ('DBSCAN', DBSCAN(eps=0.3, min_samples=5)),\n",
    "    ('Hierarchical', AgglomerativeClustering(n_clusters=4)),\n",
    "    ('GMM', GaussianMixture(n_components=4, random_state=42))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Store results\n",
    "comparison_results = []\n",
    "\n",
    "for idx, (name, algorithm) in enumerate(algorithms):\n",
    "    # Fit algorithm\n",
    "    if hasattr(algorithm, 'fit_predict'):\n",
    "        labels = algorithm.fit_predict(X_scaled)\n",
    "    else:\n",
    "        labels = algorithm.fit(X_scaled).predict(X_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    unique_labels = np.unique(labels[labels != -1])\n",
    "    n_clusters = len(unique_labels)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    \n",
    "    if n_clusters > 1:\n",
    "        silhouette = silhouette_score(X_scaled, labels)\n",
    "        db_index = davies_bouldin_score(X_scaled, labels)\n",
    "        ch_index = calinski_harabasz_score(X_scaled, labels)\n",
    "    else:\n",
    "        silhouette = -1\n",
    "        db_index = np.inf\n",
    "        ch_index = 0\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Algorithm': name,\n",
    "        'Clusters': n_clusters,\n",
    "        'Noise': n_noise,\n",
    "        'Silhouette': silhouette,\n",
    "        'Davies-Bouldin': db_index,\n",
    "        'Calinski-Harabasz': ch_index\n",
    "    })\n",
    "    \n",
    "    # Visualization\n",
    "    ax1 = axes[0, idx]\n",
    "    ax2 = axes[1, idx]\n",
    "    \n",
    "    # Plot clusters\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            mask = labels == label\n",
    "            ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],\n",
    "                       c='black', marker='x', s=30, alpha=0.5, label='Noise')\n",
    "        else:\n",
    "            mask = labels == label\n",
    "            ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],\n",
    "                       s=30, alpha=0.7, label=f'C{label}')\n",
    "    \n",
    "    # Add centers if available\n",
    "    if hasattr(algorithm, 'cluster_centers_'):\n",
    "        centers = scaler.inverse_transform(algorithm.cluster_centers_)\n",
    "        ax1.scatter(centers[:, 0], centers[:, 1],\n",
    "                   c='red', marker='*', s=200, \n",
    "                   edgecolors='black', linewidth=1.5)\n",
    "    elif hasattr(algorithm, 'means_'):\n",
    "        centers = scaler.inverse_transform(algorithm.means_)\n",
    "        ax1.scatter(centers[:, 0], centers[:, 1],\n",
    "                   c='red', marker='*', s=200, \n",
    "                   edgecolors='black', linewidth=1.5)\n",
    "    \n",
    "    ax1.set_title(f'{name}\\nClusters: {n_clusters}, Noise: {n_noise}', \n",
    "                 fontsize=10, fontweight='bold')\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    \n",
    "    # Metrics bar chart\n",
    "    metrics = ['Silhouette', 'Davies-Bouldin\\n(lower better)', \n",
    "              'Calinski-Harabasz\\n(higher better)']\n",
    "    values = [silhouette, -db_index/10, ch_index/1000]  # Normalize for display\n",
    "    colors_bar = ['green' if v > 0 else 'red' for v in values]\n",
    "    \n",
    "    bars = ax2.bar(range(3), values, color=colors_bar, alpha=0.7)\n",
    "    ax2.set_xticks(range(3))\n",
    "    ax2.set_xticklabels(metrics, fontsize=8)\n",
    "    ax2.set_title(f'{name} Metrics', fontsize=10, fontweight='bold')\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val, orig in zip(bars, values, [silhouette, db_index, ch_index]):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{orig:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.suptitle('Clustering Algorithm Comparison: Same Data, Different Approaches', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparison table\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nüìä Performance Comparison:\")\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nüéØ Algorithm Selection Guide:\")\n",
    "print(\"\\nüìå K-Means:\")\n",
    "print(\"  ‚úÖ Fast, simple, well-understood\")\n",
    "print(\"  ‚úÖ Good for spherical clusters\")\n",
    "print(\"  ‚ùå Requires K specification\")\n",
    "print(\"  ‚ùå Sensitive to outliers\")\n",
    "\n",
    "print(\"\\nüìå DBSCAN:\")\n",
    "print(\"  ‚úÖ Finds arbitrary shapes\")\n",
    "print(\"  ‚úÖ Identifies outliers\")\n",
    "print(\"  ‚ùå Sensitive to parameters\")\n",
    "print(\"  ‚ùå Struggles with varying densities\")\n",
    "\n",
    "print(\"\\nüìå Hierarchical:\")\n",
    "print(\"  ‚úÖ No need to specify K upfront\")\n",
    "print(\"  ‚úÖ Creates taxonomy\")\n",
    "print(\"  ‚ùå Computationally expensive\")\n",
    "print(\"  ‚ùå Hard to interpret large dendrograms\")\n",
    "\n",
    "print(\"\\nüìå GMM:\")\n",
    "print(\"  ‚úÖ Soft clustering\")\n",
    "print(\"  ‚úÖ Handles overlapping clusters\")\n",
    "print(\"  ‚ùå Assumes Gaussian distribution\")\n",
    "print(\"  ‚ùå Can overfit with many components\")"
   ],
   "id": "bd82d633-8766-45a3-bcc5-715b1925af5e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mistakes Gallery"
   ],
   "id": "6229a0e3-df5d-4b32-bfe0-3109fb84a7a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common clustering mistakes\n",
    "print(\"‚ö†Ô∏è Common Clustering Mistakes and How to Fix Them\\n\")\n",
    "\n",
    "# Generate data\n",
    "X_mistakes, y_mistakes = make_blobs(n_samples=300, centers=3, \n",
    "                                   n_features=2, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "# Mistake 1: Not scaling features\n",
    "ax1 = axes[0, 0]\n",
    "X_unscaled = X_mistakes.copy()\n",
    "X_unscaled[:, 1] *= 100  # Make one feature much larger\n",
    "kmeans_unscaled = KMeans(n_clusters=3, random_state=42)\n",
    "labels_unscaled = kmeans_unscaled.fit_predict(X_unscaled)\n",
    "ax1.scatter(X_unscaled[:, 0], X_unscaled[:, 1], c=labels_unscaled, \n",
    "           cmap='viridis', s=30, alpha=0.7)\n",
    "ax1.set_title('‚ùå Mistake: Unscaled Features', fontsize=10, fontweight='bold', color='red')\n",
    "ax1.set_xlabel('Feature 1 (range: 0-10)')\n",
    "ax1.set_ylabel('Feature 2 (range: 0-1000)')\n",
    "\n",
    "# Fix 1: Scale features\n",
    "ax2 = axes[0, 1]\n",
    "scaler = StandardScaler()\n",
    "X_scaled_fix = scaler.fit_transform(X_unscaled)\n",
    "kmeans_scaled = KMeans(n_clusters=3, random_state=42)\n",
    "labels_scaled = kmeans_scaled.fit_predict(X_scaled_fix)\n",
    "ax2.scatter(X_scaled_fix[:, 0], X_scaled_fix[:, 1], c=labels_scaled, \n",
    "           cmap='viridis', s=30, alpha=0.7)\n",
    "ax2.set_title('‚úÖ Fix: Scaled Features', fontsize=10, fontweight='bold', color='green')\n",
    "ax2.set_xlabel('Feature 1 (standardized)')\n",
    "ax2.set_ylabel('Feature 2 (standardized)')\n",
    "\n",
    "# Mistake 2: Wrong number of clusters\n",
    "ax3 = axes[0, 2]\n",
    "kmeans_wrong_k = KMeans(n_clusters=10, random_state=42)  # Too many!\n",
    "labels_wrong_k = kmeans_wrong_k.fit_predict(X_mistakes)\n",
    "ax3.scatter(X_mistakes[:, 0], X_mistakes[:, 1], c=labels_wrong_k, \n",
    "           cmap='tab10', s=30, alpha=0.7)\n",
    "silhouette_wrong = silhouette_score(X_mistakes, labels_wrong_k)\n",
    "ax3.set_title(f'‚ùå Mistake: Too Many Clusters (K=10)\\nSilhouette: {silhouette_wrong:.3f}', \n",
    "             fontsize=10, fontweight='bold', color='red')\n",
    "\n",
    "# Fix 2: Use elbow method\n",
    "ax4 = axes[1, 0]\n",
    "kmeans_correct_k = KMeans(n_clusters=3, random_state=42)\n",
    "labels_correct_k = kmeans_correct_k.fit_predict(X_mistakes)\n",
    "ax4.scatter(X_mistakes[:, 0], X_mistakes[:, 1], c=labels_correct_k, \n",
    "           cmap='viridis', s=30, alpha=0.7)\n",
    "silhouette_correct = silhouette_score(X_mistakes, labels_correct_k)\n",
    "ax4.set_title(f'‚úÖ Fix: Optimal K=3\\nSilhouette: {silhouette_correct:.3f}', \n",
    "             fontsize=10, fontweight='bold', color='green')\n",
    "\n",
    "# Mistake 3: Ignoring outliers\n",
    "ax5 = axes[1, 1]\n",
    "X_with_outliers = X_mistakes.copy()\n",
    "outliers = np.random.uniform(-15, 15, (20, 2))\n",
    "X_with_outliers = np.vstack([X_with_outliers, outliers])\n",
    "kmeans_outliers = KMeans(n_clusters=3, random_state=42)\n",
    "labels_outliers = kmeans_outliers.fit_predict(X_with_outliers)\n",
    "ax5.scatter(X_with_outliers[:, 0], X_with_outliers[:, 1], \n",
    "           c=labels_outliers, cmap='viridis', s=30, alpha=0.7)\n",
    "ax5.set_title('‚ùå Mistake: K-Means with Outliers', fontsize=10, fontweight='bold', color='red')\n",
    "\n",
    "# Fix 3: Use DBSCAN\n",
    "ax6 = axes[1, 2]\n",
    "dbscan_fix = DBSCAN(eps=1.5, min_samples=5)\n",
    "labels_dbscan = dbscan_fix.fit_predict(X_with_outliers)\n",
    "unique_labels = set(labels_dbscan)\n",
    "colors_db = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "for k, col in zip(unique_labels, colors_db):\n",
    "    if k == -1:\n",
    "        col = 'black'\n",
    "        marker = 'x'\n",
    "    else:\n",
    "        marker = 'o'\n",
    "    class_member_mask = (labels_dbscan == k)\n",
    "    xy = X_with_outliers[class_member_mask]\n",
    "    ax6.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, s=30, alpha=0.7)\n",
    "ax6.set_title('‚úÖ Fix: DBSCAN Handles Outliers', fontsize=10, fontweight='bold', color='green')\n",
    "\n",
    "plt.suptitle('Common Clustering Mistakes and Solutions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìö Summary of Common Mistakes:\")\n",
    "print(\"\\n1. üìè Not Scaling Features:\")\n",
    "print(\"   Problem: Features with larger scales dominate distance calculations\")\n",
    "print(\"   Solution: Always standardize or normalize your data\")\n",
    "\n",
    "print(\"\\n2. üî¢ Wrong Number of Clusters:\")\n",
    "print(\"   Problem: Too many/few clusters lead to poor segmentation\")\n",
    "print(\"   Solution: Use elbow method, silhouette analysis, or domain knowledge\")\n",
    "\n",
    "print(\"\\n3. üîç Ignoring Outliers:\")\n",
    "print(\"   Problem: K-means is sensitive to outliers\")\n",
    "print(\"   Solution: Use DBSCAN or remove outliers before K-means\")\n",
    "\n",
    "print(\"\\n4. üéØ Wrong Distance Metric (not shown):\")\n",
    "print(\"   Problem: Euclidean distance not always appropriate\")\n",
    "print(\"   Solution: Consider Manhattan, Cosine, or custom metrics\")\n",
    "\n",
    "print(\"\\n5. üîÑ Not Checking Stability (not shown):\")\n",
    "print(\"   Problem: Results change with different random seeds\")\n",
    "print(\"   Solution: Run multiple times, use consensus clustering\")"
   ],
   "id": "b5c7fe4c-79ac-4b84-b000-03691de757b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Part 3 - Design Integration\n",
    "Transform technical clustering results into actionable innovation insights."
   ],
   "id": "4123f424-1633-4d75-ac18-8b7e327832c9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 From Data Points to Innovation Insights"
   ],
   "id": "0c97d290-3454-4f1d-8c6c-eb19467382a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive innovation dataset\n",
    "print(\"üí° Transforming Clusters into Innovation Insights\\n\")\n",
    "\n",
    "# Create realistic innovation data\n",
    "n_innovations = 1000\n",
    "n_features = 10\n",
    "n_clusters = 5\n",
    "\n",
    "# Generate base data\n",
    "X_innovation, y_true = make_blobs(n_samples=n_innovations, \n",
    "                                 n_features=n_features,\n",
    "                                 centers=n_clusters,\n",
    "                                 cluster_std=1.2,\n",
    "                                 random_state=42)\n",
    "\n",
    "# Feature names\n",
    "feature_names = [\n",
    "    'Technical_Complexity', 'Market_Readiness', 'Investment_Required',\n",
    "    'User_Impact', 'Implementation_Time', 'Risk_Level',\n",
    "    'Innovation_Score', 'Scalability', 'Regulatory_Compliance', 'ROI_Potential'\n",
    "]\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_innovation)\n",
    "\n",
    "# Apply clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Create DataFrame\n",
    "innovation_df = pd.DataFrame(X_innovation, columns=feature_names)\n",
    "innovation_df['Cluster'] = labels\n",
    "\n",
    "# Visualize clusters in 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n",
    "for i in range(n_clusters):\n",
    "    mask = labels == i\n",
    "    ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "               c=[colors[i]], s=30, alpha=0.6,\n",
    "               label=f'Cluster {i+1}', edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Add cluster centers\n",
    "centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "ax1.scatter(centers_pca[:, 0], centers_pca[:, 1],\n",
    "           c='black', marker='*', s=300,\n",
    "           edgecolors='white', linewidth=2, zorder=10)\n",
    "\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
    "ax1.set_title('Innovation Clusters (PCA Visualization)', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cluster sizes\n",
    "cluster_sizes = innovation_df['Cluster'].value_counts().sort_index()\n",
    "ax2.bar(range(n_clusters), cluster_sizes.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Cluster', fontsize=11)\n",
    "ax2.set_ylabel('Number of Innovations', fontsize=11)\n",
    "ax2.set_title('Innovation Distribution Across Clusters', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(range(n_clusters))\n",
    "ax2.set_xticklabels([f'Cluster {i+1}' for i in range(n_clusters)])\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(cluster_sizes.values):\n",
    "    ax2.text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Innovation Landscape Overview', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Innovation Clustering Results:\")\n",
    "print(f\"Total Innovations: {n_innovations}\")\n",
    "print(f\"Number of Clusters: {n_clusters}\")\n",
    "print(f\"Average Cluster Size: {n_innovations/n_clusters:.0f}\")\n",
    "print(f\"Silhouette Score: {silhouette_score(X_scaled, labels):.3f}\")"
   ],
   "id": "e91d6c4d-5281-4af2-b1b6-9eb4a512c599"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Creating Innovation Archetypes"
   ],
   "id": "05d743e0-ea59-4a37-8962-101de283db9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create innovation archetypes from clusters\n",
    "print(\"üé≠ Creating Innovation Archetypes\\n\")\n",
    "\n",
    "# Calculate cluster characteristics\n",
    "archetype_names = [\n",
    "    'Digital Pioneers',\n",
    "    'Market Disruptors', \n",
    "    'Efficiency Optimizers',\n",
    "    'Customer Champions',\n",
    "    'Platform Builders'\n",
    "]\n",
    "\n",
    "archetype_descriptions = [\n",
    "    'High-tech, high-risk innovations targeting early adopters',\n",
    "    'Game-changing solutions that redefine market dynamics',\n",
    "    'Process improvements focusing on cost and time savings',\n",
    "    'User-centric innovations prioritizing experience',\n",
    "    'Ecosystem solutions creating network effects'\n",
    "]\n",
    "\n",
    "# Analyze each cluster\n",
    "archetypes = []\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_data = innovation_df[innovation_df['Cluster'] == cluster_id]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    archetype = {\n",
    "        'Cluster': cluster_id + 1,\n",
    "        'Name': archetype_names[cluster_id],\n",
    "        'Description': archetype_descriptions[cluster_id],\n",
    "        'Size': len(cluster_data),\n",
    "        'Percentage': f\"{len(cluster_data)/len(innovation_df)*100:.1f}%\"\n",
    "    }\n",
    "    \n",
    "    # Top features (highest average values)\n",
    "    feature_means = cluster_data[feature_names].mean()\n",
    "    top_features = feature_means.nlargest(3).index.tolist()\n",
    "    archetype['Top_Features'] = ', '.join(top_features)\n",
    "    \n",
    "    # Risk profile\n",
    "    risk_level = cluster_data['Risk_Level'].mean()\n",
    "    if risk_level > 0.5:\n",
    "        archetype['Risk_Profile'] = 'High Risk'\n",
    "    elif risk_level > -0.5:\n",
    "        archetype['Risk_Profile'] = 'Medium Risk'\n",
    "    else:\n",
    "        archetype['Risk_Profile'] = 'Low Risk'\n",
    "    \n",
    "    archetypes.append(archetype)\n",
    "\n",
    "# Create archetype cards visualization\n",
    "fig, axes = plt.subplots(1, n_clusters, figsize=(18, 6))\n",
    "\n",
    "for idx, archetype in enumerate(archetypes):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create radar chart for each archetype\n",
    "    cluster_data = innovation_df[innovation_df['Cluster'] == idx]\n",
    "    feature_values = cluster_data[feature_names[:6]].mean().values\n",
    "    \n",
    "    # Normalize to 0-1 scale\n",
    "    feature_values = (feature_values - feature_values.min()) / (feature_values.max() - feature_values.min())\n",
    "    \n",
    "    # Create radar chart\n",
    "    angles = np.linspace(0, 2*np.pi, len(feature_names[:6]), endpoint=False)\n",
    "    feature_values = np.concatenate((feature_values, [feature_values[0]]))  # Complete the circle\n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "    \n",
    "    ax = plt.subplot(1, n_clusters, idx+1, projection='polar')\n",
    "    ax.plot(angles, feature_values, 'o-', linewidth=2, color=colors[idx])\n",
    "    ax.fill(angles, feature_values, alpha=0.25, color=colors[idx])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels([f.replace('_', '\\n') for f in feature_names[:6]], fontsize=8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f\"{archetype['Name']}\\n({archetype['Size']} innovations)\", \n",
    "                fontsize=10, fontweight='bold', pad=20)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('Innovation Archetype Profiles', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display archetype summary\n",
    "archetypes_df = pd.DataFrame(archetypes)\n",
    "print(\"\\nüìã Innovation Archetype Summary:\")\n",
    "display(archetypes_df[['Name', 'Size', 'Percentage', 'Risk_Profile', 'Top_Features']])\n",
    "\n",
    "print(\"\\nüí° How to Use Archetypes:\")\n",
    "print(\"‚Ä¢ Tailor innovation strategies per archetype\")\n",
    "print(\"‚Ä¢ Allocate resources based on archetype characteristics\")\n",
    "print(\"‚Ä¢ Design specific support programs for each type\")\n",
    "print(\"‚Ä¢ Track archetype evolution over time\")"
   ],
   "id": "194fca9c-c254-4d8f-b1cb-396835ae99c8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Innovation Taxonomy & Lifecycle"
   ],
   "id": "5eb6c45a-dc5d-43e6-8aae-b3dfbb2fe6a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build innovation taxonomy\n",
    "print(\"üå≥ Building Innovation Taxonomy\\n\")\n",
    "\n",
    "# Hierarchical clustering for taxonomy\n",
    "linkage_matrix = linkage(kmeans.cluster_centers_, method='ward')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Dendrogram\n",
    "dendrogram(linkage_matrix, ax=ax1, labels=archetype_names,\n",
    "          color_threshold=0, above_threshold_color='gray')\n",
    "ax1.set_title('Innovation Taxonomy Hierarchy', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Innovation Archetype')\n",
    "ax1.set_ylabel('Distance')\n",
    "\n",
    "# Lifecycle stages\n",
    "lifecycle_stages = ['Ideation', 'Validation', 'Development', 'Launch', 'Scale', 'Maturity']\n",
    "stage_distribution = np.random.dirichlet(np.ones(len(lifecycle_stages)), size=n_clusters)\n",
    "\n",
    "# Stack bar chart for lifecycle\n",
    "bottom = np.zeros(n_clusters)\n",
    "stage_colors = plt.cm.coolwarm(np.linspace(0, 1, len(lifecycle_stages)))\n",
    "\n",
    "for stage_idx, stage in enumerate(lifecycle_stages):\n",
    "    values = stage_distribution[:, stage_idx]\n",
    "    ax2.bar(range(n_clusters), values, bottom=bottom, \n",
    "           color=stage_colors[stage_idx], label=stage, alpha=0.8)\n",
    "    bottom += values\n",
    "\n",
    "ax2.set_xlabel('Innovation Archetype', fontsize=11)\n",
    "ax2.set_ylabel('Proportion', fontsize=11)\n",
    "ax2.set_title('Innovation Lifecycle Distribution by Archetype', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(range(n_clusters))\n",
    "ax2.set_xticklabels([name.split()[0] for name in archetype_names], rotation=45)\n",
    "ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Taxonomy Insights:\")\n",
    "print(\"‚Ä¢ Digital Pioneers and Market Disruptors are closely related\")\n",
    "print(\"‚Ä¢ Efficiency Optimizers form a distinct branch\")\n",
    "print(\"‚Ä¢ Platform Builders bridge multiple categories\")"
   ],
   "id": "505b2d9d-d975-43e8-8da5-e7e3e5f91079"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Opportunity Analysis"
   ],
   "id": "5925eb9e-c16f-4101-b5e7-c7414c35026a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate opportunity heatmap\n",
    "print(\"üî• Innovation Opportunity Heatmap\\n\")\n",
    "\n",
    "# Calculate opportunity scores\n",
    "opportunity_dimensions = [\n",
    "    'Market_Size', 'Growth_Rate', 'Competition',\n",
    "    'Tech_Readiness', 'Investment_Need', 'Time_to_Market',\n",
    "    'Risk_Level', 'Regulatory', 'Customer_Demand'\n",
    "]\n",
    "\n",
    "# Create opportunity matrix\n",
    "opportunity_matrix = np.random.randn(n_clusters, len(opportunity_dimensions))\n",
    "\n",
    "# Create heatmap\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap\n",
    "im = ax1.imshow(opportunity_matrix, cmap='RdYlGn', aspect='auto', vmin=-2, vmax=2)\n",
    "ax1.set_xticks(range(len(opportunity_dimensions)))\n",
    "ax1.set_xticklabels(opportunity_dimensions, rotation=45, ha='right')\n",
    "ax1.set_yticks(range(n_clusters))\n",
    "ax1.set_yticklabels(archetype_names)\n",
    "ax1.set_title('Innovation Opportunity Heatmap', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add values\n",
    "for i in range(n_clusters):\n",
    "    for j in range(len(opportunity_dimensions)):\n",
    "        text = ax1.text(j, i, f'{opportunity_matrix[i, j]:.1f}',\n",
    "                       ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "plt.colorbar(im, ax=ax1, label='Opportunity Score')\n",
    "\n",
    "# Priority matrix\n",
    "impact = innovation_df.groupby('Cluster')['User_Impact'].mean().values\n",
    "effort = innovation_df.groupby('Cluster')['Implementation_Time'].mean().values\n",
    "\n",
    "ax2.scatter(effort, impact, s=cluster_sizes.values*2, c=colors, \n",
    "           alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, name in enumerate(archetype_names):\n",
    "    ax2.annotate(name.split()[0], (effort[i], impact[i]),\n",
    "                ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Add quadrant lines\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add quadrant labels\n",
    "ax2.text(1, 1, 'High Impact\\nHigh Effort', ha='center', va='center', \n",
    "        fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
    "ax2.text(-1, 1, 'High Impact\\nLow Effort', ha='center', va='center', \n",
    "        fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "ax2.text(-1, -1, 'Low Impact\\nLow Effort', ha='center', va='center', \n",
    "        fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "ax2.text(1, -1, 'Low Impact\\nHigh Effort', ha='center', va='center', \n",
    "        fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.3))\n",
    "\n",
    "ax2.set_xlabel('Implementation Effort', fontsize=11)\n",
    "ax2.set_ylabel('User Impact', fontsize=11)\n",
    "ax2.set_title('Innovation Priority Matrix', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Strategic Recommendations:\")\n",
    "print(\"\\nüü¢ Quick Wins (High Impact, Low Effort):\")\n",
    "print(\"  ‚Ä¢ Focus immediate resources here\")\n",
    "print(\"  ‚Ä¢ Rapid prototyping and testing\")\n",
    "print(\"\\nüü° Strategic Initiatives (High Impact, High Effort):\")\n",
    "print(\"  ‚Ä¢ Long-term investment required\")\n",
    "print(\"  ‚Ä¢ Build dedicated teams\")\n",
    "print(\"\\nüîµ Fill-ins (Low Impact, Low Effort):\")\n",
    "print(\"  ‚Ä¢ Good for learning and experimentation\")\n",
    "print(\"  ‚Ä¢ Assign to junior teams\")\n",
    "print(\"\\nüî¥ Avoid (Low Impact, High Effort):\")\n",
    "print(\"  ‚Ä¢ Deprioritize or eliminate\")\n",
    "print(\"  ‚Ä¢ Redirect resources elsewhere\")"
   ],
   "id": "0d39f814-070b-4518-8c23-7ac4ef7d2951"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Innovation Ecosystem"
   ],
   "id": "82250486-1c8c-4280-bfab-c4d397073b19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create innovation ecosystem network\n",
    "print(\"üåê Innovation Ecosystem Network\\n\")\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Create network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes for archetypes\n",
    "for i, name in enumerate(archetype_names):\n",
    "    G.add_node(name, node_type='archetype', size=cluster_sizes.values[i])\n",
    "\n",
    "# Add stakeholder nodes\n",
    "stakeholders = ['Customers', 'Partners', 'Investors', 'Regulators', 'Competitors']\n",
    "for stakeholder in stakeholders:\n",
    "    G.add_node(stakeholder, node_type='stakeholder', size=100)\n",
    "\n",
    "# Add edges (connections)\n",
    "connections = [\n",
    "    ('Digital Pioneers', 'Investors', 0.8),\n",
    "    ('Digital Pioneers', 'Partners', 0.6),\n",
    "    ('Market Disruptors', 'Competitors', 0.9),\n",
    "    ('Market Disruptors', 'Customers', 0.7),\n",
    "    ('Efficiency Optimizers', 'Partners', 0.8),\n",
    "    ('Efficiency Optimizers', 'Regulators', 0.5),\n",
    "    ('Customer Champions', 'Customers', 0.9),\n",
    "    ('Customer Champions', 'Partners', 0.6),\n",
    "    ('Platform Builders', 'Partners', 0.9),\n",
    "    ('Platform Builders', 'Investors', 0.7)\n",
    "]\n",
    "\n",
    "for source, target, weight in connections:\n",
    "    G.add_edge(source, target, weight=weight)\n",
    "\n",
    "# Visualize network\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "\n",
    "# Draw nodes\n",
    "archetype_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'archetype']\n",
    "stakeholder_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'stakeholder']\n",
    "\n",
    "# Archetype nodes\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=archetype_nodes,\n",
    "                      node_color=colors[:len(archetype_nodes)],\n",
    "                      node_size=[G.nodes[n]['size']*5 for n in archetype_nodes],\n",
    "                      alpha=0.7, ax=ax)\n",
    "\n",
    "# Stakeholder nodes\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=stakeholder_nodes,\n",
    "                      node_color='lightgray',\n",
    "                      node_size=500,\n",
    "                      node_shape='s',\n",
    "                      alpha=0.8, ax=ax)\n",
    "\n",
    "# Draw edges\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u, v in edges]\n",
    "nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights],\n",
    "                      alpha=0.5, ax=ax)\n",
    "\n",
    "# Labels\n",
    "labels = {n: n.split()[0] if len(n.split()) > 1 else n for n in G.nodes()}\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=10, font_weight='bold', ax=ax)\n",
    "\n",
    "ax.set_title('Innovation Ecosystem Network', fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "legend_elements = [\n",
    "    Circle((0, 0), 0.1, facecolor=colors[0], alpha=0.7, label='Innovation Archetypes'),\n",
    "    Rectangle((0, 0), 0.1, 0.1, facecolor='lightgray', alpha=0.8, label='Stakeholders')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüåê Ecosystem Insights:\")\n",
    "print(\"‚Ä¢ Digital Pioneers have strong investor connections\")\n",
    "print(\"‚Ä¢ Customer Champions directly connect with users\")\n",
    "print(\"‚Ä¢ Platform Builders bridge multiple stakeholder groups\")\n",
    "print(\"‚Ä¢ Market Disruptors create competitive tension\")\n",
    "print(\"\\nüí° Use this network to:\")\n",
    "print(\"‚Ä¢ Identify collaboration opportunities\")\n",
    "print(\"‚Ä¢ Understand influence patterns\")\n",
    "print(\"‚Ä¢ Design stakeholder engagement strategies\")"
   ],
   "id": "0a799fca-993d-464b-9932-43eee3595e3f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Part 2 Summary\n",
    "\n",
    "### Technical Skills Mastered:\n",
    "1. **K-Means**: Understanding and implementing from scratch\n",
    "2. **Optimal K**: Multiple methods for finding best clusters\n",
    "3. **DBSCAN**: Handling complex shapes and outliers\n",
    "4. **Hierarchical**: Building taxonomies and dendrograms\n",
    "5. **GMM**: Soft clustering with probabilities\n",
    "6. **Comparison**: Choosing the right algorithm\n",
    "\n",
    "### Design Applications Learned:\n",
    "1. **Innovation Archetypes**: Data-driven personas\n",
    "2. **Opportunity Heatmaps**: Identifying white spaces\n",
    "3. **Priority Matrices**: Strategic resource allocation\n",
    "4. **Ecosystem Networks**: Understanding connections\n",
    "5. **Innovation Taxonomy**: Hierarchical organization\n",
    "\n",
    "### Next: Part 3 - Practice & Advanced Topics\n",
    "Apply everything with real case studies and advanced visualizations!"
   ],
   "id": "261a29b7-8643-42ba-8e5a-cf985bf81bfb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Part 2 Complete: Technical & Design Integration\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou've completed:\")\n",
    "print(\"‚Ä¢ Section 3: All clustering algorithms\")\n",
    "print(\"‚Ä¢ Section 4: Design integration and applications\")\n",
    "print(\"\\nüìö Ready for Part 3: Practice, Case Studies, and Advanced Topics\")\n",
    "print(\"\\nContinue with Week01_Part3_Practice_Advanced.ipynb\")"
   ],
   "id": "fb5a20c0-fc21-413b-aee4-c3b409b084d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}