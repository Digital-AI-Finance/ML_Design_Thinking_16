% Part 3: Modern Architectures (10 slides)

\begin{frame}{12. Human Introspection: Vision is Hierarchical}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{How Humans See (Hubel \& Wiesel 1959):}
\begin{itemize}
\item \textbf{Level 1}: Edge detection (V1 cortex)
  \begin{itemize}
  \item Horizontal, vertical, diagonal lines
  \item Local contrast detection
  \end{itemize}
\item \textbf{Level 2}: Texture \& shape (V2, V4)
  \begin{itemize}
  \item Curves, corners, textures
  \item Spatial relationships
  \end{itemize}
\item \textbf{Level 3}: Objects (IT cortex)
  \begin{itemize}
  \item Faces, cars, animals
  \item Invariant recognition
  \end{itemize}
\item \textbf{Inspiration}: Fukushima's Neocognitron (1980) - precursor to modern CNNs
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/human_visual_hierarchy.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Hubel \& Wiesel (1959 Nobel Prize): Discovered hierarchical processing in cat visual cortex - inspired modern neural network architectures}
\end{frame}

\begin{frame}{13. Hypothesis: Specialized Architectures Matching Data Structure}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{The Key Insight:}
\begin{itemize}
\item \textcolor{mlred}{Problem}: Generic MLPs ignore data structure
\item \textcolor{mlgreen}{Solution}: Architecture matches inductive bias
\item \textbf{No Free Lunch Theorem}: No single architecture optimal for all problems
\item Architecture choice encodes prior knowledge about problem structure
\end{itemize}

\textbf{Examples:}
\begin{itemize}
\item \textbf{Images}: Spatial locality -> CNNs
\item \textbf{Sequences}: Temporal order -> RNNs
\item \textbf{Graphs}: Node relationships -> GNNs
\item \textbf{Language}: Long-range dependencies -> Transformers
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/architecture_data_matching.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Right architecture = built-in prior knowledge about the problem domain}
\end{frame}

\begin{frame}{14. Zero-Jargon: Convolution as ``Sliding Pattern Detector''}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{Convolution Intuition:}
\begin{itemize}
\item Take a small ``template'' (3x3 filter)
\item Slide it across the entire image
\item At each position: compute similarity
\item Result: ``Where is this pattern?''
\end{itemize}

\textbf{Example Filters:}
\begin{itemize}
\item Edge detector: $\begin{bmatrix} -1 & 0 & 1 \\ -1 & 0 & 1 \\ -1 & 0 & 1 \end{bmatrix}$
\item Blur: $\frac{1}{9}\begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/convolution_intuition.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Convolution = template matching with learnable templates}
\end{frame}

\begin{frame}{15. Geometric Intuition: Filters Detect Edges/Textures}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{What Filters Learn:}

\textbf{Layer 1}: Low-level features
\begin{itemize}
\item Edges, corners, blobs
\item Oriented lines at different angles
\item Color gradients
\end{itemize}

\textbf{Layer 2}: Mid-level features
\begin{itemize}
\item Textures, patterns
\item Simple shapes
\item Motifs and repeating elements
\end{itemize}

\textbf{Layer 3+}: High-level features
\begin{itemize}
\item Object parts (eyes, wheels)
\item Complex patterns
\end{itemize}

\textbf{Visualization Technique:}
\begin{itemize}
\item Apply filter to white noise to see what pattern it detects
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/filter_hierarchy.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Each layer builds more complex features from simpler ones}
\end{frame}

\begin{frame}{16. CNN Architecture Details}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{Key Components:}

\textbf{1. Convolutional Layers}
\begin{itemize}
\item Multiple filters per layer
\item Shared weights across spatial locations
\item Parameter sharing: Same filter applied to all spatial locations - reduces parameters by 1000x
\end{itemize}

\textbf{2. Pooling Layers}
\begin{itemize}
\item Downsampling (max, average)
\item Translation invariance
\item Computational efficiency
\end{itemize}

\textbf{3. Fully Connected}
\begin{itemize}
\item Final classification
\item Combines all learned features
\end{itemize}

\textbf{Example Parameters:}
\begin{itemize}
\item AlexNet: 60M parameters
\item VGG: 138M, ResNet: 25M
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/cnn_architecture_detailed.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{CNN = Feature extraction (conv+pool) + Classification (FC)}
\end{frame}

\begin{frame}{17. Full Walkthrough: Convolve Filter with Actual Numbers}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{Example Calculation:}

\textbf{Input (3x3):}
$\begin{bmatrix} 1 & 2 & 1 \\ 0 & 1 & 2 \\ 1 & 0 & 1 \end{bmatrix}$

\textbf{Filter (3x3):}
$\begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}$

\textbf{Convolution (element-wise multiply + sum):}
\begin{align}
&= (-1)(1) + (0)(2) + (1)(1) + \\
&\quad (-2)(0) + (0)(1) + (2)(2) + \\
&\quad (-1)(1) + (0)(0) + (1)(1) \\
&= -1 + 0 + 1 - 0 + 0 + 4 - 1 + 0 + 1 \\
&= 4
\end{align}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/convolution_calculation.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{High response (4) means vertical edge detected at this location}
\end{frame}

\begin{frame}{18. RNN and Transformer Architectures}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{Recurrent Neural Networks:}
\begin{align}
h_t &= \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h) \\
y_t &= W_{hy}h_t + b_y
\end{align}

\begin{itemize}
\item Hidden state carries memory
\item Sequential processing ($O(n)$ path length)
\item Good for: Time series, NLP
\item Problem: Vanishing gradients over time
\end{itemize}

\textbf{Transformers (``Attention Is All You Need'' 2017):}

\textbf{Attention Mechanism:}
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
\begin{itemize}
\item Query: What I'm looking for
\item Key: What I have to offer
\item Value: What I'll return if matched
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/rnn_transformer_comparison.pdf}
\end{center}

\textbf{Key Advantages:}
\begin{itemize}
\item Self-attention: $O(1)$ path length
\item Parallel processing (no sequential bottleneck)
\item Long-range dependencies without vanishing gradients
\item State-of-the-art for language (GPT, BERT)
\end{itemize}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Transformers enable O(1) path length vs O(n) for RNNs - revolutionized NLP and beyond}
\end{frame}

\begin{frame}{19. Visualization: Feature Maps, Attention Heatmaps}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{CNN Feature Maps:}
\begin{itemize}
\item Each filter produces a feature map
\item Bright areas = high activation
\item Shows what the network ``sees''
\item Layer 1: Edges and textures
\item Layer N: Complex patterns
\end{itemize}

\textbf{Transformer Attention:}
\begin{itemize}
\item Attention weights as heatmaps
\item Shows which words influence others
\item Different heads learn different patterns
\item Interpretable relationships
\item \textbf{Technique}: Grad-CAM visualizes which regions influenced decision
\item Attention weights reveal learned linguistic structure (syntax trees emerge naturally)
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/feature_maps_attention.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Visualization reveals the internal representations learned by neural networks}
\end{frame}

\begin{frame}{20. Why It Works: Inductive Biases Reduce Search Space}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{The Core Principle:}

\textbf{Without Structure:}
\begin{itemize}
\item Search space: All possible functions
\item Size: Exponential in parameters
\item Sample complexity: Intractable
\end{itemize}

\textbf{With Architecture:}
\begin{itemize}
\item Built-in assumptions about data
\item Drastically reduced search space
\item Faster learning, better generalization
\end{itemize}

\textbf{Examples:}
\begin{itemize}
\item CNNs assume translation invariance
\item RNNs assume sequential dependence
\item Transformers assume attention patterns
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/inductive_bias_reduction.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{Architecture = built-in prior knowledge that guides learning}
\end{frame}

\begin{frame}{21. Experimental Validation: ImageNet Accuracy Over Time}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{ImageNet Challenge Results:}

\begin{center}
\begin{tabular}{lcc}
\toprule
Year & Model & Top-5 Error \\
\midrule
2010 & Traditional CV & 28.2\% \\
2012 & AlexNet (CNN) & 15.3\% \\
2013 & ZFNet & 14.8\% \\
2014 & VGGNet & 7.3\% \\
2014 & GoogLeNet & 6.7\% \\
2015 & ResNet & 3.6\% \\
2017 & DenseNet & 2.2\% \\
\midrule
- & Human Performance & 5.1\% \\
\bottomrule
\end{tabular}
\end{center}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/imagenet_progress.pdf}
\end{center}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{ResNet (2015): First to surpass human performance (3.6\% vs 5.1\%) - CNNs achieved superhuman vision in just 5 years}
\end{frame}

\begin{frame}{22. Solutions to Vanishing Gradients}
\begin{columns}[c]
\column{0.48\textwidth}
\textbf{1. ReLU Activation:}
\begin{align}
\text{ReLU}(x) &= \max(0, x) \\
\frac{d}{dx}\text{ReLU}(x) &= \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases}
\end{align}

\textbf{Comparison:}
\begin{itemize}
\item Sigmoid: $\sigma'(x) \leq 0.25$ (vanishes)
\item ReLU: gradient = 1 (constant for $x>0$)
\item Enables training 100+ layer networks
\end{itemize}

\textbf{2. Other Solutions:}
\begin{itemize}
\item Batch Normalization (2015): Stabilize distributions
\item Skip connections (ResNet 2015): Shortcut paths
\item Xavier/He initialization: Preserve variance
\end{itemize}

\column{0.48\textwidth}
\textbf{Gradient Flow Comparison:}

\textbf{Sigmoid Network (10 layers):}
\begin{itemize}
\item Layer 10 gradient: $1.0$
\item Layer 5 gradient: $0.001$
\item Layer 1 gradient: $0.000001$
\end{itemize}

\textbf{ReLU Network (10 layers):}
\begin{itemize}
\item Layer 10 gradient: $1.0$
\item Layer 5 gradient: $1.0$
\item Layer 1 gradient: $1.0$
\end{itemize}

\vspace{0.3cm}
\textbf{Result:}
\begin{itemize}
\item ReLU: All layers learn at similar rates
\item Deep networks become trainable
\item 2012 breakthrough: AlexNet (8 layers with ReLU)
\end{itemize}
\end{columns}

\vspace{\fill}
\small\textcolor{gray}{ReLU's constant gradient solved the vanishing gradient problem - enabling the deep learning revolution}
\end{frame}