% Part 1: Machine Learning Foundations
\section{Machine Learning Foundations}

% Section divider
\begin{frame}[plain]
\vfill
\centering
\begin{beamercolorbox}[sep=16pt,center]{title}
\usebeamerfont{title}\Large Part 1: Machine Learning Foundations\par
\vspace{0.5em}
\large Theory, Mathematics, and Finance Applications\par
\end{beamercolorbox}
\vfill
\end{frame}

% Opening Power Chart
\begin{frame}{The \$10 Trillion ML Revolution in Finance}
\centering
\includegraphics[width=0.9\textwidth]{charts/ml_finance_landscape.pdf}
\vspace{0.5em}
\Large\textcolor{finblue}{\textbf{Machine Learning is Transforming Every Corner of Finance}}
\end{frame}

% What is Machine Learning
\begin{frame}{Formal Definition: What is Machine Learning?}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\Large\textbf{Tom Mitchell's Definition (1997)}
\normalsize
\vspace{0.5em}

A computer program learns from:
\begin{itemize}
\item \textbf{Experience} $E$ with respect to
\item \textbf{Task} $T$ and
\item \textbf{Performance measure} $P$
\end{itemize}

if its performance at task $T$, as measured by $P$, improves with experience $E$.

\vspace{0.5em}
\begin{tcolorbox}[colback=finblue!10, colframe=finblue!50]
\textbf{Finance Example:}\\
E: Historical stock prices\\
T: Predict tomorrow's return\\
P: Sharpe ratio of predictions
\end{tcolorbox}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{Mathematical View}
\normalsize
\vspace{0.5em}

Learn function $f: \mathcal{X} \rightarrow \mathcal{Y}$

Given training set:
$$\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$$

Find:
$$\hat{f} = \argmin_{f \in \mathcal{F}} \sum_{i=1}^n L(y_i, f(x_i)) + \lambda R(f)$$

where:
\begin{itemize}
\item $L$: Loss function
\item $R$: Regularization term
\item $\lambda$: Regularization strength
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% ML vs Traditional Finance
\begin{frame}{Machine Learning vs Traditional Finance Models}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\centering
\textcolor{fingray}{\Large\textbf{Traditional Finance}}
\normalsize
\vspace{0.5em}

\textbf{Black-Scholes-Merton (1973)}
$$C = S_0\Phi(d_1) - Ke^{-rT}\Phi(d_2)$$

\begin{itemize}
\item Strong assumptions
\item Closed-form solutions
\item Model-driven
\item Limited parameters
\item Interpretable
\end{itemize}

\textcolor{finred}{\textbf{Limitations:}}
\begin{itemize}
\item Assumes log-normal returns
\item Constant volatility
\item No transaction costs
\item Perfect markets
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\centering
\textcolor{finblue}{\Large\textbf{Machine Learning}}
\normalsize
\vspace{0.5em}

\textbf{Neural Network Pricing}
$$\hat{C} = NN(S_0, K, T, \sigma_{impl}, \text{Greeks}, ...)$$

\begin{itemize}
\item Data-driven discovery
\item Non-parametric
\item Flexible architecture
\item High-dimensional
\item Accurate but opaque
\end{itemize}

\textcolor{fingreen}{\textbf{Advantages:}}
\begin{itemize}
\item Captures market microstructure
\item Adapts to regime changes
\item Includes all observables
\item Learns from anomalies
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% Types of Learning
\begin{frame}{Three Paradigms of Machine Learning}
\begin{columns}[T]
\begin{column}{0.32\textwidth}
\centering
\textcolor{finblue}{\Large\textbf{Supervised}}
\normalsize
\vspace{0.3em}

\includegraphics[width=\textwidth]{charts/supervised_paradigm.pdf}

$$\{(x_i, y_i)\}_{i=1}^n \rightarrow \hat{f}$$

\textbf{Finance Applications:}
\begin{itemize}
\item Credit scoring
\item Stock prediction
\item Fraud detection
\item Option pricing
\end{itemize}

\textbf{Key Algorithms:}
\begin{itemize}
\item Random Forest
\item XGBoost
\item Neural Networks
\end{itemize}
\end{column}

\begin{column}{0.32\textwidth}
\centering
\textcolor{fingreen}{\Large\textbf{Unsupervised}}
\normalsize
\vspace{0.3em}

\includegraphics[width=\textwidth]{charts/unsupervised_paradigm.pdf}

$$\{x_i\}_{i=1}^n \rightarrow \text{Structure}$$

\textbf{Finance Applications:}
\begin{itemize}
\item Portfolio clustering
\item Anomaly detection
\item Risk factors
\item Market regimes
\end{itemize}

\textbf{Key Algorithms:}
\begin{itemize}
\item K-means
\item PCA/ICA
\item Autoencoders
\end{itemize}
\end{column}

\begin{column}{0.32\textwidth}
\centering
\textcolor{finorange}{\Large\textbf{Reinforcement}}
\normalsize
\vspace{0.3em}

\includegraphics[width=\textwidth]{charts/reinforcement_paradigm.pdf}

$$(s_t, a_t, r_t, s_{t+1}) \rightarrow \pi^*$$

\textbf{Finance Applications:}
\begin{itemize}
\item Portfolio optimization
\item Execution strategies
\item Market making
\item Hedging
\end{itemize}

\textbf{Key Algorithms:}
\begin{itemize}
\item Q-Learning
\item PPO
\item A3C
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% The Learning Process
\begin{frame}{The Machine Learning Pipeline in Finance}
\centering
\begin{tikzpicture}[scale=0.8, transform shape]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=finblue!20]
\tikzstyle{data} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=fingreen!20]
\tikzstyle{decision} = [diamond, minimum width=2cm, minimum height=1cm, text centered, draw=black, fill=finorange!20]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Pipeline
\node[data] (raw) at (0,4) {Market Data};
\node[process] (clean) at (3,4) {Preprocessing};
\node[process] (feature) at (6,4) {Feature Engineering};
\node[process] (split) at (9,4) {Train/Val/Test Split};

\node[process] (train) at (3,2) {Model Training};
\node[decision] (validate) at (6,2) {Validation};
\node[process] (optimize) at (9,2) {Hyperparameter Tuning};

\node[process] (test) at (3,0) {Testing};
\node[decision] (deploy) at (6,0) {Production Ready?};
\node[data] (production) at (9,0) {Live Trading};

% Arrows
\draw[arrow] (raw) -- (clean);
\draw[arrow] (clean) -- (feature);
\draw[arrow] (feature) -- (split);
\draw[arrow] (split) -- (train);
\draw[arrow] (train) -- (validate);
\draw[arrow] (validate) -- (optimize);
\draw[arrow] (optimize) -- (test);
\draw[arrow] (test) -- (deploy);
\draw[arrow] (deploy) -- (production);

% Feedback loops
\draw[arrow, dashed, finred] (validate) -- node[right] {Overfit} (3,2.5) -- (train);
\draw[arrow, dashed, finred] (deploy) -- node[right] {Fail} (3,0.5) -- (test);
\end{tikzpicture}

\vspace{0.5em}
\Large\textcolor{finblue}{\textbf{Critical: 70/15/15 Split for Financial Time Series}}
\end{frame}

% Loss Functions
\begin{frame}{Loss Functions: The Optimization Objective}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\Large\textbf{Regression Losses}
\normalsize
\vspace{0.5em}

\textbf{Mean Squared Error (MSE):}
$$L_{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

\textbf{Mean Absolute Error (MAE):}
$$L_{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

\textbf{Huber Loss (Robust):}
$$L_\delta(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta|y - \hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise}
\end{cases}$$

\textbf{Quantile Loss (VaR):}
$$L_\tau = \sum_{i} \rho_\tau(y_i - \hat{y}_i)$$
where $\rho_\tau(u) = u(\tau - \mathbb{1}_{u<0})$
\end{column}

\begin{column}{0.48\textwidth}
\Large\textbf{Finance-Specific Losses}
\normalsize
\vspace{0.5em}

\textbf{Sharpe Ratio Loss:}
$$L_{Sharpe} = -\frac{\E[R_p]}{\sqrt{\Var[R_p]}}$$

\textbf{Maximum Drawdown:}
$$L_{MDD} = \max_{t \in [0,T]} \left(1 - \frac{P_t}{\max_{s \in [0,t]} P_s}\right)$$

\textbf{Directional Accuracy:}
$$L_{DA} = -\frac{1}{n}\sum_{i=1}^{n} \mathbb{1}[\sign(y_i) = \sign(\hat{y}_i)]$$

\textbf{Profit \& Loss:}
$$L_{PnL} = -\sum_{i=1}^{n} \hat{y}_i \cdot r_i$$
where $r_i$ is actual return
\end{column}
\end{columns}
\end{frame}

% Bias-Variance Tradeoff
\begin{frame}{The Bias-Variance Decomposition}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\Large\textbf{Fundamental ML Theorem}
\normalsize
\vspace{0.5em}

For squared loss, the expected error decomposes as:

$$\E[(y - \hat{f}(x))^2] = \underbrace{\text{Bias}^2[\hat{f}(x)]}_{\text{Underfitting}} + \underbrace{\text{Var}[\hat{f}(x)]}_{\text{Overfitting}} + \underbrace{\sigma^2}_{\text{Irreducible}}$$

where:
\begin{align*}
\text{Bias}[\hat{f}(x)] &= \E[\hat{f}(x)] - f(x) \\
\text{Var}[\hat{f}(x)] &= \E[(\hat{f}(x) - \E[\hat{f}(x)])^2] \\
\sigma^2 &= \text{Var}[\epsilon]
\end{align*}

\vspace{0.5em}
\textbf{Finance Interpretation:}
\begin{itemize}
\item \textbf{Bias}: Systematic pricing errors
\item \textbf{Variance}: Model instability
\item \textbf{Noise}: Market microstructure
\end{itemize}
\end{column}

\begin{column}{0.43\textwidth}
\centering
\includegraphics[width=\textwidth]{charts/bias_variance_tradeoff.pdf}

\vspace{0.5em}
\textbf{Model Complexity \& Error}

\begin{tikzpicture}[scale=0.7]
\begin{axis}[
    xlabel={Model Complexity},
    ylabel={Error},
    xmin=0, xmax=10,
    ymin=0, ymax=1,
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]
\addplot[color=finblue, thick] coordinates {
    (1,0.9) (2,0.7) (3,0.5) (4,0.4) (5,0.35) (6,0.33) (7,0.32) (8,0.33) (9,0.35) (10,0.4)
};
\addplot[color=finred, thick] coordinates {
    (1,0.3) (2,0.3) (3,0.32) (4,0.35) (5,0.4) (6,0.5) (7,0.65) (8,0.8) (9,0.95) (10,1.0)
};
\legend{Training Error, Test Error}
\end{axis}
\end{tikzpicture}
\end{column}
\end{columns}
\end{frame}

% More slides would continue here for the remaining 17 slides of Part 1...
% Including: Model Capacity, Feature Engineering, Regularization, Cross-validation,
% Ensemble Methods, Gradient Descent, Hyperparameter Tuning, Model Selection, etc.