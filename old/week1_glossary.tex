\documentclass[8pt,aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multicol}

% Theme
\usetheme{Madrid}
\usecolortheme{seahorse}
\setbeamertemplate{navigation symbols}{}

% Colors
\definecolor{myblue}{RGB}{0,102,204}
\definecolor{mygreen}{RGB}{0,153,76}
\definecolor{myred}{RGB}{204,0,0}

\title{Technical Glossary}
\subtitle{Key Terms and Definitions for AI-Driven Design Thinking}
\author{Week 1 Appendix}
\date{}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

% Basic ML Terms
\begin{frame}[t]{Machine Learning Fundamentals}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Machine Learning (ML)}: Computer systems that learn patterns from data without explicit programming. Like teaching by examples rather than rules.

\item \textbf{Algorithm}: Step-by-step procedure for solving a problem. Think of it as a recipe that computers follow.

\item \textbf{Training}: Process of teaching an ML model by showing it examples. Like a student learning from practice problems.

\item \textbf{Model}: The learned representation that can make predictions. Like a mental model built from experience.

\item \textbf{Pattern Recognition}: Identifying regularities in data. Like recognizing faces - you know one when you see it.

\item \textbf{Feature}: A measurable property of data. Like height, weight, or color in describing a person.
\end{itemize}
\end{frame}

% Learning Types
\begin{frame}[t]{Types of Learning}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Supervised Learning}: Learning with labeled examples. Like learning with an answer key.
  \begin{itemize}
  \item Example: Email spam detection (spam/not spam labels)
  \end{itemize}

\item \textbf{Unsupervised Learning}: Finding patterns without labels. Like organizing books without categories.
  \begin{itemize}
  \item Example: Customer segmentation without predefined groups
  \end{itemize}

\item \textbf{Classification}: Predicting categories. Is this email spam? (Yes/No)

\item \textbf{Regression}: Predicting continuous values. What will the temperature be? (73.5°F)

\item \textbf{Clustering}: Grouping similar items together. Like sorting laundry by color.
\end{itemize}
\end{frame}

% NLP Terms 1
\begin{frame}[t]{Natural Language Processing (NLP) - Part 1}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{NLP}: Teaching computers to understand human language. Like Google Translate or Siri.

\item \textbf{Tokenization}: Breaking text into words or pieces. "Hello world" → ["Hello", "world"]

\item \textbf{Sentiment Analysis}: Determining emotional tone. Is this review positive or negative?

\item \textbf{Topic Modeling}: Discovering themes in text automatically. Finding what documents are about.

\item \textbf{Named Entity Recognition (NER)}: Finding names, places, dates in text. "Apple Inc." = Company

\item \textbf{Stemming}: Reducing words to root form. "running", "runs", "ran" → "run"
\end{itemize}
\end{frame}

% NLP Terms 2
\begin{frame}[t]{Natural Language Processing (NLP) - Part 2}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Lemmatization}: Smarter stemming using dictionary. "better" → "good"

\item \textbf{Stop Words}: Common words often removed. "the", "is", "at", "which"

\item \textbf{TF-IDF}: Term Frequency-Inverse Document Frequency. Measures word importance in documents.

\item \textbf{Corpus}: Collection of text documents. Your dataset of text.

\item \textbf{N-gram}: Sequence of N words. "New York" is a 2-gram (bigram).

\item \textbf{Bag of Words}: Treating text as unordered word collection. Ignoring grammar, keeping frequency.
\end{itemize}
\end{frame}

% Advanced ML Terms
\begin{frame}[t]{Advanced Machine Learning Concepts}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Neural Network}: ML model inspired by brain structure. Networks of connected nodes.

\item \textbf{Deep Learning}: Neural networks with many layers. "Deep" = many layers.

\item \textbf{Embedding}: Converting words/items to numbers. "Cat" → [0.2, -0.5, 0.8, ...]

\item \textbf{Attention Mechanism}: Model focuses on relevant parts. Like highlighting important text.

\item \textbf{Transformer}: Architecture behind ChatGPT. Processes all words simultaneously.

\item \textbf{Fine-tuning}: Adapting pre-trained model to specific task. Like specializing general knowledge.
\end{itemize}
\end{frame}

% Statistical Terms
\begin{frame}[t]{Statistical Foundations}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Mean/Average}: Sum divided by count. Central tendency measure.

\item \textbf{Variance}: How spread out data is. High variance = more spread.

\item \textbf{Standard Deviation}: Square root of variance. Average distance from mean.

\item \textbf{Correlation}: Relationship between variables. Height and weight often correlate.

\item \textbf{Distribution}: How data spreads across values. Bell curve is normal distribution.

\item \textbf{Entropy}: Measure of uncertainty/chaos. High entropy = more random.
\end{itemize}
\end{frame}

% Clustering Algorithms
\begin{frame}[t]{Clustering Algorithms}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{K-means}: Divides data into K groups. You specify number of clusters.
  \begin{itemize}
  \item Example: Segment customers into 5 groups
  \end{itemize}

\item \textbf{Hierarchical Clustering}: Builds tree of clusters. Can see relationships at different levels.

\item \textbf{DBSCAN}: Density-based clustering. Finds arbitrary shaped clusters.

\item \textbf{Centroid}: Center point of cluster. Average position of all points.

\item \textbf{Silhouette Score}: Measures how well clustered data is. Higher = better separation.
\end{itemize}
\end{frame}

% Generative AI Terms
\begin{frame}[t]{Generative AI and LLMs}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Generative AI (GenAI)}: AI that creates new content. ChatGPT, DALL-E, etc.

\item \textbf{Large Language Model (LLM)}: AI trained on vast text. GPT, Claude, BERT.

\item \textbf{Prompt}: Input text given to AI. Your question or instruction.

\item \textbf{Token}: Piece of text (word or part). "Tokenization" → ["Token", "ization"]

\item \textbf{Temperature}: Controls AI creativity. Low = predictable, High = creative.

\item \textbf{Context Window}: How much text AI can process. Like working memory.
\end{itemize}
\end{frame}

% Specific Algorithms
\begin{frame}[t]{Specific Algorithms Explained}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{LDA (Latent Dirichlet Allocation)}: Finds hidden topics in documents. Like auto-tagging articles.

\item \textbf{BERT}: Bidirectional Encoder Representations from Transformers. Understands context both ways.

\item \textbf{GPT}: Generative Pre-trained Transformer. Predicts next word repeatedly.

\item \textbf{Word2Vec}: Converts words to vectors. Similar words have similar vectors.

\item \textbf{Random Forest}: Many decision trees voting together. Wisdom of crowds for ML.

\item \textbf{Gradient Descent}: How models learn. Like finding lowest point in valley.
\end{itemize}
\end{frame}

% Evaluation Metrics
\begin{frame}[t]{Model Evaluation Terms}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Accuracy}: Percentage correct predictions. 95\% = 95 out of 100 correct.

\item \textbf{Precision}: Of predicted positives, how many correct? Quality over quantity.

\item \textbf{Recall}: Of actual positives, how many found? Finding all relevant items.

\item \textbf{F1 Score}: Balance of precision and recall. Harmonic mean of both.

\item \textbf{Cross-validation}: Testing model on different data splits. Ensures robust performance.

\item \textbf{Overfitting}: Model memorizes training data. Works great on training, poorly on new data.
\end{itemize}
\end{frame}

% Design Thinking Terms
\begin{frame}[t]{Design Thinking Integration}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Empathy}: Understanding user feelings and needs. Foundation of design thinking.

\item \textbf{Persona}: Fictional user representing a segment. "Sarah, 34, working mother"

\item \textbf{User Journey}: Path user takes to achieve goal. All touchpoints and experiences.

\item \textbf{Pain Point}: Problem causing user frustration. Opportunity for improvement.

\item \textbf{Ideation}: Generating solution ideas. Brainstorming phase.

\item \textbf{Prototype}: Early version to test ideas. Minimum viable solution.
\end{itemize}
\end{frame}

% Data Processing Terms
\begin{frame}[t]{Data Processing and Preparation}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Data Cleaning}: Fixing errors, removing duplicates. Making data usable.

\item \textbf{Normalization}: Scaling values to standard range. 0-100 → 0-1.

\item \textbf{Feature Engineering}: Creating new features from existing. Combining age and income.

\item \textbf{Pipeline}: Sequence of data processing steps. Input → Process → Output.

\item \textbf{Batch Processing}: Processing data in groups. Opposite of real-time.

\item \textbf{API}: Application Programming Interface. How programs talk to each other.
\end{itemize}
\end{frame}

% Advanced Concepts
\begin{frame}[t]{Advanced Concepts}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Hyperparameter}: Setting that controls learning. Like learning rate or tree depth.

\item \textbf{Ensemble Method}: Combining multiple models. Like getting second opinions.

\item \textbf{Transfer Learning}: Using knowledge from one task for another. Like applying skills.

\item \textbf{Reinforcement Learning}: Learning through trial and reward. How games learn to play.

\item \textbf{Federated Learning}: Training on distributed data. Privacy-preserving ML.

\item \textbf{Edge Computing}: Processing near data source. Phone ML vs cloud ML.
\end{itemize}
\end{frame}

% Privacy and Ethics
\begin{frame}[t]{Privacy and Ethics Terms}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Bias}: Systematic unfairness in predictions. Historical biases in data.

\item \textbf{Fairness Metrics}: Measures of equal treatment. Demographic parity, equal opportunity.

\item \textbf{Differential Privacy}: Adding noise to protect individuals. Privacy with utility.

\item \textbf{SHAP}: SHapley Additive exPlanations. Explains model decisions.

\item \textbf{LIME}: Local Interpretable Model-agnostic Explanations. Why did model decide this?

\item \textbf{Explainable AI}: Models that can explain decisions. Transparency in AI.
\end{itemize}
\end{frame}

% Scale and Performance
\begin{frame}[t]{Scale and Performance Concepts}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{Big Data}: Datasets too large for traditional tools. Millions/billions of records.

\item \textbf{Scalability}: Ability to handle growing data. Works for 10 or 10 million.

\item \textbf{Real-time Processing}: Immediate data analysis. Results in milliseconds.

\item \textbf{Latency}: Delay between request and response. Lower is better.

\item \textbf{Throughput}: Amount processed per time. Transactions per second.

\item \textbf{Distributed Computing}: Using multiple computers together. Divide and conquer.
\end{itemize}
\end{frame}

% Mathematical Notation
\begin{frame}[t]{Common Mathematical Notation}
\small
\begin{itemize}
\setlength{\itemsep}{0.3em}
\item \textbf{$\sum$ (Sigma)}: Sum notation. $\sum_{i=1}^{n} x_i$ = $x_1 + x_2 + ... + x_n$

\item \textbf{$\mu$ (Mu)}: Population mean (average)

\item \textbf{$\sigma$ (Sigma)}: Standard deviation

\item \textbf{$p(x)$}: Probability of x occurring

\item \textbf{$\log$}: Logarithm. Inverse of exponential

\item \textbf{$\nabla$ (Nabla)}: Gradient. Direction of steepest increase
\end{itemize}
\end{frame}

% Quick Reference
\begin{frame}[t]{Quick Reference Guide}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Most Important Terms:}
\begin{itemize}
\item ML = Machine Learning
\item AI = Artificial Intelligence
\item NLP = Natural Language Processing
\item LLM = Large Language Model
\item API = Application Programming Interface
\end{itemize}

\column{0.5\textwidth}
\textbf{Key Concepts to Remember:}
\begin{itemize}
\item Clustering = Grouping similar items
\item Classification = Predicting categories
\item Regression = Predicting numbers
\item Training = Teaching the model
\item Inference = Using the model
\end{itemize}
\end{columns}

\vspace{0.5em}
\begin{center}
\textbf{Remember}: Every technical term is just a name for a simple concept!
\end{center}
\end{frame}

\end{document}