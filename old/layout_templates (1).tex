\documentclass[8pt,aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{algorithm2e}
\usepackage{booktabs}

% Theme
\usetheme{Madrid}
\usecolortheme{seahorse}
\setbeamertemplate{navigation symbols}{}

% Colors
\definecolor{myblue}{RGB}{0,102,204}
\definecolor{mygreen}{RGB}{0,153,76}
\definecolor{myred}{RGB}{204,0,0}
\definecolor{lightblue}{RGB}{173,216,230}

% Commands
\newcommand{\highlight}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\pred}[1]{\textcolor{mygreen}{\texttt{#1}}}
\newcommand{\context}[1]{\textcolor{myred}{\texttt{#1}}}

\title{Week 4: Recurrent Neural Networks}
\subtitle{Memory Matters: RNNs for Sequential Prediction}
\author{Next-Word Prediction Course}
\date{}

\begin{document}

% Part 1: Introduction (Slides 1-5)
% Slide 1: Title
\begin{frame}
\titlepage
\end{frame}

% Slide 2: Learning Objectives
\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this week, you will understand:}
\begin{itemize}
\setlength{\itemsep}{0.5em}
\item How RNNs maintain \highlight{memory} across sequences
\item The concept of \highlight{hidden states} as context representation
\item \highlight{Backpropagation through time} (BPTT) for training
\item The \highlight{vanishing gradient} problem and its implications
\item Why RNNs are better than n-grams for \highlight{long-range dependencies}
\end{itemize}

\vspace{0.5em}
\textbf{Key Innovation}: Networks that remember their past to predict the future!
\end{frame}

% Slide 3: The Problem
\begin{frame}[t]{The Problem: Sequential Memory in Language}
\begin{center}
\includegraphics[width=0.9\textwidth]{../figures/week04/opening_problem.pdf}
\end{center}
\end{frame}

% Slide 4: Why This Matters
\begin{frame}[t]{Why RNNs Matter for Language}
\textbf{Limitations of Feedforward Networks:}
\begin{enumerate}
\item \highlight{No Memory}: Each prediction independent
\item \highlight{Fixed Context}: Cannot handle variable-length sequences
\item \highlight{No State}: Cannot track discourse or dialogue
\item \highlight{Redundant Parameters}: Separate weights for each position
\end{enumerate}

\vspace{0.5em}
\textbf{The RNN Solution:}
\begin{itemize}
\item \highlight{Hidden State}: Maintains summary of past
\item \highlight{Parameter Sharing}: Same weights across time
\item \highlight{Dynamic Context}: Adapts to sequence length
\item \highlight{Sequential Processing}: Natural for language
\end{itemize}
\end{frame}

% Slide 5: Real-world Motivation
\begin{frame}[t]{Real-World Impact: Smart Text Prediction}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Example: Email Autocomplete}
\begin{itemize}
\item Context: "Thank you for your..."
\item Feedforward: Only sees fixed window
\item RNN: Remembers entire email thread
\item Better predictions from fuller context
\end{itemize}

\column{0.5\textwidth}
\textbf{Early Applications (1990s-2000s):}
\begin{itemize}
\item Speech recognition systems
\item Handwriting recognition
\item Early machine translation
\item Text-to-speech synthesis
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Key Insight}: Language is inherently sequential - our models should be too!
\end{frame}

% Part 2: Foundation (Slides 6-10)
% Slide 6: Historical Context
\begin{frame}[t]{Historical Context: The Evolution to RNNs}
\textbf{Timeline of Sequential Models:}
\begin{itemize}
\item \textbf{1986}: Jordan Networks - Output fed back as input
\item \textbf{1990}: Elman Networks - Hidden state recurrence
\item \textbf{1997}: LSTM proposed to solve gradient issues
\item \textbf{2000s}: RNNs for language modeling take off
\item \textbf{2010s}: Deep RNNs achieve state-of-the-art
\end{itemize}

\vspace{0.5em}
\textbf{Key Pioneers:}
\begin{itemize}
\item Jeffrey Elman: Simple recurrent networks
\item Michael Jordan: Alternative recurrent architecture
\item Yoshua Bengio: Gradient flow analysis
\item Jürgen Schmidhuber: Long short-term memory
\end{itemize}
\end{frame}

% Slide 7: Evolution from Feedforward
\begin{frame}[t]{Evolution: From Feedforward to Recurrent}
\begin{columns}
\column{0.5\textwidth}
\textbf{Feedforward Limitations:}
\begin{itemize}
\item Fixed input size
\item No temporal dynamics
\item Position-dependent weights
\item Cannot model sequences
\end{itemize}

\column{0.5\textwidth}
\textbf{Recurrent Innovations:}
\begin{itemize}
\item Variable sequence length
\item Hidden state evolution
\item Weight sharing across time
\item Natural sequence modeling
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Architectural Evolution:}
\begin{enumerate}
\item Feedforward: $y = f(Wx + b)$
\item Time-delay: $y_t = f(Wx_t + Ux_{t-1} + b)$
\item Recurrent: $h_t = f(Wx_t + Uh_{t-1} + b)$
\end{enumerate}
\end{frame}

% Slide 8: Limitations of Previous Approaches
\begin{frame}[t]{Limitations: Why We Need Recurrence}
\textbf{N-gram Models:}
\begin{itemize}
\item Fixed context window
\item Exponential parameter growth
\item No parameter sharing
\item Cannot generalize patterns
\end{itemize}

\textbf{Feedforward Neural LMs:}
\begin{itemize}
\item Still fixed context size
\item No state between predictions
\item Cannot handle variable length
\item Positional parameters wasteful
\end{itemize}

\vspace{0.5em}
\textbf{What We Need:}
\begin{itemize}
\item \highlight{Unbounded context} in principle
\item \highlight{Parameter efficiency} through sharing
\item \highlight{State maintenance} across predictions
\item \highlight{Sequential inductive bias}
\end{itemize}
\end{frame}

% Slide 9: Basic Concept 1 - Hidden State
\begin{frame}[t]{Core Concept: Hidden State as Memory}
\textbf{The Hidden State $h_t$:}
\begin{itemize}
\item Summarizes history up to time $t$
\item Updated at each time step
\item Passed to next time step
\item Used for prediction
\end{itemize}

\vspace{0.5em}
\textbf{Conceptual View:}
\begin{center}
\begin{tabular}{lll}
Time & Input & Hidden State Contains \\
\hline
$t=1$ & "The" & \{start-of-sentence\} \\
$t=2$ & "cat" & \{definite article + subject\} \\
$t=3$ & "sat" & \{subject + past action\} \\
$t=4$ & "on" & \{complete subject-verb phrase\} \\
\end{tabular}
\end{center}

\textbf{Key Property}: $h_t$ is a \highlight{learned representation} of relevant history
\end{frame}

% Slide 10: Basic Concept 2 - Parameter Sharing
\begin{frame}[t]{Core Concept: Parameter Sharing Across Time}
\textbf{Same Weights Everywhere:}
\begin{itemize}
\item $W_{xh}$: Input-to-hidden (same at all times)
\item $W_{hh}$: Hidden-to-hidden (same at all times)
\item $W_{hy}$: Hidden-to-output (same at all times)
\end{itemize}

\vspace{0.5em}
\textbf{Benefits:}
\begin{enumerate}
\item \highlight{Generalization}: Pattern learned at position 5 works at position 50
\item \highlight{Efficiency}: $O(H^2)$ parameters, not $O(TH^2)$
\item \highlight{Inductive Bias}: Assumes time-invariant dynamics
\end{enumerate}

\textbf{Example}: Learning "not" negation
\begin{itemize}
\item Sees: "I do \highlight{not} like..."
\item Learns: "not" $\rightarrow$ negation
\item Generalizes: "They will \highlight{not} come..." (different position)
\end{itemize}
\end{frame}

% Part 3: Core Theory (Slides 11-15)
% Slide 11: Mathematical Foundation 1
\begin{frame}[t]{Mathematics: RNN Forward Pass}
\textbf{Core RNN Equations:}

\begin{align}
h_t &= \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\
y_t &= W_{hy} h_t + b_y \\
\hat{p}_t &= \text{softmax}(y_t)
\end{align}

\textbf{Where:}
\begin{itemize}
\item $x_t \in \mathbb{R}^{|V|}$: One-hot input at time $t$
\item $h_t \in \mathbb{R}^H$: Hidden state (typically $H = 128-512$)
\item $y_t \in \mathbb{R}^{|V|}$: Output scores
\item $\hat{p}_t$: Probability distribution over vocabulary
\end{itemize}

\textbf{Initial State}: $h_0 = \vec{0}$ or learned parameter
\end{frame}

% Slide 12: Mathematical Foundation 2
\begin{frame}[t]{Mathematics: Hidden State Update}
\textbf{Hidden State Evolution:}
$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$

\textbf{Unrolling the Recursion:}
\begin{align}
h_1 &= \tanh(W_{hh} h_0 + W_{xh} x_1 + b_h) \\
h_2 &= \tanh(W_{hh} h_1 + W_{xh} x_2 + b_h) \\
h_3 &= \tanh(W_{hh} h_2 + W_{xh} x_3 + b_h)
\end{align}

\textbf{Key Insight}: $h_t$ depends on \highlight{entire history} $\{x_1, ..., x_t\}$

\textbf{Information Flow:}
\begin{itemize}
\item Previous state: $W_{hh} h_{t-1}$ (memory)
\item Current input: $W_{xh} x_t$ (new information)
\item Nonlinearity: $\tanh$ (enables complex functions)
\end{itemize}
\end{frame}

% Slide 13: Mathematical Foundation 3
\begin{frame}[t]{Mathematics: Backpropagation Through Time}
\textbf{Loss Function:}
$$L = -\sum_{t=1}^T \log p(w_t | w_{<t}) = -\sum_{t=1}^T \log \hat{p}_t[w_t]$$

\textbf{BPTT Algorithm:}
\begin{enumerate}
\item Forward pass: Compute all $h_t$ and $\hat{p}_t$
\item Compute loss at each time step
\item Backward pass: Accumulate gradients backwards
\item Update weights using total gradient
\end{enumerate}

\textbf{Gradient Flow:}
$$\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial L_t}{\partial W_{hh}}$$

\textbf{Challenge}: Gradients pass through many $\tanh$ layers!
\end{frame}

% Slide 14: Formula Visualization
\begin{frame}[t]{Visualizing RNN Computation}
\begin{center}
\includegraphics[width=0.9\textwidth]{../figures/week04/rnn_unrolling.pdf}
\end{center}
\end{frame}

% Slide 15: Intuitive Explanation
\begin{frame}[t]{Intuition: Why Gradients Vanish}
\textbf{The Vanishing Gradient Problem:}

During backpropagation:
$$\frac{\partial h_t}{\partial h_{t-k}} = \prod_{i=1}^k W_{hh}^T \cdot \text{diag}(\tanh'(h_{t-i+1}))$$

\textbf{Why Gradients Vanish:}
\begin{enumerate}
\item $\tanh'(x) \in [0, 1]$ (derivative bounded)
\item Multiple by values $< 1$ repeatedly
\item Exponential decay: $(0.9)^{10} = 0.35$, $(0.9)^{100} = 0.000027$
\item Long-term dependencies get no gradient!
\end{enumerate}

\textbf{Consequences:}
\begin{itemize}
\item RNN "forgets" after 5-10 steps
\item Cannot learn long-range patterns
\item Biased toward recent context
\end{itemize}
\end{frame}

% Part 4: Implementation (Slides 16-20)
% Slide 16: How It Works - Part 1
\begin{frame}[t]{Implementation: Unrolling the Network}
\textbf{From Recursion to Computation Graph:}

\begin{center}
\includegraphics[width=0.8\textwidth]{../figures/week04/rnn_unrolling.pdf}
\end{center}

\textbf{Implementation Strategy:}
\begin{enumerate}
\item Create copies of RNN cell for each time step
\item Share parameters across all copies
\item Connect hidden states sequentially
\item Compute forward pass left-to-right
\end{enumerate}
\end{frame}

% Slide 17: How It Works - Part 2
\begin{frame}[t]{Implementation: Training with BPTT}
\textbf{Truncated BPTT (Practical Approach):}
\begin{enumerate}
\item Process sequence in chunks (e.g., 35 tokens)
\item Carry hidden state forward
\item Only backpropagate within chunk
\item Approximates full BPTT efficiently
\end{enumerate}

\textbf{Pseudocode:}
\begin{algorithm}[H]
\SetAlgoLined
\KwIn{Sequence $X = (x_1, ..., x_T)$, chunk\_size $K$}
$h_0 \leftarrow \text{initialize}()$\;
\For{$i = 0$ to $T/K - 1$}{
    chunk $\leftarrow X[i \cdot K : (i+1) \cdot K]$\;
    $h_{i \cdot K}, losses \leftarrow \text{forward}(chunk, h_{i \cdot K})$\;
    gradients $\leftarrow \text{backward}(losses)$\;
    \text{update\_weights}(gradients)\;
    $h_{(i+1) \cdot K} \leftarrow h_{i \cdot K}.\text{detach}()$\;
}
\end{algorithm}
\end{frame}

% Slide 18: How It Works - Part 3
\begin{frame}[t]{Implementation: Handling Variable Lengths}
\textbf{Challenge}: Sequences have different lengths

\textbf{Solutions:}
\begin{enumerate}
\item \textbf{Padding}: Add special <PAD> tokens
\item \textbf{Masking}: Ignore padded positions in loss
\item \textbf{Packing}: Process efficiently in batches
\item \textbf{Dynamic Batching}: Group similar lengths
\end{enumerate}

\textbf{Example Batch:}
\begin{itemize}
\item ``The cat sat''
\item ``A very long sentence with many words''
\item ``Short''
\end{itemize}

\textbf{After Padding (max\_len=7):}
\begin{itemize}
\item ``The cat sat \texttt{<PAD> <PAD> <PAD> <PAD>}''
\item ``A very long sentence with many words''
\item ``Short \texttt{<PAD> <PAD> <PAD> <PAD> <PAD> <PAD>}''
\end{itemize}
\end{frame}

% Slide 19: Architecture Visualization
\begin{frame}[t]{Complete RNN Architecture}
\begin{center}
\includegraphics[width=0.9\textwidth]{../figures/week04/architecture.pdf}
\end{center}
\end{frame}

% Slide 20: Complexity Analysis
\begin{frame}[t]{Complexity Analysis}
\textbf{Time Complexity:}
\begin{itemize}
\item Forward pass: $O(T \cdot H^2)$ where $T$ = sequence length, $H$ = hidden size
\item Backward pass: $O(T \cdot H^2)$ (same as forward)
\item Total per sequence: $O(T \cdot H^2)$
\end{itemize}

\textbf{Space Complexity:}
\begin{itemize}
\item Parameters: $O(H^2 + H \cdot |V|)$
\item Activations: $O(T \cdot H)$ (must store all hidden states)
\item Gradients: $O(H^2 + H \cdot |V|)$
\end{itemize}

\textbf{Comparison:}
\begin{center}
\begin{tabular}{lcc}
\toprule
Model & Parameters & Computation \\
\midrule
N-gram & $O(|V|^n)$ & $O(1)$ \\
Feedforward & $O(n \cdot H + H \cdot |V|)$ & $O(H^2)$ \\
RNN & $O(H^2 + H \cdot |V|)$ & $O(T \cdot H^2)$ \\
\bottomrule
\end{tabular}
\end{center}
\end{frame}

% Part 5: Applications (Slides 21-25)
% Slide 21: Real-World Use 1
\begin{frame}[t]{Application: Text Generation}
\textbf{Character-Level RNN Example:}
\begin{itemize}
\item Train on Shakespeare
\item Generate character-by-character
\item Learns spelling, words, grammar, style!
\end{itemize}

\textbf{Sample Output:}
\begin{quote}
\small
"KING LEAR: \\
Thou hast been a knave's mind in the world, \\
And therefore I have seen the day of the death \\
That thou hast speak to me."
\end{quote}

\textbf{What RNN Learned:}
\begin{itemize}
\item Character sequences forming words
\item Word boundaries and punctuation
\item Grammar patterns
\item Shakespeare's style
\end{itemize}
\end{frame}

% Slide 22: Real-World Use 2
\begin{frame}[t]{Application: Sentiment Analysis}
\textbf{Sequential Sentiment Modeling:}
\begin{itemize}
\item Input: "The movie started great but became boring"
\item RNN tracks sentiment evolution
\item Final hidden state $\rightarrow$ classification
\end{itemize}

\textbf{Hidden State Evolution:}
\begin{center}
\includegraphics[width=0.8\textwidth]{../figures/week04/hidden_evolution.pdf}
\end{center}

\textbf{Advantages over Bag-of-Words:}
\begin{itemize}
\item Captures "but" reversals
\item Understands negation context
\item Models opinion progression
\end{itemize}
\end{frame}

% Slide 23: Real-World Use 3
\begin{frame}[t]{Application: Named Entity Recognition}
\textbf{Sequential Labeling with RNNs:}

Input: "Apple Inc. was founded by Steve Jobs"

\begin{center}
\begin{tabular}{lccccccc}
Word: & Apple & Inc. & was & founded & by & Steve & Jobs \\
Label: & B-ORG & I-ORG & O & O & O & B-PER & I-PER \\
\end{tabular}
\end{center}

\textbf{Why RNNs Excel:}
\begin{itemize}
\item Context determines entity type
\item "Apple" could be fruit or company
\item RNN uses surrounding words
\item Maintains entity boundaries
\end{itemize}

\textbf{BiRNN Enhancement:}
\begin{itemize}
\item Forward RNN: left context
\item Backward RNN: right context
\item Combine for full context
\end{itemize}
\end{frame}

% Slide 24: Case Study
\begin{frame}[t]{Case Study: Early Dialogue Systems}
\textbf{RNN-based Chatbots (circa 2015):}

\begin{columns}
\column{0.5\textwidth}
\textbf{Architecture:}
\begin{itemize}
\item Encoder RNN: Process input
\item Decoder RNN: Generate response
\item Hidden state bridges them
\item Trained on conversation pairs
\end{itemize}

\column{0.5\textwidth}
\textbf{Example Dialogue:}
\begin{itemize}
\item Human: "How are you?"
\item Bot: "I'm doing well, thanks!"
\item Human: "What's the weather?"
\item Bot: "I don't have access to that."
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Limitations Discovered:}
\begin{itemize}
\item Poor long conversation memory
\item Generic responses
\item No true understanding
\item Gradient vanishing limits context
\end{itemize}
\end{frame}

% Slide 25: Performance Metrics
\begin{frame}[t]{Performance: RNN vs Previous Methods}
\begin{center}
\includegraphics[width=0.9\textwidth]{../figures/week04/performance_comparison.pdf}
\end{center}
\end{frame}

% Part 6: Summary & Next Steps (Slides 26-30)
% Slide 26: Key Takeaways
\begin{frame}[t]{Key Takeaways}
\begin{enumerate}
\item \textbf{Sequential Memory}
   \begin{itemize}
   \item RNNs maintain hidden state across time
   \item Theoretically unlimited context
   \end{itemize}
   
\item \textbf{Parameter Sharing}
   \begin{itemize}
   \item Same weights used at all time steps
   \item Enables generalization across positions
   \end{itemize}
   
\item \textbf{Gradient Challenges}
   \begin{itemize}
   \item Vanishing gradients limit effective context
   \item Typically 5-10 words in practice
   \end{itemize}
   
\item \textbf{Natural for Language}
   \begin{itemize}
   \item Processes text left-to-right
   \item Hidden state summarizes context
   \end{itemize}
\end{enumerate}
\end{frame}

% Slide 27: Comparison
\begin{frame}[t]{Model Comparison: Evolution of Context}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Context} & \textbf{Parameters} & \textbf{Gradient Flow} \\
\midrule
N-gram & Fixed $(n-1)$ & Exponential in $n$ & N/A \\
Feedforward & Fixed window & Linear in window & Direct \\
RNN & Unlimited* & Constant & Through time \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.5em}
\textbf{*In Practice:}
\begin{itemize}
\item Theoretical: Unlimited context
\item Practical: 5-10 words due to gradient vanishing
\item Still better than fixed window
\item Motivated next innovation: LSTM/GRU
\end{itemize}

\textbf{Key Trade-off}: Memory vs Gradient Flow
\end{frame}

% Slide 28: Summary Visualization
\begin{frame}[t]{Visual Summary: RNN Capabilities}
\begin{center}
\includegraphics[width=0.9\textwidth]{../figures/week04/closing_summary.pdf}
\end{center}
\end{frame}

% Slide 29: Further Reading
\begin{frame}[t]{Further Reading}
\textbf{Foundational Papers:}
\begin{itemize}
\item Elman (1990): "Finding Structure in Time"
\item Bengio et al. (1994): "Learning Long-term Dependencies with Gradient Descent is Difficult"
\item Mikolov et al. (2010): "Recurrent Neural Network based Language Model"
\end{itemize}

\textbf{Practical Resources:}
\begin{itemize}
\item Karpathy (2015): "The Unreasonable Effectiveness of RNNs"
\item Olah (2015): "Understanding LSTM Networks" (blog)
\item PyTorch/TensorFlow RNN tutorials
\end{itemize}

\textbf{Key Insight}: RNNs opened the door, but gradients held them back
\end{frame}

% Slide 30: Next Week Preview
\begin{frame}[t]{Next Week: Solving the Memory Problem}
\textbf{LSTM/GRU - Engineering Better Memory:}
\begin{itemize}
\item How do we maintain gradients over 100+ steps?
\item What are "gates" and how do they help?
\item Can we learn what to remember and forget?
\end{itemize}

\vspace{0.5em}
\textbf{Preview:}
\begin{itemize}
\item LSTM: 4 gates to control information flow
\item GRU: Simplified but effective variant
\item Gradient highways for long-range learning
\item State-of-the-art until attention mechanisms
\end{itemize}

\textbf{The Journey}: N-grams → Neural → RNN → LSTM → Transformers
\end{frame}

\end{document}