\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{xcolor}

% Define custom colors
\definecolor{mlblue}{RGB}{52, 152, 219}
\definecolor{mlorange}{RGB}{230, 126, 34}
\definecolor{mlgreen}{RGB}{46, 204, 113}
\definecolor{mlred}{RGB}{231, 76, 60}
\definecolor{mlpurple}{RGB}{155, 89, 182}
\definecolor{mlyellow}{RGB}{241, 196, 15}

% Python code styling
\lstset{
    language=Python,
    basicstyle=\tiny\ttfamily,
    keywordstyle=\color{mlblue},
    stringstyle=\color{mlgreen},
    commentstyle=\color{gray},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    frame=single,
    rulecolor=\color{gray!30},
    breaklines=true
}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title information
\title{Week 1: Foundation + Clustering}
\subtitle{ML/AI Design Thinking: Empathize Phase}
\author{BSc Data Science \& Design}
\date{2025}

\begin{document}

% Title Slide
\begin{frame}
\titlepage
\end{frame}

% Slide 1: Opening Power Chart - Convergence Flow
\begin{frame}[plain]
\begin{center}
\vspace{-0.5cm}
{\Large \textbf{The Convergence Flow}}\\[0.3cm]
{\small How Random Data Points Become Meaningful Groups}\\[0.5cm]
\includegraphics[width=0.85\textwidth]{ML_Design_Course/week01_visuals/convergence_flow.pdf}\\[0.3cm]
{\footnotesize \textit{Watch as 1000 users organize themselves into natural groups}}
\end{center}
\end{frame}

% PART I: FOUNDATION (8 slides)
% Introduction & Foundations (3 slides)

% Slide 2: Welcome
\begin{frame}{Welcome to ML + Design Thinking}
\begin{columns}
\column{0.5\textwidth}
\textbf{Traditional Design Thinking}
\begin{itemize}
    \item Empathize with users
    \item Define problems
    \item Ideate solutions
    \item Prototype ideas
    \item Test and iterate
\end{itemize}

\column{0.5\textwidth}
\textbf{+ Machine Learning Power}
\begin{itemize}
    \item Analyze thousands of users
    \item Find hidden patterns
    \item Generate insights automatically
    \item Validate with data
    \item Scale your understanding
\end{itemize}
\end{columns}
\vspace{0.5cm}
\begin{center}
\colorbox{mlblue!20}{\textbf{This Week: Using clustering to truly understand your users}}
\end{center}
\end{frame}

% Slide 3: The Empathize Phase
\begin{frame}{The Empathize Phase: Understanding Your Users}
\begin{columns}
\column{0.6\textwidth}
\textbf{What is Empathizing?}
\begin{itemize}
    \item Walking in your users' shoes
    \item Understanding their needs, wants, fears
    \item Discovering what they don't tell you
    \item Finding patterns in behavior
\end{itemize}

\textbf{Traditional Methods:}
\begin{itemize}
    \item Interviews (5-20 people)
    \item Observations (days/weeks)
    \item Surveys (100s of responses)
\end{itemize}

\column{0.4\textwidth}
\textbf{ML-Enhanced Methods:}
\begin{itemize}
    \item Analyze millions of interactions
    \item Find patterns humans miss
    \item Work 24/7 automatically
    \item Unbiased grouping
\end{itemize}

\vspace{0.3cm}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/user_empathy_visual.pdf}
\end{columns}
\end{frame}

% Slide 4: Why Clustering Helps
\begin{frame}{Why Clustering Helps Us Understand People}
\begin{center}
\textbf{From Chaos to Clarity}
\end{center}

\begin{columns}
\column{0.33\textwidth}
\centering
\textbf{Before Clustering}\\[0.2cm]
\includegraphics[width=0.9\textwidth]{ML_Design_Course/week01_visuals/chaos_to_clarity.pdf}\\
{\footnotesize 10,000 users = overwhelming}

\column{0.33\textwidth}
\centering
\textbf{Clustering Process}\\[0.2cm]
\includegraphics[width=0.9\textwidth]{ML_Design_Course/week01_visuals/kmeans_animation.pdf}\\
{\footnotesize ML finds natural groups}

\column{0.33\textwidth}
\centering
\textbf{After Clustering}\\[0.2cm]
\includegraphics[width=0.9\textwidth]{ML_Design_Course/week01_visuals/customer_segments.pdf}\\
{\footnotesize 5 clear user types!}
\end{columns}

\vspace{0.5cm}
\begin{itemize}
    \item \textbf{Power Users}: Heavy usage, all features
    \item \textbf{Casuals}: Weekend usage, basic features
    \item \textbf{Professionals}: Business hours, productivity focus
    \item \textbf{Students}: Evening usage, collaboration features
    \item \textbf{Explorers}: Try everything once
\end{itemize}
\end{frame}

% Theory & Concepts (4 slides)

% Slide 5: What is Clustering?
\begin{frame}{What is Clustering?}
\begin{center}
\textbf{Clustering = Finding Natural Groups in Data}
\end{center}

\begin{columns}
\column{0.5\textwidth}
\textbf{Real-World Examples:}
\begin{itemize}
    \item Spotify: Grouping similar listeners
    \item Netflix: Finding viewer types
    \item Amazon: Customer segments
    \item Instagram: Content categories
    \item Gmail: Organizing emails
\end{itemize}

\column{0.5\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/clustering_examples.pdf}
\end{columns}

\vspace{0.3cm}
\textbf{Key Idea:} Items in the same group are more similar to each other than to items in other groups

\vspace{0.2cm}
\colorbox{mlgreen!20}{No labels needed - the algorithm finds groups automatically!}
\end{frame}

% Slide 6: How Do We Measure "Similar"?
\begin{frame}{How Do We Measure ``Similar''?}
\begin{center}
\textbf{Distance = Difference Between Things}
\end{center}

\begin{columns}
\column{0.6\textwidth}
\textbf{Simple Example: App Usage}
\begin{itemize}
    \item User A: 2 hours/day, 10 features used
    \item User B: 2.5 hours/day, 12 features used  
    \item User C: 8 hours/day, 50 features used
\end{itemize}

\vspace{0.3cm}
Who is more similar?
\begin{itemize}
    \item A and B are close (similar usage)
    \item C is far from both (power user)
\end{itemize}

\column{0.4\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/distance_visual.pdf}

\vspace{0.2cm}
{\footnotesize 
\textbf{Common Measures:}
\begin{itemize}
    \item Straight line (Euclidean)
    \item City blocks (Manhattan)
    \item Correlation-based
\end{itemize}
}
\end{columns}

\vspace{0.3cm}
\colorbox{mlyellow!20}{Think of it like: ``How different are these users?''}
\end{frame}

% Slide 7: K-means - Finding Group Centers
\begin{frame}{K-means: Finding Group Centers}
\begin{center}
\textbf{Like Finding the Best Meeting Points for Groups}
\end{center}

\begin{columns}
\column{0.5\textwidth}
\textbf{How K-means Works:}
\begin{enumerate}
    \item Pick K center points randomly
    \item Assign each user to nearest center
    \item Move centers to group middle
    \item Repeat until stable
\end{enumerate}

\vspace{0.3cm}
\textbf{Real Example:}\\
Finding 3 types of coffee drinkers:
\begin{itemize}
    \item Morning rushers
    \item Afternoon socializers  
    \item All-day workers
\end{itemize}

\column{0.5\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/kmeans_animation.pdf}

\vspace{0.2cm}
{\footnotesize
\textbf{Pros:} Fast, simple, scalable\\
\textbf{Cons:} Need to know K, assumes round clusters
}
\end{columns}
\end{frame}

% Slide 8: Hierarchical - Building a Family Tree
\begin{frame}{Hierarchical: Building a Family Tree}
\begin{columns}
\column{0.45\textwidth}
\textbf{Bottom-Up Approach:}
\begin{enumerate}
    \item Start: Everyone is separate
    \item Find two most similar users
    \item Group them together
    \item Repeat with groups
    \item Stop when all connected
\end{enumerate}

\vspace{0.3cm}
\textbf{Like Making Friends:}
\begin{itemize}
    \item Best friends first
    \item Then friend groups
    \item Then communities
    \item Finally, everyone connected
\end{itemize}

\column{0.55\textwidth}
\centering
\textbf{Dendrogram: The Family Tree}\\[0.3cm]
\includegraphics[width=0.95\textwidth]{ML_Design_Course/week01_visuals/dendrogram_example.pdf}

{\footnotesize Cut the tree at any height to get different numbers of groups!}
\end{columns}
\end{frame}

% Slide 9: How Do We Know Our Groups Are Good?
\begin{frame}{How Do We Know Our Groups Are Good?}
\begin{center}
\textbf{Three Simple Checks}
\end{center}

\begin{columns}
\column{0.33\textwidth}
\centering
\textbf{1. Tight Groups}\\[0.2cm]
\includegraphics[width=0.8\textwidth]{ML_Design_Course/week01_visuals/cluster_quality.pdf}\\
{\footnotesize Users in same group should be close together}

\column{0.33\textwidth}
\centering
\textbf{2. Separated Groups}\\[0.2cm]
\includegraphics[width=0.8\textwidth]{ML_Design_Course/week01_visuals/cluster_quality.pdf}\\
{\footnotesize Different groups should be far apart}

\column{0.33\textwidth}
\centering
\textbf{3. Makes Sense}\\[0.2cm]
\includegraphics[width=0.8\textwidth]{ML_Design_Course/week01_visuals/cluster_quality.pdf}\\
{\footnotesize Groups should mean something real}
\end{columns}

\vspace{0.5cm}
\textbf{Simple Metrics:}
\begin{itemize}
    \item \textbf{Elbow Method}: Plot error vs. number of clusters, look for ``elbow''
    \item \textbf{Silhouette Score}: -1 (bad) to +1 (perfect), aim for > 0.5
    \item \textbf{Business Sense}: Can you name and use each group?
\end{itemize}
\end{frame}

% PART II: TECHNICAL IMPLEMENTATION (10 slides)

% Slide 10: Preparing Your Data
\begin{frame}[fragile]{Preparing Your Data}
\textbf{Getting Data Ready for Clustering}

\begin{lstlisting}
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load your user data
users = pd.read_csv('user_behavior.csv')

# Select features for clustering
features = ['daily_usage_hours', 'features_used', 
            'days_active', 'messages_sent']

# Handle missing values
users[features] = users[features].fillna(users[features].mean())

# Normalize: Make all features same scale (0-1)
scaler = StandardScaler()
users_normalized = scaler.fit_transform(users[features])

print("Before:", users[features].iloc[0].values)
# [8.5, 45, 28, 156]
print("After:", users_normalized[0])  
# [1.2, 0.8, 1.1, 0.9] - all similar scale!
\end{lstlisting}

\colorbox{mlred!20}{\textbf{Why normalize?} So ``hours used'' doesn't dominate ``features used''}
\end{frame}

% Slide 11: Your First K-means in Python
\begin{frame}[fragile]{Your First K-means in Python}
\textbf{Just 5 Lines to Find User Groups!}

\begin{lstlisting}
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Create and fit K-means (let's find 4 groups)
kmeans = KMeans(n_clusters=4, random_state=42)
users['cluster'] = kmeans.fit_predict(users_normalized)

# See the groups
print(users.groupby('cluster')[features].mean())
\end{lstlisting}

\begin{columns}
\column{0.5\textwidth}
\begin{verbatim}
Cluster 0: Power Users
  - 8.2 hours/day
  - 52 features used
  
Cluster 1: Casual Users  
  - 1.5 hours/day
  - 8 features used
\end{verbatim}

\column{0.5\textwidth}
\begin{verbatim}
Cluster 2: Regular Users
  - 4.1 hours/day
  - 25 features used
  
Cluster 3: New Users
  - 0.8 hours/day
  - 3 features used
\end{verbatim}
\end{columns}

\vspace{0.3cm}
\colorbox{mlgreen!20}{That's it! You've segmented thousands of users in seconds}
\end{frame}

% Slide 12: How Many Groups? The Elbow Method
\begin{frame}[fragile]{How Many Groups? The Elbow Method}
\textbf{Finding the ``Just Right'' Number of Clusters}

\begin{columns}
\column{0.55\textwidth}
\begin{lstlisting}
# Try different numbers of clusters
inertias = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(users_normalized)
    inertias.append(kmeans.inertia_)

# Plot the elbow curve
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('Number of Clusters')
plt.ylabel('Total Distance')
plt.title('The Elbow Method')
plt.show()
\end{lstlisting}

Look for the ``elbow'' - where adding more clusters doesn't help much

\column{0.45\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/elbow_method.pdf}

\vspace{0.2cm}
{\footnotesize
\textbf{In this example:}
\begin{itemize}
    \item 2-3 clusters: Big improvement
    \item 4-5 clusters: Good improvement  
    \item 6+ clusters: Diminishing returns
    \item \textbf{Choose: 4 or 5 clusters}
\end{itemize}
}
\end{columns}
\end{frame}

% Slide 13: Creating Dendrograms
\begin{frame}[fragile]{Creating Dendrograms (Tree Diagrams)}
\textbf{Visualizing How Users Group Together}

\begin{columns}
\column{0.6\textwidth}
\begin{lstlisting}
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Create the linkage matrix
linkage_matrix = linkage(users_normalized, 
                        method='ward')

# Plot dendrogram
plt.figure(figsize=(10, 6))
dendrogram(linkage_matrix,
          labels=users['user_id'].values,
          leaf_rotation=90)
plt.title('User Clustering Dendrogram')
plt.xlabel('User ID')
plt.ylabel('Distance')
plt.show()

# Cut tree to get 4 clusters
from scipy.cluster.hierarchy import fcluster
users['h_cluster'] = fcluster(linkage_matrix, 
                              t=4, 
                              criterion='maxclust')
\end{lstlisting}

\column{0.4\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/dendrogram_cut.pdf}

{\footnotesize
\textbf{Reading the Tree:}
\begin{itemize}
    \item Bottom: Individual users
    \item Height: How different groups are
    \item Branches: Groups forming
    \item Cut line: Your chosen clusters
\end{itemize}
}
\end{columns}
\end{frame}

% Slide 14: Finding Dense Areas with DBSCAN
\begin{frame}[fragile]{Finding Dense Areas with DBSCAN}
\textbf{When Your Groups Aren't Round}

\begin{columns}
\column{0.55\textwidth}
\begin{lstlisting}
from sklearn.cluster import DBSCAN

# DBSCAN: Finds dense regions
dbscan = DBSCAN(eps=0.5,      # neighborhood size
                min_samples=5)  # min points
users['db_cluster'] = dbscan.fit_predict(
    users_normalized)

# Check results
print(f"Found {len(set(users['db_cluster']))-1} clusters")
print(f"Outliers: {sum(users['db_cluster']==-1)}")

# Visualize
colors = ['red','blue','green','yellow','purple']
for i in range(max(users['db_cluster'])+1):
    if i == -1:  # Outliers
        plt.scatter(X[users['db_cluster']==i, 0],
                   X[users['db_cluster']==i, 1],
                   c='gray', marker='x', alpha=0.3)
    else:
        plt.scatter(X[users['db_cluster']==i, 0],
                   X[users['db_cluster']==i, 1],
                   c=colors[i], alpha=0.6)
\end{lstlisting}

\column{0.45\textwidth}
\centering
\textbf{DBSCAN Advantages:}\\[0.2cm]
\includegraphics[width=0.9\textwidth]{ML_Design_Course/week01_visuals/dbscan_shapes.pdf}

{\footnotesize
\begin{itemize}
    \item Finds any shape clusters
    \item Identifies outliers (noise)
    \item No need to specify K
    \item Great for unusual patterns
\end{itemize}
}
\end{columns}
\end{frame}

% Slide 15: Choosing the Right Features
\begin{frame}{Choosing the Right Features}
\textbf{What to Measure for Good Clustering}

\begin{columns}
\column{0.5\textwidth}
\textbf{Good Features for User Clustering:}
\begin{itemize}
    \item \textcolor{mlgreen}{Usage frequency}
    \item \textcolor{mlgreen}{Feature adoption}
    \item \textcolor{mlgreen}{Time patterns}
    \item \textcolor{mlgreen}{Interaction types}
    \item \textcolor{mlgreen}{Content preferences}
\end{itemize}

\textbf{Avoid These:}
\begin{itemize}
    \item \textcolor{mlred}{User ID (unique)}
    \item \textcolor{mlred}{Registration date (if not relevant)}
    \item \textcolor{mlred}{Random identifiers}
    \item \textcolor{mlred}{Highly correlated features}
\end{itemize}

\column{0.5\textwidth}
\textbf{Feature Engineering Example:}

\colorbox{gray!20}{
\begin{tabular}{ll}
\textbf{Raw Data} & \textbf{Engineered Feature} \\
\hline
Login times & Morning/Evening user \\
Click events & Clicks per session \\
Page views & Depth of exploration \\
Purchase history & Spending tier \\
Support tickets & Frustration level \\
\end{tabular}
}

\vspace{0.3cm}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/feature_importance.pdf}
\end{columns}

\vspace{0.3cm}
\colorbox{mlblue!20}{\textbf{Pro Tip:} Start with 3-5 strong features, add more if needed}
\end{frame}

% Slide 16: Making Clusters Visible (2D Plots)
\begin{frame}[fragile]{Making Clusters Visible (2D Plots)}
\textbf{Reducing Dimensions to See Patterns}

\begin{columns}
\column{0.55\textwidth}
\begin{lstlisting}
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reduce to 2D for visualization
pca = PCA(n_components=2)
users_2d = pca.fit_transform(users_normalized)

# Plot with cluster colors
plt.figure(figsize=(10, 8))
colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']
for i in range(4):
    cluster_data = users_2d[users['cluster'] == i]
    plt.scatter(cluster_data[:, 0], 
               cluster_data[:, 1],
               c=colors[i], 
               label=f'Cluster {i}',
               alpha=0.6, s=50)

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('User Clusters Visualization')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
\end{lstlisting}

\column{0.45\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/pca_clusters.pdf}

{\footnotesize
\textbf{What PCA Does:}
\begin{itemize}
    \item Combines all features into 2D
    \item Preserves most important differences
    \item Makes patterns visible
    \item Great for presentations!
\end{itemize}
}
\end{columns}
\end{frame}

% Slide 17: Working with Categories
\begin{frame}[fragile]{Working with Categories}
\textbf{Clustering with Non-Numeric Data}

\begin{columns}
\column{0.6\textwidth}
\begin{lstlisting}
# Convert categories to numbers
from sklearn.preprocessing import LabelEncoder

# Example: Device type
le = LabelEncoder()
users['device_encoded'] = le.fit_transform(
    users['device_type'])
# 'mobile' -> 0, 'desktop' -> 1, 'tablet' -> 2

# Better: One-hot encoding for clustering
device_dummies = pd.get_dummies(
    users['device_type'], 
    prefix='device')
# Creates: device_mobile, device_desktop, device_tablet

# Combine with numeric features
features_all = pd.concat([
    users[numeric_features],
    device_dummies,
    pd.get_dummies(users['subscription_type'])
], axis=1)

# Now cluster as normal
kmeans = KMeans(n_clusters=4)
users['cluster'] = kmeans.fit_predict(features_all)
\end{lstlisting}

\column{0.4\textwidth}
\textbf{Category Examples:}
\begin{itemize}
    \item Device type
    \item Subscription level
    \item Country/Region
    \item Product categories
    \item User role
\end{itemize}

\vspace{0.3cm}
\colorbox{mlyellow!20}{
\begin{minipage}{0.9\textwidth}
\textbf{Rule:} Use one-hot encoding for clustering, not label encoding
\end{minipage}
}
\end{columns}
\end{frame}

% Slide 18: Real Example - Customer Segmentation
\begin{frame}[fragile]{Real Example: E-commerce Customer Segmentation}
\textbf{Finding Customer Types in Online Shopping Data}

\begin{lstlisting}
# Real e-commerce features
customer_features = [
    'total_spent', 'order_frequency', 'avg_cart_size',
    'categories_browsed', 'return_rate', 'review_count'
]

# Cluster and analyze
kmeans = KMeans(n_clusters=5)
customers['segment'] = kmeans.fit_predict(customers_scaled)

# Results interpretation
for i in range(5):
    segment = customers[customers['segment'] == i]
    print(f"\nSegment {i}: {len(segment)} customers")
    print(segment[customer_features].mean())
\end{lstlisting}

\begin{columns}
\column{0.5\textwidth}
\textbf{Discovered Segments:}
\begin{itemize}
    \item \textbf{VIP Shoppers}: High spend, low returns
    \item \textbf{Bargain Hunters}: Sale-focused, high cart
    \item \textbf{Window Shoppers}: Browse, rarely buy
    \item \textbf{Loyal Regulars}: Consistent, medium spend
    \item \textbf{One-timers}: Single purchase, dormant
\end{itemize}

\column{0.5\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/customer_segments.pdf}
\end{columns}
\end{frame}

% Slide 19: Interactive Demo Code
\begin{frame}[fragile]{Interactive Demo: Try It Yourself!}
\textbf{Complete Clustering Pipeline}

\begin{lstlisting}
# Complete working example you can run
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Generate sample data (replace with your data)
np.random.seed(42)
n_users = 1000
data = {
    'usage_hours': np.random.exponential(3, n_users),
    'features_used': np.random.poisson(15, n_users),
    'days_active': np.random.randint(1, 31, n_users)
}
df = pd.DataFrame(data)

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# Cluster
kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(X_scaled)

# Visualize and interpret
print(df.groupby('cluster').mean())
# Try changing n_clusters and see what happens!
\end{lstlisting}
\end{frame}

% PART III: DESIGN INTEGRATION (8 slides)

% Slide 20: From Data Clusters to User Personas
\begin{frame}{From Data Clusters to User Personas}
\textbf{Turning Numbers into People}

\begin{columns}
\column{0.5\textwidth}
\textbf{Cluster Statistics:}
\colorbox{gray!20}{
\begin{tabular}{lcc}
\textbf{Metric} & \textbf{Cluster 0} \\
\hline
Avg. Usage & 7.2 hrs/day \\
Features Used & 45/50 \\
Peak Time & 9am-5pm \\
Device & Desktop (85\%) \\
Retention & 95\% \\
\end{tabular}
}

\column{0.5\textwidth}
\textbf{Persona Created:}
\colorbox{mlblue!20}{
\begin{minipage}{0.9\textwidth}
\textbf{``Professional Paula''}\\
\textit{Power user, 32, Marketing Manager}\\[0.2cm]
\textbf{Goals:} Maximize productivity\\
\textbf{Needs:} Advanced features, shortcuts\\
\textbf{Pain:} Slow load times\\
\textbf{Quote:} ``This tool is my office''
\end{minipage}
}
\end{columns}

\vspace{0.5cm}
\textbf{Transformation Process:}
\begin{enumerate}
    \item Analyze cluster statistics
    \item Identify defining characteristics
    \item Create realistic profile
    \item Add human elements (name, photo, quote)
    \item Validate with real user interviews
\end{enumerate}
\end{frame}

% Slide 21: Creating Empathy Maps from Data
\begin{frame}{Creating Empathy Maps from Data}
\textbf{What Your Clusters Think, Feel, Say, and Do}

\begin{center}
\includegraphics[width=0.8\textwidth]{ML_Design_Course/week01_visuals/empathy_map_clusters.pdf}
\end{center}

\begin{columns}
\column{0.25\textwidth}
\textbf{THINK}\\
{\footnotesize From search queries and help topics}

\column{0.25\textwidth}
\textbf{FEEL}\\
{\footnotesize From sentiment in reviews and feedback}

\column{0.25\textwidth}
\textbf{SAY}\\
{\footnotesize From support tickets and forums}

\column{0.25\textwidth}
\textbf{DO}\\
{\footnotesize From clickstream and usage data}
\end{columns}

\vspace{0.3cm}
\colorbox{mlgreen!20}{Each cluster fills the empathy map differently based on their data patterns}
\end{frame}

% Slide 22: Finding User Pain Points
\begin{frame}{Finding User Pain Points}
\textbf{Where Each Cluster Struggles}

\begin{columns}
\column{0.6\textwidth}
\textbf{Pain Point Detection Methods:}
\begin{itemize}
    \item High exit rates at specific features
    \item Support ticket clustering
    \item Feature abandonment patterns
    \item Error message frequency
    \item Negative sentiment spikes
\end{itemize}

\vspace{0.3cm}
\textbf{Cluster-Specific Pain Points:}
\begin{itemize}
    \item \textbf{Power Users}: Need bulk operations
    \item \textbf{New Users}: Overwhelming interface
    \item \textbf{Mobile Users}: Desktop-only features
    \item \textbf{Free Users}: Paywall friction
\end{itemize}

\column{0.4\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/pain_points_heatmap.pdf}

{\footnotesize Red = High frustration areas per cluster}
\end{columns}

\vspace{0.3cm}
\colorbox{mlred!20}{Different clusters = Different problems = Different solutions needed}
\end{frame}

% Slide 23: Discovering Behavior Patterns
\begin{frame}{Discovering Behavior Patterns}
\textbf{How Different Clusters Use Your Product}

\begin{center}
\includegraphics[width=0.85\textwidth]{ML_Design_Course/week01_visuals/behavior_patterns.pdf}
\end{center}

\begin{columns}
\column{0.33\textwidth}
\textbf{Morning Rushers}
\begin{itemize}
    \item 6am-9am peak
    \item Quick actions
    \item Mobile-heavy
    \item Notifications on
\end{itemize}

\column{0.33\textwidth}
\textbf{Deep Workers}
\begin{itemize}
    \item 2-4 hour sessions
    \item Complex workflows
    \item Desktop only
    \item Focus mode users
\end{itemize}

\column{0.33\textwidth}
\textbf{Social Butterflies}
\begin{itemize}
    \item Share frequently
    \item Collaboration tools
    \item Comments active
    \item Team features
\end{itemize}
\end{columns}

\colorbox{mlyellow!20}{Design different experiences for different behavior patterns}
\end{frame}

% Slide 24: Building User Journey Maps
\begin{frame}{Building User Journey Maps}
\textbf{Each Cluster's Path Through Your Product}

\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/journey_map_clusters.pdf}

\textbf{Cluster-Based Journey Insights:}
\begin{columns}
\column{0.5\textwidth}
\begin{itemize}
    \item \textbf{Discoverers}: Long exploration phase
    \item \textbf{Goal-Oriented}: Direct to action
    \item \textbf{Learners}: Heavy documentation use
\end{itemize}

\column{0.5\textwidth}
\begin{itemize}
    \item \textbf{Social}: Share early and often
    \item \textbf{Cautious}: Multiple trial sessions
    \item \textbf{Power}: Skip onboarding
\end{itemize}
\end{columns}

\vspace{0.3cm}
\colorbox{mlblue!20}{Optimize each touchpoint for each cluster's journey style}
\end{frame}

% Slide 25: Identifying Stakeholder Groups
\begin{frame}{Identifying Stakeholder Groups}
\textbf{Who Really Uses Your Product?}

\begin{columns}
\column{0.5\textwidth}
\textbf{Discovered Stakeholders:}
\begin{itemize}
    \item \textbf{Decision Makers} (5\%)
    \begin{itemize}
        \item Admin features
        \item Billing pages
        \item Team management
    \end{itemize}
    \item \textbf{Daily Users} (60\%)
    \begin{itemize}
        \item Core features
        \item Regular patterns
        \item Productivity focus
    \end{itemize}
    \item \textbf{Influencers} (15\%)
    \begin{itemize}
        \item Share features
        \item Invite others
        \item Write reviews
    \end{itemize}
    \item \textbf{Evaluators} (20\%)
    \begin{itemize}
        \item Trial users
        \item Comparison shoppers
        \item Feature testers
    \end{itemize}
\end{itemize}

\column{0.5\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/stakeholder_network.pdf}

{\footnotesize Network shows how different groups interact and influence each other}
\end{columns}
\end{frame}

% Slide 26: Visual Persona Cards
\begin{frame}{Visual Persona Cards}
\textbf{Making Clusters Memorable and Actionable}

\begin{center}
\includegraphics[width=0.9\textwidth]{ML_Design_Course/week01_visuals/persona_cards.pdf}
\end{center}

\textbf{Each Card Includes:}
\begin{columns}
\column{0.25\textwidth}
\begin{itemize}
    \item Photo/Avatar
    \item Name \& Role
    \item Key Stats
\end{itemize}

\column{0.25\textwidth}
\begin{itemize}
    \item Goals
    \item Frustrations
    \item Needs
\end{itemize}

\column{0.25\textwidth}
\begin{itemize}
    \item Tech savviness
    \item Usage patterns
    \item Feature preferences
\end{itemize}

\column{0.25\textwidth}
\begin{itemize}
    \item Quote
    \item Design implications
    \item Priority level
\end{itemize}
\end{columns}

\colorbox{mlgreen!20}{Print and post these cards - keep users visible during design!}
\end{frame}

% Slide 27: Making Design Decisions from Clusters
\begin{frame}{Making Design Decisions from Clusters}
\textbf{From Insights to Action}

\begin{columns}
\column{0.6\textwidth}
\textbf{Cluster-Driven Decisions:}

\colorbox{gray!20}{
\begin{tabular}{ll}
\textbf{Finding} & \textbf{Design Decision} \\
\hline
3 skill levels found & Progressive disclosure UI \\
Mobile vs Desktop split & Responsive-first design \\
Power users frustrated & Advanced mode option \\
New users confused & Better onboarding \\
Social cluster exists & Add sharing features \\
\end{tabular}
}

\vspace{0.3cm}
\textbf{Prioritization Framework:}
\begin{enumerate}
    \item Size of cluster (impact)
    \item Pain intensity (urgency)
    \item Business value (ROI)
    \item Implementation cost (feasibility)
\end{enumerate}

\column{0.4\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/design_priority_matrix.pdf}

{\footnotesize Plot features by cluster importance vs. effort}
\end{columns}

\colorbox{mlblue!20}{Design for your biggest, most valuable, or most struggling clusters first}
\end{frame}

% PART IV: SUMMARY & NEXT STEPS (5 slides)

% Slide 28: Other Clustering Methods Overview
\begin{frame}{Other Clustering Methods Overview}
\textbf{Beyond K-means: More Tools in Your Toolkit}

\begin{columns}
\column{0.5\textwidth}
\textbf{Gaussian Mixture Models}
\begin{itemize}
    \item Soft clustering (probability-based)
    \item Handles overlapping groups
    \item Good for uncertain boundaries
\end{itemize}

\textbf{Mean Shift}
\begin{itemize}
    \item Finds density peaks automatically
    \item No need to specify K
    \item Great for image segmentation
\end{itemize}

\column{0.5\textwidth}
\textbf{Spectral Clustering}
\begin{itemize}
    \item Handles complex shapes
    \item Uses graph theory
    \item Good for social networks
\end{itemize}

\textbf{OPTICS}
\begin{itemize}
    \item Like DBSCAN but better
    \item Handles varying densities
    \item Creates reachability plots
\end{itemize}
\end{columns}

\vspace{0.5cm}
\begin{center}
\includegraphics[width=0.7\textwidth]{ML_Design_Course/week01_visuals/clustering_methods_comparison.pdf}
\end{center}
\end{frame}

% Slide 29: Industry Case Study - Spotify
\begin{frame}{Industry Case Study: Spotify's Discover Weekly}
\textbf{How Clustering Powers Personalized Playlists}

\begin{columns}
\column{0.6\textwidth}
\textbf{The Challenge:}
\begin{itemize}
    \item Hundreds of millions of users
    \item Tens of millions of songs
    \item Create unique playlists weekly
    \item Feel personally curated
\end{itemize}

\textbf{The Solution:}
\begin{enumerate}
    \item Cluster users by listening history
    \item Cluster songs by audio features
    \item Find songs your cluster likes that you haven't heard
    \item Mix in variety from adjacent clusters
    \item Result: 30 new songs every Monday
\end{enumerate}

\column{0.4\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/spotify_clustering.pdf}

{\footnotesize 
\textbf{Impact:}
\begin{itemize}
    \item Significant user engagement
    \item Major impact on listening behavior
    \item Transformed music discovery
\end{itemize}
}
\end{columns}

\colorbox{mlgreen!20}{Clustering at scale: From understanding users to delighting them}
\end{frame}

% Slide 30: Key Takeaways
\begin{frame}{Week 1: Key Takeaways}
\textbf{What You've Learned}

\begin{columns}
\column{0.5\textwidth}
\textbf{Technical Skills:}
\begin{itemize}
    \item K-means clustering implementation
    \item Hierarchical clustering with dendrograms
    \item DBSCAN for density-based groups
    \item Data preparation and scaling
    \item Cluster evaluation methods
    \item Visualization techniques
\end{itemize}

\textbf{Design Skills:}
\begin{itemize}
    \item Creating data-driven personas
    \item Building empathy maps
    \item Identifying pain points
    \item Journey mapping
    \item Stakeholder identification
\end{itemize}

\column{0.5\textwidth}
\textbf{Key Insights:}
\begin{itemize}
    \item Clustering reveals hidden user groups
    \item Different algorithms for different data
    \item Always validate with business sense
    \item Clusters drive design decisions
    \item Scale empathy with data
\end{itemize}

\vspace{0.3cm}
\colorbox{mlblue!20}{
\begin{minipage}{0.9\textwidth}
\textbf{Remember:} Clustering is about understanding, not just grouping
\end{minipage}
}
\end{columns}
\end{frame}

% Slide 31: Preview of Week 2
\begin{frame}{Preview: Week 2}
\textbf{Advanced Clustering + Deep Empathy}

\begin{columns}
\column{0.6\textwidth}
\textbf{Next Week You'll Learn:}
\begin{itemize}
    \item Time-series clustering for behavior evolution
    \item Multi-view clustering (combining data sources)
    \item Online clustering for real-time segmentation
    \item Clustering validation techniques
    \item A/B testing with clusters
    \item Emotional journey mapping
    \item Micro-moment identification
    \item Cluster-based personalization
\end{itemize}

\textbf{Practical Project:}\\
Build a complete user segmentation system for a real app

\column{0.4\textwidth}
\includegraphics[width=\textwidth]{ML_Design_Course/week01_visuals/week2_preview.pdf}

\vspace{0.3cm}
{\footnotesize
\textbf{Homework:}
\begin{itemize}
    \item Practice K-means on your data
    \item Create one persona from a cluster
    \item Read: Chapter 2 materials
\end{itemize}
}
\end{columns}

\vspace{0.3cm}
\begin{center}
\colorbox{mlorange!20}{\textbf{See you next week for deeper dives into clustering!}}
\end{center}
\end{frame}

% Slide 32: References
\begin{frame}{References \& Resources}
\textbf{Core Algorithms:}
\begin{itemize}
    \item MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations
    \item Ester et al. (1996). A density-based algorithm for discovering clusters (DBSCAN)
    \item Lloyd, S. (1982). Least squares quantization in PCM (K-means)
\end{itemize}

\textbf{Design Thinking Integration:}
\begin{itemize}
    \item Brown, T. (2009). Change by Design: How Design Thinking Transforms Organizations
    \item IDEO Design Thinking Toolkit
\end{itemize}

\textbf{Tools \& Libraries:}
\begin{itemize}
    \item scikit-learn: Machine Learning in Python
    \item matplotlib \& seaborn: Visualization libraries
    \item Course repository: github.com/ml-design-thinking
\end{itemize}
\end{frame}

% APPENDIX: MATHEMATICAL DETAILS
\appendix

% Appendix Slide 1: Mathematical Foundations
\begin{frame}{Appendix: Mathematical Foundations}
\textbf{Distance Metrics Formulas}

\textbf{Euclidean Distance:}
$$d(x,y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$$

\textbf{Manhattan Distance:}
$$d(x,y) = \sum_{i=1}^{n}|x_i - y_i|$$

\textbf{Cosine Similarity:}
$$\text{similarity}(x,y) = \frac{x \cdot y}{||x|| \cdot ||y||} = \frac{\sum_{i=1}^{n}x_i y_i}{\sqrt{\sum_{i=1}^{n}x_i^2} \cdot \sqrt{\sum_{i=1}^{n}y_i^2}}$$

\textbf{Minkowski Distance:}
$$d(x,y) = \left(\sum_{i=1}^{n}|x_i - y_i|^p\right)^{1/p}$$
\end{frame}

% Appendix Slide 2: K-means Algorithm Details
\begin{frame}{Appendix: K-means Algorithm Details}
\textbf{Complete Algorithm Specification}

\textbf{Objective Function (Minimize):}
$$J = \sum_{i=1}^{k}\sum_{x \in C_i}||x - \mu_i||^2$$

where $\mu_i$ is the mean of cluster $C_i$

\textbf{Algorithm Steps:}
\begin{enumerate}
    \item Initialize: Choose $k$ points as initial centroids
    \item Assignment: $C_i = \{x_p : ||x_p - \mu_i||^2 \leq ||x_p - \mu_j||^2 \forall j, 1 \leq j \leq k\}$
    \item Update: $\mu_i = \frac{1}{|C_i|}\sum_{x \in C_i}x$
    \item Repeat until convergence: $||\mu_i^{(t+1)} - \mu_i^{(t)}|| < \epsilon$
\end{enumerate}

\textbf{Complexity:} $O(n \cdot k \cdot d \cdot i)$ where $n$ = points, $k$ = clusters, $d$ = dimensions, $i$ = iterations
\end{frame}

% Appendix Slide 3: Silhouette Coefficient
\begin{frame}{Appendix: Silhouette Coefficient Calculation}
\textbf{Cluster Quality Metric}

For each point $i$:
\begin{itemize}
    \item $a(i)$ = average distance to points in same cluster
    \item $b(i)$ = minimum average distance to points in different cluster
\end{itemize}

\textbf{Silhouette Coefficient:}
$$s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$$

\textbf{Interpretation:}
\begin{itemize}
    \item $s(i) \approx 1$: Well clustered
    \item $s(i) \approx 0$: On border between clusters
    \item $s(i) \approx -1$: Misclassified
\end{itemize}

\textbf{Overall Score:}
$$S = \frac{1}{n}\sum_{i=1}^{n}s(i)$$
\end{frame}

% Appendix Slide 4: Gaussian Mixture Models
\begin{frame}{Appendix: Gaussian Mixture Models}
\textbf{Probabilistic Clustering}

\textbf{Model:}
$$p(x) = \sum_{k=1}^{K}\pi_k \mathcal{N}(x|\mu_k, \Sigma_k)$$

where:
\begin{itemize}
    \item $\pi_k$ = mixing coefficient (prior probability)
    \item $\mu_k$ = mean of component $k$
    \item $\Sigma_k$ = covariance matrix of component $k$
\end{itemize}

\textbf{EM Algorithm:}
\begin{itemize}
    \item \textbf{E-step:} Compute responsibilities
    $$\gamma_{nk} = \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)}{\sum_{j=1}^{K}\pi_j \mathcal{N}(x_n|\mu_j, \Sigma_j)}$$
    
    \item \textbf{M-step:} Update parameters
    $$\mu_k = \frac{\sum_n \gamma_{nk}x_n}{\sum_n \gamma_{nk}}$$
\end{itemize}
\end{frame}

% Appendix Slide 5: t-SNE Mathematics
\begin{frame}{Appendix: t-SNE Mathematical Background}
\textbf{t-Distributed Stochastic Neighbor Embedding}

\textbf{High-dimensional similarity:}
$$p_{j|i} = \frac{\exp(-||x_i - x_j||^2/2\sigma_i^2)}{\sum_{k \neq i}\exp(-||x_i - x_k||^2/2\sigma_i^2)}$$

\textbf{Low-dimensional similarity (Student t-distribution):}
$$q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l}(1 + ||y_k - y_l||^2)^{-1}}$$

\textbf{Objective (KL divergence):}
$$C = \sum_i KL(P_i||Q_i) = \sum_i \sum_j p_{ij}\log\frac{p_{ij}}{q_{ij}}$$

\textbf{Gradient:}
$$\frac{\partial C}{\partial y_i} = 4\sum_j(p_{ij} - q_{ij})(y_i - y_j)(1 + ||y_i - y_j||^2)^{-1}$$
\end{frame}

% Appendix Slide 6: Computational Complexity
\begin{frame}{Appendix: Computational Complexity Analysis}
\textbf{Time and Space Complexity of Clustering Algorithms}

\begin{center}
\begin{tabular}{lll}
\hline
\textbf{Algorithm} & \textbf{Time Complexity} & \textbf{Space Complexity} \\
\hline
K-means & $O(n \cdot k \cdot d \cdot i)$ & $O(n \cdot d + k \cdot d)$ \\
Hierarchical & $O(n^2 \log n)$ to $O(n^3)$ & $O(n^2)$ \\
DBSCAN & $O(n \log n)$ average & $O(n)$ \\
GMM & $O(n \cdot k \cdot d^2 \cdot i)$ & $O(n \cdot d + k \cdot d^2)$ \\
Spectral & $O(n^3)$ & $O(n^2)$ \\
Mean Shift & $O(n^2 \cdot i)$ & $O(n \cdot d)$ \\
\hline
\end{tabular}
\end{center}

\textbf{Legend:}
\begin{itemize}
    \item $n$ = number of data points
    \item $k$ = number of clusters
    \item $d$ = dimensionality
    \item $i$ = number of iterations
\end{itemize}

\textbf{Scalability Tips:}
\begin{itemize}
    \item Use Mini-batch K-means for $n > 10,000$
    \item Consider sampling for hierarchical clustering
    \item Use approximate nearest neighbors for DBSCAN
\end{itemize}
\end{frame}

% Appendix Slide 7: Research Papers
\begin{frame}{Appendix: Further Reading}
\textbf{Key Papers and Resources}

\textbf{Foundational Papers:}
\begin{itemize}
    \item MacQueen, J. (1967). ``Some methods for classification and analysis of multivariate observations''
    \item Ester et al. (1996). ``A density-based algorithm for discovering clusters'' (DBSCAN)
    \item Ng et al. (2002). ``On spectral clustering: Analysis and an algorithm''
\end{itemize}

\textbf{Modern Applications:}
\begin{itemize}
    \item Sculley, D. (2010). ``Web-scale k-means clustering'' (Google)
    \item McInnes et al. (2017). ``hdbscan: Hierarchical density based clustering''
    \item Spotify Research. ``Understanding Music through Machine Learning''
\end{itemize}

\textbf{Online Resources:}
\begin{itemize}
    \item scikit-learn clustering documentation
    \item Google's Machine Learning Crash Course
    \item Fast.ai Practical Deep Learning course
\end{itemize}
\end{frame}

\end{document}