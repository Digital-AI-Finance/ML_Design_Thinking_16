\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning}

% Define colors matching the overview presentation
\definecolor{mlblue}{RGB}{31, 119, 180}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlpurple}{RGB}{148, 103, 189}
\definecolor{mlbrown}{RGB}{140, 86, 75}
\definecolor{mlpink}{RGB}{227, 119, 194}
\definecolor{mlgray}{RGB}{127, 127, 127}
\definecolor{mlyellow}{RGB}{255, 187, 120}
\definecolor{mlcyan}{RGB}{23, 190, 207}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

\title{\Large\textbf{Machine Learning for Smarter Innovation}\\
\vspace{0.5em}
\Large Week 1: Foundations \& Clustering}
\subtitle{Augmenting the Empathize Phase with ML}
\author{BSc Course in AI-Enhanced Innovation}
\date{}

\begin{document}

% Title slide
\begin{frame}
\titlepage
\end{frame}

% Slide 2: Opening Power Chart
\begin{frame}[plain]
\begin{center}
\includegraphics[width=0.85\textwidth]{charts/convergence_flow.pdf}
\end{center}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{The Convergence Flow: Order from Chaos}}\\
\normalsize\textit{Watch 5000 data points self-organize into meaningful clusters}
\end{center}
\end{frame}

% PART 1: FOUNDATION & CONTEXT (8 slides)

% Slide 3: The Innovation Challenge
\begin{frame}
\frametitle{\Large The Innovation Challenge}
\framesubtitle{Why Traditional Design Needs AI Enhancement}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlred!10, colframe=mlred!50, title=Traditional Design Limits]
\begin{itemize}
\item \textbf{Scale}: Can interview 50 users, not 50,000
\item \textbf{Speed}: Months for insights
\item \textbf{Bias}: Designer's perspective dominates
\item \textbf{Patterns}: Miss hidden connections
\item \textbf{Iteration}: Slow feedback loops
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title=AI-Enhanced Innovation]
\begin{itemize}
\item \textbf{Scale}: Analyze millions of data points
\item \textbf{Speed}: Real-time insights
\item \textbf{Objectivity}: Data-driven discovery
\item \textbf{Patterns}: Find non-obvious relationships
\item \textbf{Iteration}: Continuous learning
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}

\vspace{0.5em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{The Promise: 100x more insights, 10x faster innovation}}
\end{center}
\end{frame}

% Slide 4: The Dual Pipeline Chart
\begin{frame}
\frametitle{\Large The Dual Pipeline}
\framesubtitle{Where ML Meets Design Thinking}

\begin{center}
\includegraphics[width=0.85\textwidth]{../ML_Design_Course/course_visuals/dual_pipeline.pdf}
\end{center}

\vspace{0.5em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{Two Powerful Methodologies Converge}}
\end{center}
\end{frame}

% Slide 5: The Dual Pipeline Comparison
\begin{frame}
\frametitle{\Large The Dual Pipeline (Continued)}
\framesubtitle{Understanding Both Worlds}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlblue!10, colframe=mlblue!50, title=ML Pipeline]
\normalsize
\textbf{Data} $\rightarrow$ \textbf{Preprocess} $\rightarrow$ \textbf{Model} $\rightarrow$ \textbf{Evaluate} $\rightarrow$ \textbf{Deploy}
\vspace{0.3em}
\begin{itemize}
\item Collect user behavior
\item Clean and transform
\item Train algorithms
\item Validate accuracy
\item Scale to production
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlorange!10, colframe=mlorange!50, title=Design Pipeline]
\normalsize
\textbf{Empathize} $\rightarrow$ \textbf{Define} $\rightarrow$ \textbf{Ideate} $\rightarrow$ \textbf{Prototype} $\rightarrow$ \textbf{Test}
\vspace{0.3em}
\begin{itemize}
\item Understand users
\item Frame problems
\item Generate solutions
\item Build concepts
\item Validate with users
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}

\vspace{0.5em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{Integration = Innovation at Scale}}
\end{center}
\end{frame}

% Slide 6: Your Innovation Journey Chart
\begin{frame}
\frametitle{\Large Your Innovation Journey}
\framesubtitle{10 Weeks to AI-Powered Design Mastery}

\begin{center}
\includegraphics[width=0.85\textwidth]{../ML_Design_Course/course_visuals/journey_roadmap.pdf}
\end{center}
\end{frame}

% Slide 7: Your Innovation Journey Details
\begin{frame}
\frametitle{\Large Your Innovation Journey (Continued)}
\framesubtitle{What You'll Master in Each Stage}

\begin{center}
\Large
\begin{tabular}{lll}
\toprule
\textbf{Stage} & \textbf{Weeks} & \textbf{Innovation Unlocked} \\
\midrule
\textcolor{mlpurple}{Empathize} & 1-2 & Discover hidden user needs at scale \\
\textcolor{mlblue}{Define} & 3-4 & Identify the right problems to solve \\
\textcolor{mlgreen}{Ideate} & 5-6 & Generate novel solutions with AI \\
\textcolor{mlorange}{Prototype} & 7-8 & Build smart, adaptive concepts \\
\textcolor{mlred}{Test} & 9-10 & Evolve through continuous learning \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.5em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{This Week: Clustering for Deep User Understanding}}
\end{center}
\end{frame}

% Slide 8: Week 1 Focus
\begin{frame}
\frametitle{\Large Week 1: Clustering for Empathy}
\framesubtitle{From Random Data to User Understanding}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\Large\textbf{What We'll Learn:}
\normalsize
\begin{itemize}
\item How clustering reveals user segments
\item K-means algorithm fundamentals
\item Finding the optimal number of clusters
\item Quality metrics for validation
\item Advanced clustering techniques
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\Large\textbf{Design Applications:}
\normalsize
\begin{itemize}
\item Create data-driven personas
\item Map user journeys by segment
\item Identify pain points systematically
\item Prioritize design efforts
\item Scale empathy to thousands
\end{itemize}
\end{column}
\end{columns}

\vspace{0.5em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{Goal: Transform data points into human insights}}
\end{center}
\end{frame}

% PART 2: TECHNICAL CORE (10 slides)

% Slide 9: What is Clustering?
\begin{frame}
\frametitle{\Large What is Clustering?}
\framesubtitle{Finding Natural Groups in Data}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/chaos_to_clarity.pdf}
\end{center}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{Clustering Finds:}
\normalsize
\begin{itemize}
\item Natural groupings
\item Similar behaviors
\item Hidden segments
\item Pattern relationships
\end{itemize}

\vspace{0.5em}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50]
\textbf{Key Insight:}\\
Users who behave similarly likely have similar needs
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

% Slide 10: K-Means Algorithm
\begin{frame}
\frametitle{\Large K-Means: The Workhorse Algorithm}
\framesubtitle{How It Organizes Your Users}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\Large\textbf{The Process:}
\normalsize
\begin{enumerate}
\item Choose K (number of clusters)
\item Place K random centroids
\item Assign points to nearest centroid
\item Move centroids to cluster mean
\item Repeat until stable
\end{enumerate}

\vspace{0.5em}
\textbf{Strengths:}
\begin{itemize}
\item Fast and scalable
\item Easy to understand
\item Works well for spherical clusters
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/kmeans_animation.pdf}
\end{center}
\end{column}
\end{columns}
\end{frame}

% Slide 11: K-Means Animation
\begin{frame}
\frametitle{\Large K-Means in Action}
\framesubtitle{Step-by-Step Convergence}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/kmeans_animation.pdf}
\end{center}

\begin{center}
\normalsize
Iteration 1 $\rightarrow$ Iteration 3 $\rightarrow$ Iteration 5 $\rightarrow$ \textbf{Converged}
\end{center}
\end{frame}

% Slide 12: Choosing K - Elbow Method
\begin{frame}
\frametitle{\Large How Many Clusters?}
\framesubtitle{The Elbow Method}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/elbow_method.pdf}
\end{center}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{Finding the Elbow:}
\normalsize
\begin{itemize}
\item Plot inertia vs K
\item Look for the ``elbow''
\item Balance between:
    \begin{itemize}
    \small
    \item Too few: Mixed groups
    \item Too many: Overfitting
    \end{itemize}
\end{itemize}

\vspace{0.5em}
\begin{tcolorbox}[colback=mlblue!10, colframe=mlblue!50]
\textbf{Optimal K = 5}\\
Best trade-off between simplicity and accuracy
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

% Slide 13: Distance Metrics
\begin{frame}
\frametitle{\Large Distance Metrics}
\framesubtitle{How We Measure Similarity}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/distance_visual.pdf}
\end{center}

\begin{columns}[T]
\begin{column}{0.32\textwidth}
\begin{center}
\textbf{Euclidean}\\
\small Direct distance\\
Best for continuous data
\end{center}
\end{column}

\begin{column}{0.32\textwidth}
\begin{center}
\textbf{Manhattan}\\
\small City-block distance\\
Good for grid-like data
\end{center}
\end{column}

\begin{column}{0.32\textwidth}
\begin{center}
\textbf{Cosine}\\
\small Angle between vectors\\
Ideal for text/preferences
\end{center}
\end{column}
\end{columns}
\end{frame}

% Slide 14: Cluster Quality
\begin{frame}
\frametitle{\Large Cluster Quality Metrics}
\framesubtitle{How Good Are Your Groups?}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/cluster_quality.pdf}
\end{center}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{Silhouette Score:}
\normalsize
\begin{itemize}
\item Ranges from -1 to +1
\item Higher = better separation
\item Our score: \textbf{0.73}
\end{itemize}

\vspace{0.5em}
\textbf{What it measures:}
\begin{itemize}
\item Within-cluster cohesion
\item Between-cluster separation
\item Overall cluster validity
\end{itemize}

\vspace{0.5em}
\textcolor{mlgreen}{\textbf{0.73 = Strong clusters!}}
\end{column}
\end{columns}
\end{frame}

% Slide 15: Beyond K-Means - DBSCAN
\begin{frame}
\frametitle{\Large Beyond K-Means: DBSCAN}
\framesubtitle{Finding Arbitrary Shaped Clusters}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\Large\textbf{DBSCAN Advantages:}
\normalsize
\begin{itemize}
\item No need to specify K
\item Finds arbitrary shapes
\item Identifies outliers
\item Handles noise well
\end{itemize}

\vspace{0.5em}
\textbf{Perfect for:}
\begin{itemize}
\item Non-spherical patterns
\item Varying densities
\item Outlier detection
\item Exploratory analysis
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/dbscan_shapes.pdf}
\end{center}
\end{column}
\end{columns}
\end{frame}

% Slide 16: DBSCAN Shapes
\begin{frame}
\frametitle{\Large DBSCAN: Complex Patterns}
\framesubtitle{When K-Means Isn't Enough}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/dbscan_shapes.pdf}
\end{center}

\begin{center}
\normalsize
K-Means: Forces spherical shapes | DBSCAN: Finds natural boundaries
\end{center}
\end{frame}

% Slide 17: Hierarchical Clustering
\begin{frame}
\frametitle{\Large Hierarchical Clustering}
\framesubtitle{Building a Tree of Relationships}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/dendrogram_example.pdf}
\end{center}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{Dendrogram Benefits:}
\normalsize
\begin{itemize}
\item Shows cluster hierarchy
\item Multiple granularities
\item Natural relationships
\item No preset K needed
\end{itemize}

\vspace{0.5em}
\textbf{Cut the tree at any level:}
\begin{itemize}
\item High cut = Few clusters
\item Low cut = Many clusters
\item Choose based on needs
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% Slide 18: Feature Importance
\begin{frame}
\frametitle{\Large What Drives the Clusters?}
\framesubtitle{Feature Importance Analysis}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/feature_importance.pdf}
\end{center}

\begin{center}
\Large\textcolor{mlpurple}{\textbf{Key Insight: Usage frequency matters most!}}
\end{center}
\end{frame}

% PART 3: DESIGN INTEGRATION (8 slides)

% Slide 19: From Data to Empathy
\begin{frame}
\frametitle{\Large From Data Points to Human Understanding}
\framesubtitle{Bridging the Technical-Human Gap}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/user_empathy_visual.pdf}
\end{center}

\begin{center}
\Large\textcolor{mlpurple}{\textbf{Each cluster represents real human needs}}
\end{center}
\end{frame}

% Slide 20: AI-Generated Personas
\begin{frame}
\frametitle{\Large AI-Generated User Personas}
\framesubtitle{Data-Driven Character Development}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/persona_cards.pdf}
\end{center}

\begin{center}
\normalsize
Power Users | Casual Browsers | Price-Conscious | Feature Seekers | New Users
\end{center}
\end{frame}

% Slide 21: Empathy Maps
\begin{frame}
\frametitle{\Large Cluster-Based Empathy Mapping}
\framesubtitle{Understanding Each Segment's Experience}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/empathy_map_clusters.pdf}
\end{center}
\end{frame}

% Slide 22: Journey Mapping
\begin{frame}
\frametitle{\Large Different Journeys for Different Clusters}
\framesubtitle{Personalized Path Understanding}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/journey_map_clusters.pdf}
\end{center}
\end{frame}

% Slide 23: Pain Points Heatmap
\begin{frame}
\frametitle{\Large Pain Points by Cluster}
\framesubtitle{Where Each Segment Struggles}

\begin{columns}[T]
\begin{column}{0.65\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/pain_points_heatmap.pdf}
\end{center}
\end{column}

\begin{column}{0.33\textwidth}
\Large\textbf{Key Findings:}
\normalsize
\begin{itemize}
\item New users: Onboarding
\item Power users: Speed
\item Casual: Complexity
\item Price-conscious: Value
\end{itemize}

\vspace{0.5em}
\textcolor{mlred}{\textbf{Design implication:}}\\
\small One solution won't fit all!
\end{column}
\end{columns}
\end{frame}

% Slide 24: Behavior Patterns
\begin{frame}
\frametitle{\Large Behavioral Patterns Revealed}
\framesubtitle{What Clusters Tell Us About Usage}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/behavior_patterns.pdf}
\end{center}
\end{frame}

% Slide 25: Design Priority Matrix
\begin{frame}
\frametitle{\Large Design Priority Matrix}
\framesubtitle{Where to Focus Your Efforts}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/design_priority_matrix.pdf}
\end{center}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{Priority Quadrants:}
\normalsize
\begin{itemize}
\item \textcolor{mlred}{\textbf{High Impact + High Effort}}\\
  Strategic initiatives
\item \textcolor{mlgreen}{\textbf{High Impact + Low Effort}}\\
  Quick wins
\item \textcolor{mlorange}{\textbf{Low Impact + Low Effort}}\\
  Fill-ins
\item \textcolor{mlgray}{\textbf{Low Impact + High Effort}}\\
  Avoid
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% Slide 26: Stakeholder Network
\begin{frame}
\frametitle{\Large Understanding Stakeholder Connections}
\framesubtitle{Network Analysis of User Relationships}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/stakeholder_network.pdf}
\end{center}
\end{frame}

% PART 4: SUMMARY & PRACTICE (5 slides)

% Slide 27: Case Study - Spotify
\begin{frame}
\frametitle{\Large Case Study: Spotify's Clustering Success}
\framesubtitle{Real-World Application}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/spotify_clustering.pdf}
\end{center}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{Spotify Uses Clustering For:}
\normalsize
\begin{itemize}
\item Music taste profiles
\item Discover Weekly playlists
\item User segmentation
\item Recommendation engine
\end{itemize}

\vspace{0.5em}
\textbf{Results:}
\begin{itemize}
\item Personalized experience
\item Increased engagement
\item Better retention
\item Discovery of new artists
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% Slide 28: Key Takeaways
\begin{frame}
\frametitle{\Large Key Takeaways}
\framesubtitle{What We've Learned}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlblue!10, colframe=mlblue!50, title=Technical Skills]
\begin{itemize}
\item K-means clustering algorithm
\item Choosing optimal K with elbow method
\item Silhouette scores for validation
\item DBSCAN for complex shapes
\item Hierarchical clustering
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlorange!10, colframe=mlorange!50, title=Design Applications]
\begin{itemize}
\item Data-driven personas
\item Segment-specific journeys
\item Pain point identification
\item Priority matrices
\item Scaled empathy
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}

\vspace{0.5em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{Clustering transforms data into actionable user insights}}
\end{center}
\end{frame}

% Slide 29: Your Turn
\begin{frame}
\frametitle{\Large Your Turn: Practice Exercise}
\framesubtitle{Apply What You've Learned}

\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title=\Large Exercise: Segment Your Users]
\Large\textbf{Scenario:}
\normalsize
You have data from 1000 app users including:
\begin{itemize}
\item Usage frequency
\item Feature preferences  
\item Time spent
\item Purchase history
\end{itemize}

\vspace{0.5em}
\Large\textbf{Tasks:}
\normalsize
\begin{enumerate}
\item Choose appropriate features for clustering
\item Determine optimal number of clusters
\item Interpret what each cluster represents
\item Create one persona per cluster
\item Identify key pain points for each segment
\end{enumerate}
\end{tcolorbox}
\end{frame}

% Slide 30: Next Week Preview
\begin{frame}
\frametitle{\Large Next Week: Advanced Clustering}
\framesubtitle{Going Deeper into User Understanding}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/week2_preview.pdf}
\end{center}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{Week 2 Topics:}
\normalsize
\begin{itemize}
\item Density-based clustering
\item Gaussian mixture models
\item Clustering validation
\item Feature engineering
\item Real-time clustering
\end{itemize}

\vspace{0.5em}
\textbf{Design Focus:}
\begin{itemize}
\item Dynamic personas
\item Evolving segments
\item Predictive empathy
\item Micro-segmentation
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% Slide 31: Resources
\begin{frame}
\frametitle{\Large Resources \& Further Reading}
\framesubtitle{Deepen Your Understanding}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlblue!10, colframe=mlblue!50, title=Technical Resources]
\normalsize
\textbf{Papers:}
\begin{itemize}
\small
\item MacQueen, J. (1967). K-means
\item Ester et al. (1996). DBSCAN
\item Rousseeuw (1987). Silhouettes
\end{itemize}

\textbf{Tools:}
\begin{itemize}
\small
\item scikit-learn clustering
\item Orange data mining
\item KNIME analytics
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlorange!10, colframe=mlorange!50, title=Design Resources]
\normalsize
\textbf{Books:}
\begin{itemize}
\small
\item ``Design Thinking'' - Tim Brown
\item ``Sprint'' - Jake Knapp
\item ``Lean UX'' - Jeff Gothelf
\end{itemize}

\textbf{Applications:}
\begin{itemize}
\small
\item Miro (journey mapping)
\item Figma (persona creation)
\item Optimal Workshop
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}

\vspace{0.5em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{Questions? Let's discuss!}}
\end{center}
\end{frame}

% APPENDIX: TECHNICAL DEEP DIVE
\appendix
\section{Technical Appendix}

% Appendix Slide 1: K-Means Mathematics
\begin{frame}
\frametitle{\Large Appendix: K-Means Mathematics}
\framesubtitle{The Mathematical Foundation}

\Large\textbf{Objective Function (Inertia):}
\normalsize
$$J = \sum_{i=1}^{n} \sum_{j=1}^{k} w_{ij} ||x_i - \mu_j||^2$$

Where:
\begin{itemize}
\item $n$ = number of data points
\item $k$ = number of clusters
\item $w_{ij}$ = 1 if $x_i$ belongs to cluster $j$, 0 otherwise
\item $\mu_j$ = centroid of cluster $j$
\end{itemize}

\vspace{0.5em}
\Large\textbf{Update Rules:}
\normalsize
\begin{enumerate}
\item Assignment: $c^{(i)} = \arg\min_j ||x^{(i)} - \mu_j||^2$
\item Update: $\mu_j = \frac{1}{|S_j|} \sum_{i \in S_j} x^{(i)}$
\end{enumerate}
\end{frame}

% Appendix Slide 2: Distance Metrics Formulas
\begin{frame}
\frametitle{\Large Appendix: Distance Metrics}
\framesubtitle{Mathematical Definitions}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Euclidean Distance:}
$$d(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$$

\vspace{0.5em}
\textbf{Manhattan Distance:}
$$d(x,y) = \sum_{i=1}^{n} |x_i - y_i|$$

\vspace{0.5em}
\textbf{Minkowski Distance:}
$$d(x,y) = \left(\sum_{i=1}^{n} |x_i - y_i|^p\right)^{1/p}$$
\end{column}

\begin{column}{0.48\textwidth}
\textbf{Cosine Similarity:}
$$\cos(\theta) = \frac{x \cdot y}{||x|| \cdot ||y||}$$

\vspace{0.5em}
\textbf{Jaccard Distance:}
$$J(A,B) = 1 - \frac{|A \cap B|}{|A \cup B|}$$

\vspace{0.5em}
\textbf{Mahalanobis Distance:}
$$d(x,y) = \sqrt{(x-y)^T S^{-1} (x-y)}$$
\end{column}
\end{columns}
\end{frame}

% Appendix Slide 3: Silhouette Coefficient
\begin{frame}
\frametitle{\Large Appendix: Silhouette Coefficient}
\framesubtitle{Measuring Cluster Quality}

\Large\textbf{Silhouette Score for point $i$:}
\normalsize
$$s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$$

Where:
\begin{itemize}
\item $a(i)$ = average distance to points in same cluster
\item $b(i)$ = average distance to points in nearest neighbor cluster
\end{itemize}

\vspace{0.5em}
\Large\textbf{Interpretation:}
\normalsize
\begin{itemize}
\item $s(i) \approx 1$: Well clustered
\item $s(i) \approx 0$: On border between clusters
\item $s(i) \approx -1$: Misclassified
\end{itemize}

\vspace{0.5em}
\Large\textbf{Overall Score:}
\normalsize
$$S = \frac{1}{n} \sum_{i=1}^{n} s(i)$$
\end{frame}

% Appendix Slide 4: PCA for Visualization
\begin{frame}
\frametitle{\Large Appendix: PCA for Cluster Visualization}
\framesubtitle{Dimensionality Reduction}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/pca_clusters.pdf}
\end{center}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{PCA Process:}
\normalsize
\begin{enumerate}
\item Standardize data
\item Compute covariance matrix
\item Find eigenvectors/values
\item Select top 2 components
\item Transform data
\end{enumerate}

\vspace{0.5em}
\textbf{Variance Explained:}
\begin{itemize}
\item PC1: 45.2\%
\item PC2: 28.7\%
\item Total: 73.9\%
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% Appendix Slide 5: DBSCAN Algorithm
\begin{frame}
\frametitle{\Large Appendix: DBSCAN Algorithm}
\framesubtitle{Density-Based Clustering Details}

\Large\textbf{Key Parameters:}
\normalsize
\begin{itemize}
\item $\epsilon$ (eps): Maximum distance between points
\item MinPts: Minimum points to form dense region
\end{itemize}

\vspace{0.5em}
\Large\textbf{Point Classification:}
\normalsize
\begin{itemize}
\item \textbf{Core point}: Has $\geq$ MinPts within $\epsilon$
\item \textbf{Border point}: Within $\epsilon$ of core point
\item \textbf{Noise point}: Neither core nor border
\end{itemize}

\vspace{0.5em}
\Large\textbf{Algorithm Steps:}
\normalsize
\begin{enumerate}
\item Find all core points
\item Form clusters from core points within $\epsilon$
\item Assign border points to clusters
\item Mark remaining as noise
\end{enumerate}
\end{frame}

% Appendix Slide 6: Implementation Hints
\begin{frame}
\frametitle{\Large Appendix: Implementation Guidelines}
\framesubtitle{Practical Considerations}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlblue!10, colframe=mlblue!50, title=Data Preparation]
\small
\begin{itemize}
\item Standardize features
\item Handle missing values
\item Remove outliers (if needed)
\item Feature selection/engineering
\item Consider scaling methods
\end{itemize}
\end{tcolorbox}

\vspace{0.5em}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title=Algorithm Selection]
\small
\begin{itemize}
\item K-means: Spherical, similar size
\item DBSCAN: Arbitrary shapes
\item Hierarchical: Nested structure
\item GMM: Overlapping clusters
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlorange!10, colframe=mlorange!50, title=Validation Methods]
\small
\begin{itemize}
\item Silhouette score
\item Davies-Bouldin index
\item Calinski-Harabasz score
\item Visual inspection
\item Domain expert review
\end{itemize}
\end{tcolorbox}

\vspace{0.5em}
\begin{tcolorbox}[colback=mlred!10, colframe=mlred!50, title=Common Pitfalls]
\small
\begin{itemize}
\item Not scaling features
\item Wrong distance metric
\item Ignoring outliers
\item Over-clustering
\item Forcing clusters
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

\end{document}