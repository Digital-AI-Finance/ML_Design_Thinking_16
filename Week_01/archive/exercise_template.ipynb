{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Clustering for Innovation Discovery - Practice Exercise\n",
    "\n",
    "## Objective\n",
    "Apply clustering algorithms to discover patterns in innovation data from a company hackathon.\n",
    "\n",
    "## Dataset\n",
    "- 500 innovation proposals from employees\n",
    "- Features: category scores, complexity, impact, feasibility\n",
    "- Goal: Discover natural groupings to inform innovation strategy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Set style and seed\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print('Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic Innovation Data\n",
    "In practice, you would load real data. Here we simulate it for learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic innovation data\n",
    "n_innovations = 500\n",
    "\n",
    "# Create 4 innovation archetypes with different characteristics\n",
    "# Archetype 1: Quick Wins (low complexity, moderate impact)\n",
    "quick_wins = np.random.multivariate_normal(\n",
    "    [3, 6, 7, 5],  # [complexity, impact, feasibility, novelty]\n",
    "    [[1, 0.2, 0.3, 0.1], \n",
    "     [0.2, 1.5, 0.2, 0.3],\n",
    "     [0.3, 0.2, 1, 0.2],\n",
    "     [0.1, 0.3, 0.2, 1]], \n",
    "    125\n",
    ")\n",
    "\n",
    "# Archetype 2: Moonshots (high complexity, high impact)\n",
    "moonshots = np.random.multivariate_normal(\n",
    "    [8, 9, 4, 9],\n",
    "    [[1.5, 0.4, -0.3, 0.5], \n",
    "     [0.4, 1, 0.1, 0.4],\n",
    "     [-0.3, 0.1, 1.5, -0.2],\n",
    "     [0.5, 0.4, -0.2, 1]], \n",
    "    100\n",
    ")\n",
    "\n",
    "# Archetype 3: Incremental (low complexity, low impact)\n",
    "incremental = np.random.multivariate_normal(\n",
    "    [2, 3, 8, 2],\n",
    "    [[1, 0.3, 0.4, 0.2], \n",
    "     [0.3, 1.2, 0.3, 0.1],\n",
    "     [0.4, 0.3, 1, 0.3],\n",
    "     [0.2, 0.1, 0.3, 1]], \n",
    "    175\n",
    ")\n",
    "\n",
    "# Archetype 4: Transformative (moderate complexity, high impact)\n",
    "transformative = np.random.multivariate_normal(\n",
    "    [6, 8, 6, 7],\n",
    "    [[1.2, 0.3, 0.2, 0.4], \n",
    "     [0.3, 1.3, 0.3, 0.3],\n",
    "     [0.2, 0.3, 1.1, 0.2],\n",
    "     [0.4, 0.3, 0.2, 1.2]], \n",
    "    100\n",
    ")\n",
    "\n",
    "# Combine all data\n",
    "X = np.vstack([quick_wins, moonshots, incremental, transformative])\n",
    "true_labels = np.array([0]*125 + [1]*100 + [2]*175 + [3]*100)\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = ['Complexity', 'Impact', 'Feasibility', 'Novelty']\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['True_Archetype'] = true_labels\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory Data Analysis\n",
    "Understand your data before clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualizations to understand the data\n",
    "# Hint: Use pairplot, correlation matrix, or distribution plots\n",
    "\n",
    "# Your code here:\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Distribution plots\n",
    "for i, col in enumerate(feature_names):\n",
    "    ax = axes[i//2, i%2]\n",
    "    ax.hist(df[col], bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax.set_title(f'Distribution of {col}')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df[feature_names].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preprocessing\n",
    "Standardize features for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Standardize the features\n",
    "# Why is this important for clustering?\n",
    "\n",
    "# Your code here:\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[feature_names])\n",
    "\n",
    "print('Data standardized!')\n",
    "print(f'Mean: {X_scaled.mean(axis=0)}')\n",
    "print(f'Std: {X_scaled.std(axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Determine Optimal Number of Clusters\n",
    "Use elbow method and silhouette analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement elbow method\n",
    "# Calculate inertia for k=2 to k=10\n",
    "\n",
    "# Your code here:\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "ax1.plot(K_range, inertias, 'bo-')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Silhouette plot\n",
    "ax2.plot(K_range, silhouettes, 'ro-')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What is your optimal k?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Apply Multiple Clustering Algorithms\n",
    "Compare different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply K-Means, DBSCAN, and GMM\n",
    "# Compare their performance\n",
    "\n",
    "# Your code here:\n",
    "algorithms = {\n",
    "    'K-Means': KMeans(n_clusters=4, random_state=42, n_init=10),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'Hierarchical': AgglomerativeClustering(n_clusters=4),\n",
    "    'GMM': GaussianMixture(n_components=4, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, algorithm in algorithms.items():\n",
    "    if name == 'GMM':\n",
    "        labels = algorithm.fit_predict(X_scaled)\n",
    "    else:\n",
    "        labels = algorithm.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate metrics (skip for DBSCAN if it finds -1 labels)\n",
    "    if len(set(labels)) > 1 and -1 not in labels:\n",
    "        sil_score = silhouette_score(X_scaled, labels)\n",
    "        db_score = davies_bouldin_score(X_scaled, labels)\n",
    "        ch_score = calinski_harabasz_score(X_scaled, labels)\n",
    "    else:\n",
    "        sil_score = db_score = ch_score = None\n",
    "    \n",
    "    results[name] = {\n",
    "        'labels': labels,\n",
    "        'silhouette': sil_score,\n",
    "        'davies_bouldin': db_score,\n",
    "        'calinski_harabasz': ch_score\n",
    "    }\n",
    "    \n",
    "    print(f'{name}:')\n",
    "    print(f'  Unique clusters: {len(set(labels))}')\n",
    "    if sil_score:\n",
    "        print(f'  Silhouette Score: {sil_score:.3f}')\n",
    "        print(f'  Davies-Bouldin: {db_score:.3f}')\n",
    "        print(f'  Calinski-Harabasz: {ch_score:.1f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Clustering Results\n",
    "Use PCA for 2D visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create PCA visualization for each algorithm\n",
    "\n",
    "# Your code here:\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot true labels\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels, \n",
    "                         cmap='viridis', alpha=0.6, s=30)\n",
    "axes[0].set_title('True Archetypes')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "plt.colorbar(scatter, ax=axes[0])\n",
    "\n",
    "# Plot each algorithm's results\n",
    "for i, (name, result) in enumerate(results.items(), 1):\n",
    "    scatter = axes[i].scatter(X_pca[:, 0], X_pca[:, 1], c=result['labels'], \n",
    "                             cmap='viridis', alpha=0.6, s=30)\n",
    "    axes[i].set_title(f'{name}')\n",
    "    axes[i].set_xlabel(f'PC1')\n",
    "    axes[i].set_ylabel(f'PC2')\n",
    "    if result['silhouette']:\n",
    "        axes[i].text(0.02, 0.98, f'Sil: {result[\"silhouette\"]:.3f}',\n",
    "                    transform=axes[i].transAxes, fontsize=9,\n",
    "                    verticalalignment='top')\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Interpret Innovation Clusters\n",
    "What patterns do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze cluster characteristics\n",
    "# What makes each cluster unique?\n",
    "\n",
    "# Your code here:\n",
    "best_algorithm = 'K-Means'  # Choose based on metrics\n",
    "best_labels = results[best_algorithm]['labels']\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['Cluster'] = best_labels\n",
    "\n",
    "# Calculate cluster centers\n",
    "cluster_summary = df.groupby('Cluster')[feature_names].mean()\n",
    "print('Cluster Centers (Original Scale):')\n",
    "print(cluster_summary.round(2))\n",
    "print()\n",
    "\n",
    "# Interpret clusters\n",
    "interpretations = {\n",
    "    0: 'Quick Wins - Low complexity, good feasibility',\n",
    "    1: 'Moonshots - High impact, high novelty',\n",
    "    2: 'Incremental - Low risk, easy implementation',\n",
    "    3: 'Transformative - Balanced high potential'\n",
    "}\n",
    "\n",
    "print('Cluster Interpretations:')\n",
    "for cluster_id in range(len(cluster_summary)):\n",
    "    size = (df['Cluster'] == cluster_id).sum()\n",
    "    print(f'Cluster {cluster_id}: {size} innovations')\n",
    "    if cluster_id in interpretations:\n",
    "        print(f'  â†’ {interpretations[cluster_id]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Innovation Strategy Recommendations\n",
    "Based on your clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create actionable recommendations\n",
    "# How should the company prioritize these innovation clusters?\n",
    "\n",
    "# Your analysis here:\n",
    "recommendations = \"\"\"\n",
    "INNOVATION PORTFOLIO STRATEGY:\n",
    "\n",
    "1. QUICK WINS (Cluster 0): {:.0f} innovations\n",
    "   - Implement immediately for rapid value\n",
    "   - Low resource requirement\n",
    "   - Build momentum and credibility\n",
    "\n",
    "2. MOONSHOTS (Cluster 1): {:.0f} innovations  \n",
    "   - Select top 2-3 for long-term investment\n",
    "   - High risk, high reward\n",
    "   - Requires dedicated innovation lab\n",
    "\n",
    "3. INCREMENTAL (Cluster 2): {:.0f} innovations\n",
    "   - Delegate to operational teams\n",
    "   - Continuous improvement focus\n",
    "   - Minimal oversight needed\n",
    "\n",
    "4. TRANSFORMATIVE (Cluster 3): {:.0f} innovations\n",
    "   - Strategic priorities for next quarter\n",
    "   - Balance of risk and reward\n",
    "   - Cross-functional teams required\n",
    "\n",
    "RECOMMENDED PORTFOLIO MIX:\n",
    "- 40% Quick Wins (immediate impact)\n",
    "- 30% Transformative (strategic growth)\n",
    "- 20% Incremental (continuous improvement)\n",
    "- 10% Moonshots (future disruption)\n",
    "\"\"\".format(\n",
    "    (df['Cluster'] == 0).sum(),\n",
    "    (df['Cluster'] == 1).sum(),\n",
    "    (df['Cluster'] == 2).sum(),\n",
    "    (df['Cluster'] == 3).sum()\n",
    ")\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Results and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustered data\n",
    "df.to_csv('innovation_clusters.csv', index=False)\n",
    "print('Results saved to innovation_clusters.csv')\n",
    "\n",
    "# Summary statistics\n",
    "print('\\nFINAL SUMMARY:')\n",
    "print(f'Total innovations analyzed: {len(df)}')\n",
    "print(f'Clusters discovered: {len(set(best_labels))}')\n",
    "print(f'Best algorithm: {best_algorithm}')\n",
    "print(f'Silhouette score: {results[best_algorithm][\"silhouette\"]:.3f}')\n",
    "\n",
    "print('\\nNEXT STEPS:')\n",
    "print('1. Validate clusters with domain experts')\n",
    "print('2. Deep dive into each cluster for detailed insights')\n",
    "print('3. Create innovation roadmap based on clusters')\n",
    "print('4. Set up monitoring for cluster evolution over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge: Advanced Analysis\n",
    "\n",
    "Try these extensions:\n",
    "1. **Optimal eps for DBSCAN**: Use k-distance plot\n",
    "2. **Cluster stability**: Run clustering multiple times with different seeds\n",
    "3. **Feature importance**: Which features drive the clustering?\n",
    "4. **Outlier analysis**: Identify and analyze outlier innovations\n",
    "5. **Time complexity**: Measure and compare algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your bonus code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "1. Complete all TODO sections\n",
    "2. Add your interpretations and insights\n",
    "3. Export notebook as PDF/HTML\n",
    "4. Submit via course portal by [deadline]\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "- **Data Exploration** (20%): Thorough EDA with visualizations\n",
    "- **Algorithm Implementation** (30%): Correct application of multiple algorithms\n",
    "- **Evaluation** (20%): Proper use of metrics and comparison\n",
    "- **Interpretation** (20%): Meaningful cluster analysis and insights\n",
    "- **Recommendations** (10%): Actionable innovation strategy\n",
    "\n",
    "Good luck! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}