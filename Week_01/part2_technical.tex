% PART 2 SECTION DIVIDER
\begin{frame}[plain]
\begin{center}
\vspace{2em}
\Large\textcolor{mlgreen}{\textbf{PART 2}}\\
\vspace{0.5em}
\Large\textbf{Technical Core}\\
\vspace{2em}
\Large
What we'll learn:\\
\vspace{1em}
\normalsize
\begin{itemize}
\item K-means clustering algorithm
\item Finding optimal K with elbow method
\item Distance metrics and quality measures
\item Advanced techniques (DBSCAN, Hierarchical)
\item Feature importance analysis
\end{itemize}
\vspace{2em}
\Large\textcolor{mlpurple}{\textbf{Learning the basics step by step}}
\end{center}
\end{frame}

% Learning Objectives for Part 2
\begin{frame}
\frametitle{\Large Part 2: Learning Objectives}
\framesubtitle{Technical Skills You'll Develop}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title={By the end of Part 2, you will understand:}]
\normalsize
\begin{itemize}
\item \textbf{How} K-means clustering works
\item \textbf{What} the elbow method shows us
\item \textbf{Why} we measure distances
\item \textbf{How to check} if clusters are good
\item \textbf{Differences} between algorithms
\item \textbf{When to use} each method
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlorange!10, colframe=mlorange!50, title=Practical Skills]
\normalsize
\begin{itemize}
\item Use K-means step by step
\item Understand quality scores
\item Pick the right algorithm
\item Adjust settings properly
\item Work with different patterns
\item Prepare data for analysis
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

% PART 2: TECHNICAL CORE (10 slides)

% Section Divider: Part 2
\begin{frame}
\begin{center}
\vspace{2cm}
{\Huge\textbf{PART 2}}

\vspace{0.5cm}
{\LARGE Technical Core}

\vspace{1cm}
{\large Machine Learning Algorithms \& Implementation}
\end{center}
\end{frame}

% THE CLUSTERING PROBLEM
\begin{frame}
\frametitle{\Large The Innovation Classification Problem}
\framesubtitle{5000 Ideas - How Do They Connect?}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlred!10, colframe=mlred!50, title=The Pain]
\textbf{Current Reality:}
\begin{itemize}
\item One-size-fits-all solutions
\item Generic innovation categories
\item Missed opportunities
\item Unhappy edge cases
\end{itemize}
\vspace{0.5em}
\textbf{The Cost:}
\begin{itemize}
\item Most innovations get misclassified
\item Features with low adoption rates
\item Inefficient resource allocation
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title=The Question]
\Large\textbf{What if we could...}
\normalsize
\begin{itemize}
\item Find natural innovation clusters?
\item Discover innovation patterns?
\item Innovate at scale?
\item Identify opportunity gaps?
\end{itemize}
\vspace{0.5em}
\Large\textcolor{mlpurple}{\textbf{We can!}}\\
\normalsize\textbf{Solution: Clustering}
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

% NEW: Current Reality Visualization
\begin{frame}
\frametitle{\Large Current Reality: The Problem}
\framesubtitle{Why One-Size-Fits-All Doesn't Work}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/current_reality_visual.pdf}
\end{center}
\end{frame}

% NEW: Innovation Archetypes Definition
\begin{frame}
\frametitle{\Large Innovation Archetypes}
\framesubtitle{Common Patterns We'll Discover}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{\large Core Innovation Types}
\vspace{0.5em}

\textcolor{mlred}{\textbf{Disruptive Innovation}}\\
Completely new approaches that reshape markets

\vspace{0.5em}
\textcolor{mlblue}{\textbf{Incremental Innovation}}\\
Step-by-step improvements to existing solutions

\vspace{0.5em}
\textcolor{mlgreen}{\textbf{Service Innovation}}\\
New ways to deliver value to customers
\end{column}

\begin{column}{0.48\textwidth}
\textbf{\large Emerging Patterns}
\vspace{0.5em}

\textcolor{mlorange}{\textbf{Business Model Innovation}}\\
New ways to create and capture value

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Process Innovation}}\\
Better ways to produce and deliver

\vspace{0.5em}
\textcolor{mlpink}{\textbf{Platform Innovation}}\\
Creating ecosystems for innovation
\end{column}
\end{columns}

\vspace{0.5em}
\begin{center}
\large\textcolor{mlpurple}{\textbf{Clustering will reveal which type each idea belongs to}}
\end{center}
\end{frame}

% Slide 9: What is Clustering?
\begin{frame}
\frametitle{\Large What is Clustering?}
\framesubtitle{Like Organizing a Messy Room - Finding Things That Belong Together}

\begin{center}
\includegraphics[width=0.75\textwidth]{charts/chaos_to_clarity.pdf}
\end{center}

\vspace{0.5em}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\Large\textbf{Clustering Finds:}
\normalsize
\begin{itemize}
\item Natural groupings
\item Similar approaches  
\item Hidden patterns
\item Innovation relationships
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50]
\textbf{Key Insight:}\\
Things that look similar often belong in the same group\\
\textit{(Just like organizing books by topic on a shelf)}
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

% Slide 10: K-Means Algorithm Part 1
\begin{frame}
\frametitle{\Large K-Means: The Basic Clustering Method (Part 1)}
\framesubtitle{Initial Setup - Like Choosing City Centers}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\Large\textbf{Steps 1-2: Setup}
\normalsize
\vspace{0.5em}

\textbf{Step 1: Choose K}
\begin{itemize}
\item Decide number of clusters
\item Based on domain knowledge
\item Or use elbow method
\end{itemize}

\vspace{0.5em}
\textbf{Step 2: Initialize Centroids}
\begin{itemize}
\item Place K random points
\item These are initial cluster centers
\item Random but spread out
\end{itemize}

\vspace{0.5em}
\textcolor{mlblue}{\textbf{Key Decision: How many groups make sense?}}
\end{column}

\begin{column}{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/kmeans_animation.pdf}
\end{center}
\textit{Visualization: Initial random centroids placed}
\end{column}
\end{columns}
\end{frame}

% Slide 11: K-Means Algorithm Part 2
\begin{frame}
\frametitle{\Large K-Means: The Basic Clustering Method (Part 2)}
\framesubtitle{Iteration Process - Finding Natural Groups}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\Large\textbf{Steps 3-5: Iterate}
\normalsize
\vspace{0.5em}

\textbf{Step 3: Assign Points}
\begin{itemize}
\item Calculate distance to each centroid
\item Assign to nearest centroid
\item Forms initial clusters
\end{itemize}

\vspace{0.5em}
\textbf{Step 4: Update Centroids}
\begin{itemize}
\item Calculate mean of each cluster
\item Move centroid to that mean
\item Centers shift to better positions
\end{itemize}

\vspace{0.5em}
\textbf{Step 5: Check Convergence}
\begin{itemize}
\item Repeat steps 3-4
\item Stop when centroids don't move
\item Usually 5-10 iterations
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/kmeans_animation.pdf}
\end{center}
\textit{Iteration 1 $\rightarrow$ 3 $\rightarrow$ 5 $\rightarrow$ \textbf{Converged}}
\end{column}
\end{columns}
\end{frame}

% PROBLEM: HOW MANY GROUPS?
\begin{frame}
\frametitle{\Large The Goldilocks Problem}
\framesubtitle{Too Few vs. Too Many Groups}

\begin{columns}[T]
\begin{column}{0.32\textwidth}
\begin{tcolorbox}[colback=mlred!10, colframe=mlred!50, title={Too Few (K=2)}]
\begin{center}
\textbf{Oversimplification}
\end{center}
\begin{itemize}
\normalsize
\item Mixed segments
\item Lost nuance
\item Generic solutions
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.32\textwidth}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title=Just Right (K=5)]
\begin{center}
\textbf{Optimal Balance}
\end{center}
\begin{itemize}
\normalsize
\item Clear segments
\item Actionable insights
\item Manageable complexity
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.32\textwidth}
\begin{tcolorbox}[colback=mlorange!10, colframe=mlorange!50, title=Too Many (K=20)]
\begin{center}
\textbf{Analysis Paralysis}
\end{center}
\begin{itemize}
\normalsize
\item Overfitting
\item Tiny segments
\item Impossible to act on
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}

\vspace{1em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{How do we find the sweet spot?}}
\end{center}
\end{frame}

% Slide 12: Choosing K - Elbow Method
\begin{frame}
\frametitle{\Large The Elbow Method}
\framesubtitle{How Many Groups Should We Have? (Like Goldilocks - Not Too Few, Not Too Many)}

\begin{columns}[T]
\begin{column}{0.43\textwidth}
\Large\textbf{Finding the Elbow:}
\normalsize
\begin{itemize}
\item Plot inertia vs K
\item Look for the ``elbow''
\item Balance between:
    \begin{itemize}
    \normalsize
    \item Too few: Mixed groups
    \item Too many: Overfitting
    \end{itemize}
\end{itemize}

\vspace{0.5em}
\begin{tcolorbox}[colback=mlblue!10, colframe=mlblue!50]
\textbf{Optimal K = 5}\\
Best trade-off between simplicity and accuracy
\end{tcolorbox}
\end{column}

\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/elbow_method.pdf}
\end{center}
\end{column}
\end{columns}
\end{frame}

% Slide 13: Distance Metrics
\begin{frame}
\frametitle{\Large Distance Metrics}
\framesubtitle{Different Ways to Measure "How Close" Things Are}

\begin{center}
\includegraphics[width=0.9\textwidth]{charts/distance_metrics_detailed.pdf}
\end{center}

\vspace{0.5em}
\begin{center}
\large\textcolor{mlpurple}{\textbf{Each metric reveals different patterns in your data}}
\end{center}
\end{frame}

% Slide 14: Cluster Quality
\begin{frame}
\frametitle{\Large Cluster Quality Metrics}
\framesubtitle{Are Our Groups Any Good? (Like Checking Your Work)}

\begin{center}
\includegraphics[width=0.75\textwidth]{charts/cluster_quality.pdf}
\end{center}

\vspace{0.5em}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\Large\textbf{Silhouette Score:}
\normalsize
\begin{itemize}
\item Ranges from -1 to +1
\item Higher = better separation
\item Our score: \textbf{0.73}
\end{itemize}
\textcolor{mlgreen}{\textbf{0.73 = Strong clusters!}}
\end{column}

\begin{column}{0.48\textwidth}
\textbf{What it measures:}
\begin{itemize}
\item Within-cluster cohesion
\item Between-cluster separation  
\item Overall cluster validity
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% Evaluation Metrics Slide 1: Silhouette Score
\begin{frame}
\frametitle{\Large Evaluation Metric 1: Silhouette Score}
\framesubtitle{Measuring Cluster Cohesion and Separation}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/silhouette_score.pdf}
\end{center}
\end{frame}

% Evaluation Metrics Slide 2: Elbow Method
\begin{frame}
\frametitle{\Large Evaluation Metric 2: Elbow Method}
\framesubtitle{Finding the Right Number of Clusters}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/elbow_method.pdf}
\end{center}
\end{frame}

% Evaluation Metrics Slide 3: Davies-Bouldin Index
\begin{frame}
\frametitle{\Large Evaluation Metric 3: Davies-Bouldin Index}
\framesubtitle{Balancing Within and Between Cluster Distances}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/davies_bouldin.pdf}
\end{center}
\end{frame}

% PROBLEM: NON-SPHERICAL GROUPS
\begin{frame}
\frametitle{\Large When Circles Don't Work}
\framesubtitle{Real Innovation Clusters Have Complex Shapes}

\begin{center}
\Large\textbf{K-Means Assumes Spherical Clusters}\\
\vspace{1em}
\normalsize But what about:\\
\vspace{1em}
\begin{itemize}
\item Innovations connected through technology stacks
\item Domain-specific innovation clusters
\item Evolution patterns (incremental, disruptive)
\item Outliers and noise points
\end{itemize}
\vspace{1em}
\Large\textcolor{mlred}{\textbf{K-Means Forces Round Pegs into Round Holes}}\\
\vspace{1em}
\Large\textcolor{mlgreen}{\textbf{Solution: Density-Based Clustering}}
\end{center}
\end{frame}

% Slide 15: Beyond K-Means - DBSCAN
\begin{frame}
\frametitle{\Large DBSCAN: Finding Groups Naturally}
\framesubtitle{Like Finding Groups of People at a Party - Where Are the Crowds?}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\Large\textbf{DBSCAN Advantages:}
\normalsize
\begin{itemize}
\item No need to specify K \textit{(finds groups automatically)}
\item Finds arbitrary shapes \textit{(not just circles)}
\item Identifies outliers \textit{(points that don't belong)}
\item Handles noise well \textit{(robust to random points)}
\end{itemize}

\vspace{0.5em}
\textbf{Perfect for:}
\begin{itemize}
\item Non-spherical patterns
\item Varying densities
\item Outlier detection
\item Exploratory analysis
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/dbscan_shapes.pdf}
\end{center}
\end{column}
\end{columns}
\end{frame}

% Slide 16: DBSCAN Parameters Explained
\begin{frame}
\frametitle{\Large DBSCAN: Understanding Parameters}
\framesubtitle{Two Simple Settings Control Everything}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title=Epsilon (Distance)]
\normalsize
\textbf{What it does:}\\
Sets the maximum distance to consider points as neighbors

\vspace{0.5em}
\textbf{Think of it as:}\\
How far can points be apart and still be friends?

\vspace{0.5em}
\textbf{Too small:} Many tiny clusters\\
\textbf{Too large:} Everything merges
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlorange!10, colframe=mlorange!50, title=MinPts (Density)]
\normalsize
\textbf{What it does:}\\
Minimum neighbors needed to form a dense region

\vspace{0.5em}
\textbf{Think of it as:}\\
How many friends make a group?

\vspace{0.5em}
\textbf{Too small:} Noise becomes clusters\\
\textbf{Too large:} Small clusters vanish
\end{tcolorbox}
\end{column}
\end{columns}

\vspace{0.5em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{Rule of thumb: MinPts = 2 × dimensions}}
\end{center}
\end{frame}

% Clustering Algorithm Comparison Table - Slide 1
\begin{frame}
\frametitle{\Large Clustering Algorithm Comparison}
\framesubtitle{Technical Characteristics at a Glance}

\vspace{0.5em}
\begin{center}
\normalsize
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Speed} & \textbf{Shape} & \textbf{Outliers} & \textbf{Params} & \textbf{Best For} \\
\midrule
\textcolor{mlblue}{\textbf{K-Means}} & Fast & Spherical & Sensitive & K only & Quick segments \\
& O(nkt) & clusters & & & \\
\midrule
\textcolor{mlgreen}{\textbf{DBSCAN}} & Medium & Any & Robust & eps, MinPts & Complex shapes \\
& O(n log n) & shape & (detects) & & \\
\midrule
\textcolor{mlorange}{\textbf{Hierarchical}} & Slow & Any & Moderate & Distance & Multi-level \\
& O(n²) & shape & & threshold & analysis \\
\midrule
\textcolor{mlpurple}{\textbf{GMM}} & Medium & Elliptical & Moderate & K, & Overlapping \\
& O(nkt) & clusters & & covariance & groups \\
\bottomrule
\end{tabular}
\end{center}

\vspace{1em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{Each algorithm has its strengths - choose wisely!}}
\end{center}
\end{frame}

% Clustering Algorithm Selection Guide - Slide 2
\begin{frame}
\frametitle{\Large When to Use Each Algorithm}
\framesubtitle{Practical Decision Guide}

\vspace{0.5em}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlblue!10, colframe=mlblue!50, title=\textbf{K-Means}]
\textbf{Perfect when:}
\begin{itemize}
\item Speed is critical
\item Clusters are roughly equal size
\item You know K in advance
\item Data has spherical patterns
\end{itemize}
\end{tcolorbox}

\vspace{0.5em}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title=\textbf{DBSCAN}]
\textbf{Perfect when:}
\begin{itemize}
\item Clusters have irregular shapes
\item Outliers need identification
\item Density varies across data
\item You don't know K
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlorange!10, colframe=mlorange!50, title=\textbf{Hierarchical}]
\textbf{Perfect when:}
\begin{itemize}
\item Need multiple granularities
\item Want to visualize relationships
\item Small to medium datasets
\item Exploring data structure
\end{itemize}
\end{tcolorbox}

\vspace{0.5em}
\begin{tcolorbox}[colback=mlpurple!10, colframe=mlpurple!50, title=\textbf{GMM}]
\textbf{Perfect when:}
\begin{itemize}
\item Groups overlap
\item Need probability scores
\item Elliptical cluster shapes
\item Soft assignments needed
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

% NEW: Algorithm Visual Examples
\begin{frame}
\frametitle{\Large Algorithm Visual Comparison}
\framesubtitle{Same Data, Different Approaches}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/algorithm_visual_examples.pdf}
\end{center}
\end{frame}

% NEW: Gaussian Mixture Models Detailed
\begin{frame}
\frametitle{\Large Gaussian Mixture Models (GMM)}
\framesubtitle{Soft Clustering for Overlapping Innovation Categories}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/gmm_detailed.pdf}
\end{center}
\end{frame}

% PROBLEM: MULTIPLE GRANULARITIES
\begin{frame}
\frametitle{\Large The Granularity Challenge}
\framesubtitle{When You Need Multiple Levels of Detail}

\begin{center}
\Large\textbf{Fixed K Gives One View}\\
\vspace{1em}
\normalsize But real relationships are hierarchical:\\
\vspace{1em}
\begin{itemize}
\item Organization: Company → Department → Team → Individual
\item Geography: Country → Region → City → Neighborhood
\item Products: Category → Subcategory → Brand → SKU
\item Innovations: All → Categories → Sub-types → Specific solutions
\end{itemize}
\vspace{1em}
\Large\textcolor{mlred}{\textbf{K-means: Pick 5 groups and that's it}}\\
\vspace{1em}
\Large\textcolor{mlgreen}{\textbf{What if we need flexibility?}}\\
\vspace{0.5em}
\normalsize Solution: See the full hierarchy, cut where needed
\end{center}
\end{frame}

% Slide 17: Hierarchical Clustering
\begin{frame}
\frametitle{\Large Hierarchical Clustering}
\framesubtitle{Building a Tree of Relationships}

\begin{columns}[T]
\begin{column}{0.43\textwidth}
\Large\textbf{Dendrogram Benefits:}
\normalsize
\begin{itemize}
\item Shows cluster hierarchy
\item Multiple granularities
\item Natural relationships
\item No preset K needed
\end{itemize}

\vspace{0.5em}
\textbf{Cut the tree at any level:}
\begin{itemize}
\item High cut = Few clusters
\item Low cut = Many clusters
\item Choose based on needs
\end{itemize}
\end{column}

\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/dendrogram_example.pdf}
\end{center}
\end{column}
\end{columns}
\end{frame}

% Slide 18: Feature Importance
\begin{frame}
\frametitle{\Large What Drives the Clusters?}
\framesubtitle{Feature Importance Analysis}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/feature_importance.pdf}
\end{center}

\begin{center}
\Large\textcolor{mlpurple}{\textbf{Key Insight: Usage frequency matters most!}}
\end{center}
\end{frame}

% NEW: Preprocessing Pipeline
\begin{frame}
\frametitle{\Large Data Preprocessing Pipeline}
\framesubtitle{From Raw Data to Clustering-Ready Features}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/preprocessing_pipeline.pdf}
\end{center}
\end{frame}

% NEW: Common Mistakes and Troubleshooting
\begin{frame}
\frametitle{\Large Common Mistakes \& Troubleshooting}
\framesubtitle{Learn from These Pitfalls}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/common_mistakes.pdf}
\end{center}
\end{frame}

% NEW: Parameter Tuning Guidelines
\begin{frame}
\frametitle{\Large Parameter Tuning Guidelines}
\framesubtitle{Recommended Ranges and Best Practices}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/parameter_tuning_guide.pdf}
\end{center}
\end{frame}

% CHECKPOINT SLIDE 2
\begin{frame}
\frametitle{\Large Check Your Understanding - Part 2}
\framesubtitle{Technical Concepts Review \hfill \textcolor{mlorange}{\textbf{Progress: 2/3}}}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlblue!10, colframe=mlblue!50, title=Quick Quiz]
\normalsize
\begin{enumerate}
\item K in K-means stands for:
   \begin{itemize}
   \small
   \item[$\square$] Kernel
   \item[$\checkmark$] Number of clusters
   \item[$\square$] Constant
   \end{itemize}
\item DBSCAN finds:
   \begin{itemize}
   \small
   \item[$\square$] Only circles
   \item[$\checkmark$] Any shape clusters
   \item[$\square$] Exactly K groups
   \end{itemize}
\end{enumerate}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title=Can You Calculate?]
\normalsize
If Silhouette Score = 0.75:
\begin{itemize}
\item Is this good? \textcolor{mlgreen}{Yes!}
\item Range is [-1, 1]
\item Higher = better separation
\end{itemize}
\vspace{0.5em}
\textbf{Remember:}
\begin{itemize}
\item Elbow method finds optimal K
\item Scale your data first!
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}

\vspace{1em}
\begin{center}
\Large\textcolor{mlpurple}{\textbf{Great job! Now let's apply these concepts!}}\\
\vspace{0.3em}
\normalsize\textit{Next: Design integration, innovation patterns, and real-world applications}
\end{center}
\end{frame}

% TRANSITION TO PART 3
\begin{frame}
\frametitle{\Large From Algorithms to Innovation Insights}
\framesubtitle{What Does This Mean for Innovation Opportunities?}

\begin{center}
\Large\textbf{We've learned the technical tools:}\\
\normalsize
Clustering, metrics, quality measures\\
\vspace{1em}
\Large\textcolor{mlred}{\textbf{But clusters are just numbers...}}\\
\vspace{1em}
\normalsize
Until we connect them to innovation opportunities\\
\vspace{2em}
\Large\textcolor{mlgreen}{\textbf{Let's transform data into innovation insights}}\\
\vspace{1em}
\normalsize
Each cluster represents innovation opportunities and patterns
\end{center}
\end{frame}