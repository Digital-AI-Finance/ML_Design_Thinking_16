% APPENDIX: TECHNICAL DEEP DIVE
\appendix
\section{Technical Appendix}

% Appendix Slide 1: K-Means Mathematics
\begin{frame}
\frametitle{\Large Appendix: K-Means Mathematics (Optional)}
\framesubtitle{The Mathematical Foundation - For Those Interested}

\Large\textbf{What K-means tries to minimize:}
\normalsize
$$J = \sum_{i=1}^{n} \sum_{j=1}^{k} w_{ij} ||x_i - \mu_j||^2$$

\textit{In simple terms: Make points close to their group centers}

Where:
\begin{itemize}
\item $n$ = number of data points
\vspace{0.3em}
\item $k$ = number of clusters
\vspace{0.3em}
\item $w_{ij}$ = 1 if $x_i$ belongs to cluster $j$, 0 otherwise
\vspace{0.3em}
\item $\mu_j$ = centroid of cluster $j$
\end{itemize}

\vspace{0.5em}
\Large\textbf{Update Rules:}
\normalsize
\begin{enumerate}
\item Assignment: $c^{(i)} = \arg\min_j ||x^{(i)} - \mu_j||^2$
\item Update: $\mu_j = \frac{1}{|S_j|} \sum_{i \in S_j} x^{(i)}$
\end{enumerate}
\end{frame}

% Appendix Slide 2: Distance Metrics Formulas
\begin{frame}
\frametitle{\Large Appendix: Distance Metrics (Optional)}
\framesubtitle{Different Ways to Measure "How Far Apart" Things Are}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Euclidean Distance:}
$$d(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$$

\vspace{0.5em}
\textbf{Manhattan Distance:}
$$d(x,y) = \sum_{i=1}^{n} |x_i - y_i|$$

\vspace{0.5em}
\textbf{Minkowski Distance:}
$$d(x,y) = \left(\sum_{i=1}^{n} |x_i - y_i|^p\right)^{1/p}$$
\end{column}

\begin{column}{0.48\textwidth}
\textbf{Cosine Similarity:}
$$\cos(\theta) = \frac{x \cdot y}{||x|| \cdot ||y||}$$

\vspace{0.5em}
\textbf{Jaccard Distance:}
$$J(A,B) = 1 - \frac{|A \cap B|}{|A \cup B|}$$

\vspace{0.5em}
\textbf{Mahalanobis Distance:}
$$d(x,y) = \sqrt{(x-y)^T S^{-1} (x-y)}$$
\end{column}
\end{columns}
\end{frame}

% Appendix Slide 3: Silhouette Coefficient
\begin{frame}
\frametitle{\Large Appendix: Silhouette Score Explained}
\framesubtitle{How We Know If Groups Are Good}

\Large\textbf{Silhouette Score for point $i$:}
\normalsize
$$s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$$

Where:
\begin{itemize}
\item $a(i)$ = average distance to points in same cluster
\vspace{0.3em}
\item $b(i)$ = average distance to points in nearest neighbor cluster
\end{itemize}

\vspace{0.5em}
\Large\textbf{Interpretation:}
\normalsize
\begin{itemize}
\item $s(i) \approx 1$: Well clustered
\vspace{0.3em}
\item $s(i) \approx 0$: On border between clusters
\vspace{0.3em}
\item $s(i) \approx -1$: Misclassified
\end{itemize}

\vspace{0.5em}
\Large\textbf{Overall Score:}
\normalsize
$$S = \frac{1}{n} \sum_{i=1}^{n} s(i)$$
\end{frame}

% Appendix Slide 4: PCA for Visualization
\begin{frame}
\frametitle{\Large Appendix: Visualizing High-Dimensional Data}
\framesubtitle{Making Complex Data Viewable in 2D}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{charts/pca_clusters.pdf}
\end{center}
\end{column}

\begin{column}{0.43\textwidth}
\Large\textbf{PCA Process:}
\normalsize
\begin{enumerate}
\item Standardize data
\vspace{0.3em}
\item Compute covariance matrix
\vspace{0.3em}
\item Find eigenvectors/values
\vspace{0.3em}
\item Select top 2 components
\vspace{0.3em}
\item Transform data
\end{enumerate}

\vspace{0.5em}
\textbf{Variance Explained:}
\begin{itemize}
\item PC1: 45.2\%
\vspace{0.3em}
\item PC2: 28.7\%
\vspace{0.3em}
\item Total: 73.9\%
\end{itemize}
\end{column}
\end{columns}
\end{frame}

% NEW: t-SNE vs PCA Comparison
\begin{frame}
\frametitle{\Large Dimensionality Reduction: PCA vs t-SNE}
\framesubtitle{Revealing Hidden Patterns in High-Dimensional Innovation Space}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/tsne_pca_comparison.pdf}
\end{center}
\end{frame}

% Appendix Slide 5: DBSCAN Algorithm
\begin{frame}
\frametitle{\Large Appendix: How DBSCAN Works}
\framesubtitle{Finding Groups Based on How Close Points Are}

\Large\textbf{Key Parameters:}
\normalsize
\begin{itemize}
\item $\epsilon$ (eps): Maximum distance between points
\vspace{0.3em}
\item MinPts: Minimum points to form dense region
\end{itemize}

\vspace{0.5em}
\Large\textbf{Point Classification:}
\normalsize
\begin{itemize}
\item \textbf{Core point}: Has $\geq$ MinPts within $\epsilon$
\vspace{0.3em}
\item \textbf{Border point}: Within $\epsilon$ of core point
\vspace{0.3em}
\item \textbf{Noise point}: Neither core nor border
\end{itemize}

\vspace{0.5em}
\Large\textbf{Algorithm Steps:}
\normalsize
\begin{enumerate}
\item Find all core points
\vspace{0.3em}
\item Form clusters from core points within $\epsilon$
\vspace{0.3em}
\item Assign border points to clusters
\vspace{0.3em}
\item Mark remaining as noise
\end{enumerate}
\end{frame}

% NEW: DBSCAN Parameter Tuning
\begin{frame}
\frametitle{\Large DBSCAN Parameter Tuning}
\framesubtitle{Impact of eps and min\_samples on Clustering Results}

\begin{center}
\includegraphics[width=0.85\textwidth]{charts/dbscan_tuning.pdf}
\end{center}
\end{frame}

% Appendix Slide 6: Python Code Examples
\begin{frame}[fragile]
\frametitle{\Large Appendix: Python Implementation}
\framesubtitle{Ready-to-Use Code Snippets}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=gray!5, colframe=mlblue!50, title={\small K-Means Clustering}]
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily]
from sklearn.cluster import KMeans
import numpy as np

# Generate sample data
X = np.random.randn(1000, 2)

# Configure and fit K-means
kmeans = KMeans(
    n_clusters=3,
    random_state=42,
    n_init=10
)
labels = kmeans.fit_predict(X)

# Get cluster centers
centers = kmeans.cluster_centers_

# Calculate inertia
inertia = kmeans.inertia_
\end{lstlisting}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=gray!5, colframe=mlorange!50, title={\small DBSCAN Clustering}]
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily]
from sklearn.cluster import DBSCAN

# Configure DBSCAN
dbscan = DBSCAN(
    eps=0.3,
    min_samples=5,
    metric='euclidean'
)

# Fit and predict
labels = dbscan.fit_predict(X)

# Analyze results
outliers = labels == -1
n_clusters = len(set(labels)) - 1

print(f"Clusters: {n_clusters}")
print(f"Outliers: {sum(outliers)}")
print(f"Coverage: {100*(1-sum(outliers)/len(labels)):.1f}%")
\end{lstlisting}
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

% NEW: Code Example for Evaluation
\begin{frame}[fragile]
\frametitle{\Large Appendix: Evaluation Metrics Code}
\framesubtitle{Measuring Clustering Quality}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=gray!5, colframe=mlgreen!50, title={\small Silhouette Analysis}]
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily]
from sklearn.metrics import silhouette_score
from sklearn.metrics import silhouette_samples

# Calculate overall score
score = silhouette_score(X, labels)
print(f"Silhouette Score: {score:.3f}")

# Per-sample scores
sample_scores = silhouette_samples(X, labels)

# Per-cluster average
for i in range(n_clusters):
    cluster_scores = sample_scores[labels == i]
    avg = cluster_scores.mean()
    print(f"Cluster {i}: {avg:.3f}")
\end{lstlisting}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=gray!5, colframe=mlpurple!50, title={\small Finding Optimal K}]
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily]
# Elbow method
inertias = []
silhouettes = []
K_range = range(2, 10)

for k in K_range:
    km = KMeans(n_clusters=k,
                random_state=42)
    labels = km.fit_predict(X)
    
    inertias.append(km.inertia_)
    silhouettes.append(
        silhouette_score(X, labels)
    )

# Find elbow point
# Plot inertias vs K
# Choose K at elbow
\end{lstlisting}
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

% Appendix Slide 7: Implementation Hints
\begin{frame}
\frametitle{\Large Appendix: Implementation Guidelines}
\framesubtitle{Practical Considerations}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlblue!10, colframe=mlblue!50, title=Data Preparation]
\normalsize
\begin{itemize}
\item Standardize features
\item Handle missing values
\item Remove outliers (if needed)
\item Feature selection/engineering
\item Consider scaling methods
\end{itemize}
\end{tcolorbox}

\vspace{0.5em}
\begin{tcolorbox}[colback=mlgreen!10, colframe=mlgreen!50, title=Algorithm Selection]
\normalsize
\begin{itemize}
\item K-means: Spherical, similar size
\item DBSCAN: Arbitrary shapes
\item Hierarchical: Nested structure
\item GMM: Overlapping clusters
\end{itemize}
\end{tcolorbox}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tcolorbox}[colback=mlorange!10, colframe=mlorange!50, title=Validation Methods]
\normalsize
\begin{itemize}
\item Silhouette score
\item Davies-Bouldin index
\item Calinski-Harabasz score
\item Visual inspection
\item Domain expert review
\end{itemize}
\end{tcolorbox}

\vspace{0.5em}
\begin{tcolorbox}[colback=mlred!10, colframe=mlred!50, title=Common Pitfalls]
\normalsize
\begin{itemize}
\item Not scaling features
\item Wrong distance metric
\item Ignoring outliers
\item Over-clustering
\item Forcing clusters
\end{itemize}
\end{tcolorbox}
\end{column}
\end{columns}
\end{frame}

% NEW: Glossary of Technical Terms
\begin{frame}
\frametitle{\Large Glossary of Technical Terms}
\framesubtitle{Key Concepts Reference}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\small
\textbf{Algorithms:}
\begin{itemize}
\item \textbf{K-means}: Partitions data into K spherical clusters
\item \textbf{DBSCAN}: Density-based clustering for arbitrary shapes
\item \textbf{GMM}: Gaussian Mixture Models for soft clustering
\item \textbf{Hierarchical}: Tree-based clustering approach
\end{itemize}

\vspace{0.5em}
\textbf{Metrics:}
\begin{itemize}
\item \textbf{Silhouette}: Measures cluster separation (-1 to 1)
\item \textbf{Inertia}: Sum of squared distances to centroids
\item \textbf{Davies-Bouldin}: Ratio of within to between cluster distance
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\small
\textbf{Concepts:}
\begin{itemize}
\item \textbf{Centroid}: Center point of a cluster
\item \textbf{Elbow Method}: Technique to find optimal K
\item \textbf{Outlier}: Data point not belonging to any cluster
\item \textbf{Convergence}: When algorithm stops improving
\end{itemize}

\vspace{0.5em}
\textbf{Preprocessing:}
\begin{itemize}
\item \textbf{Standardization}: Zero mean, unit variance
\item \textbf{Normalization}: Scale to [0,1] range
\item \textbf{PCA}: Principal Component Analysis
\item \textbf{t-SNE}: t-distributed Stochastic Neighbor Embedding
\end{itemize}
\end{column}
\end{columns}
\end{frame}